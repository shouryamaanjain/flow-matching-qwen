12/02/2025 18:25:42 - INFO - training.fm_trainer - Starting training
12/02/2025 18:25:42 - INFO - training.fm_trainer -   Max steps: 210000
12/02/2025 18:25:42 - INFO - training.fm_trainer -   Global batch size: 128
12/02/2025 18:25:42 - INFO - training.fm_trainer -   Gradient accumulation steps: 32
12/02/2025 18:25:42 - INFO - data.data_loader - Including The Stack v1 datasets (RECOMMENDED):
- Contains actual code content directly
- No AWS credentials needed
- Just login with: huggingface-cli login
See: https://huggingface.co/datasets/bigcode/the-stack-dedup
12/02/2025 18:25:42 - INFO - data.data_loader - Loading dataset: dclm-baseline from mlfoundations/dclm-baseline-1.0
Resolving data files: 100%|██████████████████████████████████████████████████████████████████████████| 27838/27838 [00:09<00:00, 2796.09it/s]
12/02/2025 18:26:52 - INFO - data.data_loader - ✓ Loaded dclm-baseline with weight 60.17
12/02/2025 18:26:52 - INFO - data.data_loader - Loading dataset: openwebmath from open-web-math/open-web-math
Resolving data files: 100%|███████████████████████████████████████████████████████████████████████████████| 114/114 [00:00<00:00, 581.87it/s]
12/02/2025 18:26:54 - INFO - data.data_loader - ✓ Loaded openwebmath with weight 12.98
12/02/2025 18:26:54 - INFO - data.data_loader - Loading dataset: arxiv-summarization from ccdv/arxiv-summarization
12/02/2025 18:26:56 - INFO - data.data_loader - ✓ Loaded arxiv-summarization with weight 9.18
12/02/2025 18:26:56 - INFO - data.data_loader - Loading dataset: wikipedia-en from wikimedia/wikipedia
Resolving data files: 100%|█████████████████████████████████████████████████████████████████████████████████| 41/41 [00:00<00:00, 195.31it/s]
12/02/2025 18:26:58 - INFO - data.data_loader - ✓ Loaded wikipedia-en with weight 5.41
12/02/2025 18:26:58 - INFO - data.data_loader - Loading dataset: mathpile from zwhe99/mathpile-text
Resolving data files: 100%|█████████████████████████████████████████████████████████████████████████████████| 53/53 [00:00<00:00, 671.43it/s]
Resolving data files: 100%|███████████████████████████████████████████████████████████████████████████████| 53/53 [00:00<00:00, 24361.44it/s]
12/02/2025 18:27:00 - INFO - data.data_loader - ✓ Loaded mathpile with weight 3.24
12/02/2025 18:27:00 - INFO - data.data_loader - Loading dataset: stack-v1-python from bigcode/the-stack-dedup
Resolving data files: 100%|███████████████████████████████████████████████████████████████████████████████| 144/144 [00:00<00:00, 415.43it/s]
12/02/2025 18:27:02 - INFO - data.data_loader - ✓ Loaded stack-v1-python with weight 50.75
12/02/2025 18:27:02 - INFO - data.data_loader - Loading dataset: stack-v1-javascript from bigcode/the-stack-dedup
Resolving data files: 100%|█████████████████████████████████████████████████████████████████████████████| 310/310 [00:00<00:00, 36247.50it/s]
12/02/2025 18:27:05 - INFO - data.data_loader - ✓ Loaded stack-v1-javascript with weight 20.0
12/02/2025 18:27:05 - INFO - data.data_loader - Loading dataset: stack-v1-java from bigcode/the-stack-dedup
Resolving data files: 100%|█████████████████████████████████████████████████████████████████████████████| 208/208 [00:00<00:00, 17220.99it/s]
12/02/2025 18:27:07 - INFO - data.data_loader - ✓ Loaded stack-v1-java with weight 15.0
12/02/2025 18:27:07 - INFO - data.data_loader - Loading dataset: stack-v1-cpp from bigcode/the-stack-dedup
Resolving data files: 100%|█████████████████████████████████████████████████████████████████████████████| 110/110 [00:00<00:00, 31655.12it/s]
12/02/2025 18:27:09 - INFO - data.data_loader - ✓ Loaded stack-v1-cpp with weight 10.0
12/02/2025 18:27:09 - INFO - data.data_loader - Creating mixture with 9 datasets
12/02/2025 18:27:09 - INFO - data.data_loader - Sampling probabilities: {'dclm-baseline': 0.3222299576929256, 'openwebmath': 0.06951212981309914, 'arxiv-summarization': 0.04916189150109784, 'wikipedia-en': 0.028972312965243937, 'mathpile': 0.01735125582391689, 'stack-v1-python': 0.27178278798264877, 'stack-v1-javascript': 0.10710651743158572, 'stack-v1-java': 0.08032988807368929, 'stack-v1-cpp': 0.05355325871579286}
12/02/2025 18:27:37 - INFO - training.fm_trainer - Step 10/210000 (0.00%): loss=14.7578, lr=0.00e+00, step_time=1882.9ms, ETA 4d 13h
12/02/2025 18:27:55 - INFO - training.fm_trainer - Step 20/210000 (0.01%): loss=10.0971, lr=0.00e+00, step_time=1840.7ms, ETA 4d 13h
12/02/2025 18:28:14 - INFO - training.fm_trainer - Step 30/210000 (0.01%): loss=10.6875, lr=0.00e+00, step_time=1842.3ms, ETA 4d 13h
12/02/2025 18:28:33 - INFO - training.fm_trainer - Step 40/210000 (0.02%): loss=10.3523, lr=6.00e-07, step_time=1825.1ms, ETA 4d 13h
12/02/2025 18:28:51 - INFO - training.fm_trainer - Step 50/210000 (0.02%): loss=12.0700, lr=6.00e-07, step_time=1852.8ms, ETA 4d 12h
12/02/2025 18:28:51 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 18:28:51 - INFO - data.data_loader - Including The Stack v1 datasets (RECOMMENDED):
- Contains actual code content directly
- No AWS credentials needed
- Just login with: huggingface-cli login
See: https://huggingface.co/datasets/bigcode/the-stack-dedup
12/02/2025 18:28:51 - INFO - data.data_loader - Loading dataset: dclm-baseline from mlfoundations/dclm-baseline-1.0
Resolving data files: 100%|██████████████████████████████████████████████████████████████████████████| 27838/27838 [00:06<00:00, 4569.57it/s]
12/02/2025 18:29:45 - INFO - data.data_loader - ✓ Loaded dclm-baseline with weight 60.17
12/02/2025 18:29:45 - INFO - data.data_loader - Loading dataset: openwebmath from open-web-math/open-web-math
Resolving data files: 100%|███████████████████████████████████████████████████████████████████████████████| 114/114 [00:00<00:00, 418.94it/s]
12/02/2025 18:29:47 - INFO - data.data_loader - ✓ Loaded openwebmath with weight 12.98
12/02/2025 18:29:47 - INFO - data.data_loader - Loading dataset: arxiv-summarization from ccdv/arxiv-summarization
12/02/2025 18:29:48 - INFO - data.data_loader - ✓ Loaded arxiv-summarization with weight 9.18
12/02/2025 18:29:48 - INFO - data.data_loader - Loading dataset: wikipedia-en from wikimedia/wikipedia
Resolving data files: 100%|█████████████████████████████████████████████████████████████████████████████████| 41/41 [00:00<00:00, 218.40it/s]
12/02/2025 18:29:50 - INFO - data.data_loader - ✓ Loaded wikipedia-en with weight 5.41
12/02/2025 18:29:50 - INFO - data.data_loader - Loading dataset: mathpile from zwhe99/mathpile-text
Resolving data files: 100%|█████████████████████████████████████████████████████████████████████████████████| 53/53 [00:00<00:00, 591.61it/s]
Resolving data files: 100%|███████████████████████████████████████████████████████████████████████████████| 53/53 [00:00<00:00, 25347.56it/s]
12/02/2025 18:29:51 - INFO - data.data_loader - ✓ Loaded mathpile with weight 3.24
12/02/2025 18:29:51 - INFO - data.data_loader - Loading dataset: stack-v1-python from bigcode/the-stack-dedup
Resolving data files: 100%|███████████████████████████████████████████████████████████████████████████████| 144/144 [00:00<00:00, 241.97it/s]
12/02/2025 18:29:54 - INFO - data.data_loader - ✓ Loaded stack-v1-python with weight 50.75
12/02/2025 18:29:54 - INFO - data.data_loader - Loading dataset: stack-v1-javascript from bigcode/the-stack-dedup
Resolving data files: 100%|█████████████████████████████████████████████████████████████████████████████| 310/310 [00:00<00:00, 17536.84it/s]
12/02/2025 18:29:57 - INFO - data.data_loader - ✓ Loaded stack-v1-javascript with weight 20.0
12/02/2025 18:29:57 - INFO - data.data_loader - Loading dataset: stack-v1-java from bigcode/the-stack-dedup
Resolving data files: 100%|█████████████████████████████████████████████████████████████████████████████| 208/208 [00:00<00:00, 17395.77it/s]
12/02/2025 18:29:59 - INFO - data.data_loader - ✓ Loaded stack-v1-java with weight 15.0
12/02/2025 18:29:59 - INFO - data.data_loader - Loading dataset: stack-v1-cpp from bigcode/the-stack-dedup
Resolving data files: 100%|█████████████████████████████████████████████████████████████████████████████| 110/110 [00:00<00:00, 97008.71it/s]
12/02/2025 18:30:01 - INFO - data.data_loader - ✓ Loaded stack-v1-cpp with weight 10.0
12/02/2025 18:30:01 - INFO - data.data_loader - Creating mixture with 9 datasets
12/02/2025 18:30:01 - INFO - data.data_loader - Sampling probabilities: {'dclm-baseline': 0.3222299576929256, 'openwebmath': 0.06951212981309914, 'arxiv-summarization': 0.04916189150109784, 'wikipedia-en': 0.028972312965243937, 'mathpile': 0.01735125582391689, 'stack-v1-python': 0.27178278798264877, 'stack-v1-javascript': 0.10710651743158572, 'stack-v1-java': 0.08032988807368929, 'stack-v1-cpp': 0.05355325871579286}
12/02/2025 18:30:32 - INFO - training.fm_trainer - Eval Step 50: loss=12.2399, ppl=206871.48
12/02/2025 18:30:50 - INFO - training.fm_trainer - Step 60/210000 (0.03%): loss=12.6088, lr=6.00e-07, step_time=1839.7ms, ETA 4d 12h
12/02/2025 18:31:09 - INFO - training.fm_trainer - Step 70/210000 (0.03%): loss=12.1227, lr=1.20e-06, step_time=1838.8ms, ETA 4d 12h
12/02/2025 18:31:28 - INFO - training.fm_trainer - Step 80/210000 (0.04%): loss=12.7967, lr=1.20e-06, step_time=1863.8ms, ETA 4d 12h
12/02/2025 18:31:46 - INFO - training.fm_trainer - Step 90/210000 (0.04%): loss=10.9178, lr=1.20e-06, step_time=1873.4ms, ETA 4d 12h
12/02/2025 18:32:05 - INFO - training.fm_trainer - Step 100/210000 (0.05%): loss=18.3608, lr=1.80e-06, step_time=1832.8ms, ETA 4d 12h
12/02/2025 18:32:05 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 18:32:32 - INFO - training.fm_trainer - Eval Step 100: loss=11.9195, ppl=150160.87
12/02/2025 18:32:50 - INFO - training.fm_trainer - Step 110/210000 (0.05%): loss=12.7618, lr=1.80e-06, step_time=1843.1ms, ETA 4d 12h
12/02/2025 18:33:09 - INFO - training.fm_trainer - Step 120/210000 (0.06%): loss=13.4763, lr=1.80e-06, step_time=1853.6ms, ETA 4d 12h
12/02/2025 18:33:27 - INFO - training.fm_trainer - Step 130/210000 (0.06%): loss=11.4790, lr=2.40e-06, step_time=1836.3ms, ETA 4d 12h
12/02/2025 18:33:46 - INFO - training.fm_trainer - Step 140/210000 (0.07%): loss=11.1604, lr=2.40e-06, step_time=1852.7ms, ETA 4d 12h
Token indices sequence length is longer than the specified maximum sequence length for this model (246929 > 131072). Running this sequence through the model will result in indexing errors
12/02/2025 18:34:05 - INFO - training.fm_trainer - Step 150/210000 (0.07%): loss=11.5300, lr=2.40e-06, step_time=1829.4ms, ETA 4d 12h
12/02/2025 18:34:05 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 18:34:31 - INFO - training.fm_trainer - Eval Step 150: loss=12.0247, ppl=166826.54
12/02/2025 18:34:49 - INFO - training.fm_trainer - Step 160/210000 (0.08%): loss=11.0853, lr=3.00e-06, grad_norm=9.37, step_time=1858.6ms, ETA 4d 12h
12/02/2025 18:35:08 - INFO - training.fm_trainer - Step 170/210000 (0.08%): loss=12.6303, lr=3.00e-06, step_time=1821.1ms, ETA 4d 11h
12/02/2025 18:35:26 - INFO - training.fm_trainer - Step 180/210000 (0.09%): loss=12.4366, lr=3.00e-06, step_time=1849.9ms, ETA 4d 11h
12/02/2025 18:35:45 - INFO - training.fm_trainer - Step 190/210000 (0.09%): loss=11.1039, lr=3.00e-06, step_time=1869.0ms, ETA 4d 11h
12/02/2025 18:36:03 - INFO - training.fm_trainer - Step 200/210000 (0.10%): loss=13.0382, lr=3.60e-06, step_time=1869.8ms, ETA 4d 12h
12/02/2025 18:36:03 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 18:36:29 - INFO - training.fm_trainer - Eval Step 200: loss=11.7943, ppl=132488.98
12/02/2025 18:36:48 - INFO - training.fm_trainer - Step 210/210000 (0.10%): loss=10.5706, lr=3.60e-06, step_time=1864.2ms, ETA 4d 12h
12/02/2025 18:37:06 - INFO - training.fm_trainer - Step 220/210000 (0.10%): loss=13.5476, lr=3.60e-06, step_time=1832.4ms, ETA 4d 11h
12/02/2025 18:37:25 - INFO - training.fm_trainer - Step 230/210000 (0.11%): loss=7.4003, lr=4.20e-06, step_time=1860.4ms, ETA 4d 12h
12/02/2025 18:37:44 - INFO - training.fm_trainer - Step 240/210000 (0.11%): loss=9.4681, lr=4.20e-06, step_time=1847.2ms, ETA 4d 11h
12/02/2025 18:38:03 - INFO - training.fm_trainer - Step 250/210000 (0.12%): loss=14.8222, lr=4.20e-06, step_time=1919.3ms, ETA 4d 12h
12/02/2025 18:38:03 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 18:38:30 - INFO - training.fm_trainer - Eval Step 250: loss=11.3457, ppl=84601.84
12/02/2025 18:38:48 - INFO - training.fm_trainer - Step 260/210000 (0.12%): loss=9.8437, lr=4.80e-06, step_time=1846.3ms, ETA 4d 12h
12/02/2025 18:39:07 - INFO - training.fm_trainer - Step 270/210000 (0.13%): loss=9.2775, lr=4.80e-06, step_time=1833.6ms, ETA 4d 12h
12/02/2025 18:39:25 - INFO - training.fm_trainer - Step 280/210000 (0.13%): loss=10.1999, lr=4.80e-06, step_time=1875.9ms, ETA 4d 12h
12/02/2025 18:39:44 - INFO - training.fm_trainer - Step 290/210000 (0.14%): loss=11.5097, lr=5.40e-06, step_time=1846.7ms, ETA 4d 12h
12/02/2025 18:40:03 - INFO - training.fm_trainer - Step 300/210000 (0.14%): loss=8.6517, lr=5.40e-06, step_time=1852.3ms, ETA 4d 12h
12/02/2025 18:40:03 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 18:40:30 - INFO - training.fm_trainer - Eval Step 300: loss=10.7982, ppl=48933.52
12/02/2025 18:40:48 - INFO - training.fm_trainer - Step 310/210000 (0.15%): loss=14.4370, lr=5.40e-06, step_time=1852.9ms, ETA 4d 12h
12/02/2025 18:41:07 - INFO - training.fm_trainer - Step 320/210000 (0.15%): loss=11.5285, lr=6.00e-06, grad_norm=2.63, step_time=2030.7ms, ETA 4d 13h
12/02/2025 18:41:26 - INFO - training.fm_trainer - Step 330/210000 (0.16%): loss=12.1822, lr=6.00e-06, step_time=1836.7ms, ETA 4d 12h
12/02/2025 18:41:45 - INFO - training.fm_trainer - Step 340/210000 (0.16%): loss=10.2585, lr=6.00e-06, step_time=1840.5ms, ETA 4d 12h
12/02/2025 18:42:03 - INFO - training.fm_trainer - Step 350/210000 (0.17%): loss=10.6552, lr=6.00e-06, step_time=1891.7ms, ETA 4d 12h
12/02/2025 18:42:03 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 18:42:29 - INFO - training.fm_trainer - Eval Step 350: loss=10.2319, ppl=27776.00
12/02/2025 18:42:48 - INFO - training.fm_trainer - Step 360/210000 (0.17%): loss=9.0167, lr=6.60e-06, step_time=1831.3ms, ETA 4d 12h
12/02/2025 18:43:06 - INFO - training.fm_trainer - Step 370/210000 (0.18%): loss=10.3957, lr=6.60e-06, step_time=1829.0ms, ETA 4d 12h
12/02/2025 18:43:25 - INFO - training.fm_trainer - Step 380/210000 (0.18%): loss=10.5716, lr=6.60e-06, step_time=2076.8ms, ETA 4d 13h
12/02/2025 18:43:44 - INFO - training.fm_trainer - Step 390/210000 (0.19%): loss=6.0337, lr=7.20e-06, step_time=1845.5ms, ETA 4d 13h
12/02/2025 18:44:02 - INFO - training.fm_trainer - Step 400/210000 (0.19%): loss=9.8368, lr=7.20e-06, step_time=1835.5ms, ETA 4d 13h
12/02/2025 18:44:02 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 18:44:29 - INFO - training.fm_trainer - Eval Step 400: loss=9.4696, ppl=12959.58
12/02/2025 18:44:48 - INFO - training.fm_trainer - Step 410/210000 (0.20%): loss=9.8551, lr=7.20e-06, step_time=1934.7ms, ETA 4d 13h
12/02/2025 18:45:06 - INFO - training.fm_trainer - Step 420/210000 (0.20%): loss=6.1279, lr=7.80e-06, step_time=1819.2ms, ETA 4d 13h
12/02/2025 18:45:25 - INFO - training.fm_trainer - Step 430/210000 (0.20%): loss=7.5132, lr=7.80e-06, step_time=1845.5ms, ETA 4d 12h
12/02/2025 18:45:43 - INFO - training.fm_trainer - Step 440/210000 (0.21%): loss=5.3961, lr=7.80e-06, step_time=1945.7ms, ETA 4d 13h
12/02/2025 18:46:02 - INFO - training.fm_trainer - Step 450/210000 (0.21%): loss=9.4423, lr=8.40e-06, step_time=1810.9ms, ETA 4d 12h
12/02/2025 18:46:02 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 18:46:28 - INFO - training.fm_trainer - Eval Step 450: loss=7.9406, ppl=2809.17
12/02/2025 18:46:46 - INFO - training.fm_trainer - Step 460/210000 (0.22%): loss=9.6764, lr=8.40e-06, step_time=1842.9ms, ETA 4d 12h
12/02/2025 18:47:05 - INFO - training.fm_trainer - Step 470/210000 (0.22%): loss=6.7736, lr=8.40e-06, step_time=1868.2ms, ETA 4d 12h
12/02/2025 18:47:23 - INFO - training.fm_trainer - Step 480/210000 (0.23%): loss=9.3718, lr=9.00e-06, grad_norm=2.31, step_time=1880.4ms, ETA 4d 12h
12/02/2025 18:47:42 - INFO - training.fm_trainer - Step 490/210000 (0.23%): loss=4.6514, lr=9.00e-06, step_time=1816.9ms, ETA 4d 12h
12/02/2025 18:48:01 - INFO - training.fm_trainer - Step 500/210000 (0.24%): loss=5.6651, lr=9.00e-06, step_time=1827.4ms, ETA 4d 12h
12/02/2025 18:48:01 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 18:48:27 - INFO - training.fm_trainer - Eval Step 500: loss=8.1282, ppl=3388.65
12/02/2025 18:48:27 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-500
12/02/2025 18:48:27 - INFO - accelerate.accelerator - Saving FSDP model
/root/miniconda3/lib/python3.13/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:860: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`.
  warnings.warn(
/root/miniconda3/lib/python3.13/site-packages/torch/distributed/fsdp/_state_dict_utils.py:722: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  local_shape = tensor.shape
/root/miniconda3/lib/python3.13/site-packages/torch/distributed/fsdp/_state_dict_utils.py:739: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.shape,
/root/miniconda3/lib/python3.13/site-packages/torch/distributed/fsdp/_state_dict_utils.py:741: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.dtype,
12/02/2025 18:48:28 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-500/pytorch_model_fsdp_0
12/02/2025 18:48:39 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-500/pytorch_model_fsdp_0
12/02/2025 18:48:39 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-500
12/02/2025 18:48:39 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/02/2025 18:48:41 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-500/optimizer_0
12/02/2025 18:49:01 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-500/optimizer_0
12/02/2025 18:49:01 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-500
12/02/2025 18:49:01 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-500/scheduler.bin
12/02/2025 18:49:01 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-500/sampler.bin
12/02/2025 18:49:01 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-500/sampler_1.bin
12/02/2025 18:49:01 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-500/random_states_0.pkl
12/02/2025 18:49:01 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-500
12/02/2025 18:49:20 - INFO - training.fm_trainer - Step 510/210000 (0.24%): loss=5.0308, lr=9.00e-06, step_time=1861.2ms, ETA 4d 12h
12/02/2025 18:49:38 - INFO - training.fm_trainer - Step 520/210000 (0.25%): loss=7.7388, lr=9.60e-06, step_time=1846.5ms, ETA 4d 12h
12/02/2025 18:49:57 - INFO - training.fm_trainer - Step 530/210000 (0.25%): loss=6.2642, lr=9.60e-06, step_time=1851.4ms, ETA 4d 12h
12/02/2025 18:50:15 - INFO - training.fm_trainer - Step 540/210000 (0.26%): loss=8.8315, lr=9.60e-06, step_time=1843.0ms, ETA 4d 12h
12/02/2025 18:50:34 - INFO - training.fm_trainer - Step 550/210000 (0.26%): loss=12.0621, lr=1.02e-05, step_time=1835.4ms, ETA 4d 11h
12/02/2025 18:50:34 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 18:51:01 - INFO - training.fm_trainer - Eval Step 550: loss=7.2434, ppl=1398.86
12/02/2025 18:51:20 - INFO - training.fm_trainer - Step 560/210000 (0.27%): loss=8.6605, lr=1.02e-05, step_time=1846.2ms, ETA 4d 11h
12/02/2025 18:51:38 - INFO - training.fm_trainer - Step 570/210000 (0.27%): loss=5.8214, lr=1.02e-05, step_time=1849.6ms, ETA 4d 11h
12/02/2025 18:51:57 - INFO - training.fm_trainer - Step 580/210000 (0.28%): loss=7.8873, lr=1.08e-05, step_time=1830.7ms, ETA 4d 11h
12/02/2025 18:52:16 - INFO - training.fm_trainer - Step 590/210000 (0.28%): loss=7.3502, lr=1.08e-05, step_time=1827.6ms, ETA 4d 11h
12/02/2025 18:52:35 - INFO - training.fm_trainer - Step 600/210000 (0.29%): loss=5.4967, lr=1.08e-05, step_time=1865.9ms, ETA 4d 11h
12/02/2025 18:52:35 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 18:53:01 - INFO - training.fm_trainer - Eval Step 600: loss=6.7123, ppl=822.44
12/02/2025 18:53:19 - INFO - training.fm_trainer - Step 610/210000 (0.29%): loss=4.5524, lr=1.14e-05, step_time=1847.7ms, ETA 4d 11h
12/02/2025 18:53:38 - INFO - training.fm_trainer - Step 620/210000 (0.30%): loss=6.1928, lr=1.14e-05, step_time=1917.2ms, ETA 4d 12h
12/02/2025 18:53:57 - INFO - training.fm_trainer - Step 630/210000 (0.30%): loss=6.3590, lr=1.14e-05, step_time=1861.7ms, ETA 4d 12h
12/02/2025 18:54:16 - INFO - training.fm_trainer - Step 640/210000 (0.30%): loss=4.9450, lr=1.20e-05, grad_norm=7.68, step_time=2089.4ms, ETA 4d 13h
12/02/2025 18:54:35 - INFO - training.fm_trainer - Step 650/210000 (0.31%): loss=5.6610, lr=1.20e-05, step_time=1857.9ms, ETA 4d 13h
12/02/2025 18:54:35 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 18:55:02 - INFO - training.fm_trainer - Eval Step 650: loss=6.2410, ppl=513.37
12/02/2025 18:55:21 - INFO - training.fm_trainer - Step 660/210000 (0.31%): loss=8.2938, lr=1.20e-05, step_time=1863.2ms, ETA 4d 13h
12/02/2025 18:55:40 - INFO - training.fm_trainer - Step 670/210000 (0.32%): loss=2.9424, lr=1.20e-05, step_time=1833.6ms, ETA 4d 12h
12/02/2025 18:55:59 - INFO - training.fm_trainer - Step 680/210000 (0.32%): loss=3.3331, lr=1.26e-05, step_time=1852.7ms, ETA 4d 12h
12/02/2025 18:56:17 - INFO - training.fm_trainer - Step 690/210000 (0.33%): loss=8.3077, lr=1.26e-05, step_time=1846.6ms, ETA 4d 12h
12/02/2025 18:56:36 - INFO - training.fm_trainer - Step 700/210000 (0.33%): loss=5.4433, lr=1.26e-05, step_time=1810.2ms, ETA 4d 12h
12/02/2025 18:56:36 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 18:57:03 - INFO - training.fm_trainer - Eval Step 700: loss=5.9694, ppl=391.27
12/02/2025 18:57:22 - INFO - training.fm_trainer - Step 710/210000 (0.34%): loss=2.8204, lr=1.32e-05, step_time=1898.8ms, ETA 4d 12h
12/02/2025 18:57:41 - INFO - training.fm_trainer - Step 720/210000 (0.34%): loss=3.2679, lr=1.32e-05, step_time=1848.4ms, ETA 4d 12h
12/02/2025 18:57:59 - INFO - training.fm_trainer - Step 730/210000 (0.35%): loss=6.3731, lr=1.32e-05, step_time=1857.6ms, ETA 4d 12h
12/02/2025 18:58:18 - INFO - training.fm_trainer - Step 740/210000 (0.35%): loss=4.3343, lr=1.38e-05, step_time=1972.1ms, ETA 4d 12h
12/02/2025 18:58:37 - INFO - training.fm_trainer - Step 750/210000 (0.36%): loss=7.3312, lr=1.38e-05, step_time=1949.9ms, ETA 4d 13h
12/02/2025 18:58:37 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 18:59:03 - INFO - training.fm_trainer - Eval Step 750: loss=5.3175, ppl=203.88
12/02/2025 18:59:22 - INFO - training.fm_trainer - Step 760/210000 (0.36%): loss=2.6975, lr=1.38e-05, step_time=1826.1ms, ETA 4d 13h
12/02/2025 18:59:40 - INFO - training.fm_trainer - Step 770/210000 (0.37%): loss=2.0664, lr=1.44e-05, step_time=1841.7ms, ETA 4d 12h
12/02/2025 18:59:59 - INFO - training.fm_trainer - Step 780/210000 (0.37%): loss=4.6045, lr=1.44e-05, step_time=1829.7ms, ETA 4d 12h
12/02/2025 19:00:17 - INFO - training.fm_trainer - Step 790/210000 (0.38%): loss=1.6638, lr=1.44e-05, step_time=1856.7ms, ETA 4d 12h
12/02/2025 19:00:36 - INFO - training.fm_trainer - Step 800/210000 (0.38%): loss=2.2620, lr=1.50e-05, grad_norm=0.81, step_time=1925.4ms, ETA 4d 12h
12/02/2025 19:00:36 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:01:03 - INFO - training.fm_trainer - Eval Step 800: loss=4.7788, ppl=118.96
12/02/2025 19:01:22 - INFO - training.fm_trainer - Step 810/210000 (0.39%): loss=2.4251, lr=1.50e-05, step_time=1854.0ms, ETA 4d 12h
12/02/2025 19:01:41 - INFO - training.fm_trainer - Step 820/210000 (0.39%): loss=2.8900, lr=1.50e-05, step_time=1860.4ms, ETA 4d 12h
12/02/2025 19:01:59 - INFO - training.fm_trainer - Step 830/210000 (0.40%): loss=1.8913, lr=1.50e-05, step_time=1903.3ms, ETA 4d 12h
12/02/2025 19:02:18 - INFO - training.fm_trainer - Step 840/210000 (0.40%): loss=3.7244, lr=1.56e-05, step_time=1850.9ms, ETA 4d 12h
12/02/2025 19:02:36 - INFO - training.fm_trainer - Step 850/210000 (0.40%): loss=9.6240, lr=1.56e-05, step_time=1825.4ms, ETA 4d 12h
12/02/2025 19:02:36 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:03:05 - INFO - training.fm_trainer - Eval Step 850: loss=4.5020, ppl=90.20
12/02/2025 19:03:23 - INFO - training.fm_trainer - Step 860/210000 (0.41%): loss=2.9007, lr=1.56e-05, step_time=1821.7ms, ETA 4d 12h
12/02/2025 19:03:42 - INFO - training.fm_trainer - Step 870/210000 (0.41%): loss=3.1602, lr=1.62e-05, step_time=1810.1ms, ETA 4d 11h
12/02/2025 19:04:00 - INFO - training.fm_trainer - Step 880/210000 (0.42%): loss=4.1844, lr=1.62e-05, step_time=1847.6ms, ETA 4d 11h
12/02/2025 19:04:18 - INFO - training.fm_trainer - Step 890/210000 (0.42%): loss=2.6937, lr=1.62e-05, step_time=1864.3ms, ETA 4d 11h
12/02/2025 19:04:37 - INFO - training.fm_trainer - Step 900/210000 (0.43%): loss=9.1444, lr=1.68e-05, step_time=1832.0ms, ETA 4d 11h
12/02/2025 19:04:37 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:05:03 - INFO - training.fm_trainer - Eval Step 900: loss=4.1063, ppl=60.72
12/02/2025 19:05:22 - INFO - training.fm_trainer - Step 910/210000 (0.43%): loss=0.7031, lr=1.68e-05, step_time=1878.1ms, ETA 4d 11h
12/02/2025 19:05:40 - INFO - training.fm_trainer - Step 920/210000 (0.44%): loss=1.3280, lr=1.68e-05, step_time=1837.5ms, ETA 4d 11h
12/02/2025 19:05:58 - INFO - training.fm_trainer - Step 930/210000 (0.44%): loss=8.3378, lr=1.74e-05, step_time=1835.2ms, ETA 4d 11h
12/02/2025 19:06:17 - INFO - training.fm_trainer - Step 940/210000 (0.45%): loss=1.6924, lr=1.74e-05, step_time=1851.4ms, ETA 4d 11h
12/02/2025 19:06:36 - INFO - training.fm_trainer - Step 950/210000 (0.45%): loss=0.7864, lr=1.74e-05, step_time=2136.6ms, ETA 4d 13h
12/02/2025 19:06:36 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:07:03 - INFO - training.fm_trainer - Eval Step 950: loss=4.5095, ppl=90.88
12/02/2025 19:07:21 - INFO - training.fm_trainer - Step 960/210000 (0.46%): loss=7.7133, lr=1.80e-05, grad_norm=1.72, step_time=1909.8ms, ETA 4d 13h
12/02/2025 19:07:40 - INFO - training.fm_trainer - Step 970/210000 (0.46%): loss=2.4525, lr=1.80e-05, step_time=1836.4ms, ETA 4d 13h
12/02/2025 19:07:58 - INFO - training.fm_trainer - Step 980/210000 (0.47%): loss=1.2690, lr=1.80e-05, step_time=1841.8ms, ETA 4d 12h
12/02/2025 19:08:17 - INFO - training.fm_trainer - Step 990/210000 (0.47%): loss=0.8172, lr=1.80e-05, step_time=1826.6ms, ETA 4d 12h
12/02/2025 19:08:35 - INFO - training.fm_trainer - Step 1000/210000 (0.48%): loss=4.2750, lr=1.86e-05, step_time=1843.6ms, ETA 4d 12h
12/02/2025 19:08:35 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:09:03 - INFO - training.fm_trainer - Eval Step 1000: loss=4.5367, ppl=93.39
12/02/2025 19:09:03 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-1000
12/02/2025 19:09:03 - INFO - accelerate.accelerator - Saving FSDP model
12/02/2025 19:09:04 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-1000/pytorch_model_fsdp_0
12/02/2025 19:09:16 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-1000/pytorch_model_fsdp_0
12/02/2025 19:09:17 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-1000
12/02/2025 19:09:17 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/02/2025 19:09:20 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-1000/optimizer_0
12/02/2025 19:09:39 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-1000/optimizer_0
12/02/2025 19:09:39 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-1000
12/02/2025 19:09:39 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-1000/scheduler.bin
12/02/2025 19:09:39 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-1000/sampler.bin
12/02/2025 19:09:39 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-1000/sampler_1.bin
12/02/2025 19:09:39 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-1000/random_states_0.pkl
12/02/2025 19:09:39 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-1000
12/02/2025 19:09:58 - INFO - training.fm_trainer - Step 1010/210000 (0.48%): loss=12.4957, lr=1.86e-05, step_time=1829.2ms, ETA 4d 12h
12/02/2025 19:10:17 - INFO - training.fm_trainer - Step 1020/210000 (0.49%): loss=2.0580, lr=1.86e-05, step_time=1998.4ms, ETA 4d 12h
12/02/2025 19:10:36 - INFO - training.fm_trainer - Step 1030/210000 (0.49%): loss=12.6883, lr=1.92e-05, step_time=1852.5ms, ETA 4d 12h
12/02/2025 19:10:54 - INFO - training.fm_trainer - Step 1040/210000 (0.50%): loss=4.5361, lr=1.92e-05, step_time=1811.5ms, ETA 4d 12h
12/02/2025 19:11:13 - INFO - training.fm_trainer - Step 1050/210000 (0.50%): loss=7.5482, lr=1.92e-05, step_time=1835.4ms, ETA 4d 12h
12/02/2025 19:11:13 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:11:40 - INFO - training.fm_trainer - Eval Step 1050: loss=4.2808, ppl=72.30
12/02/2025 19:11:59 - INFO - training.fm_trainer - Step 1060/210000 (0.50%): loss=6.8173, lr=1.98e-05, step_time=1863.8ms, ETA 4d 12h
12/02/2025 19:12:18 - INFO - training.fm_trainer - Step 1070/210000 (0.51%): loss=2.5697, lr=1.98e-05, step_time=1849.0ms, ETA 4d 12h
12/02/2025 19:12:36 - INFO - training.fm_trainer - Step 1080/210000 (0.51%): loss=1.1486, lr=1.98e-05, step_time=1845.4ms, ETA 4d 12h
12/02/2025 19:12:54 - INFO - training.fm_trainer - Step 1090/210000 (0.52%): loss=11.5412, lr=2.04e-05, step_time=1827.4ms, ETA 4d 11h
12/02/2025 19:13:13 - INFO - training.fm_trainer - Step 1100/210000 (0.52%): loss=2.4276, lr=2.04e-05, step_time=1839.5ms, ETA 4d 11h
12/02/2025 19:13:13 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:13:39 - INFO - training.fm_trainer - Eval Step 1100: loss=4.4291, ppl=83.85
12/02/2025 19:13:58 - INFO - training.fm_trainer - Step 1110/210000 (0.53%): loss=0.7989, lr=2.04e-05, step_time=1808.1ms, ETA 4d 11h
12/02/2025 19:14:17 - INFO - training.fm_trainer - Step 1120/210000 (0.53%): loss=2.3712, lr=2.10e-05, grad_norm=0.26, step_time=1934.6ms, ETA 4d 11h
12/02/2025 19:14:36 - INFO - training.fm_trainer - Step 1130/210000 (0.54%): loss=0.5391, lr=2.10e-05, step_time=1845.4ms, ETA 4d 11h
12/02/2025 19:14:54 - INFO - training.fm_trainer - Step 1140/210000 (0.54%): loss=1.6730, lr=2.10e-05, step_time=1829.7ms, ETA 4d 11h
12/02/2025 19:15:13 - INFO - training.fm_trainer - Step 1150/210000 (0.55%): loss=1.0086, lr=2.10e-05, step_time=1818.0ms, ETA 4d 11h
12/02/2025 19:15:13 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:15:40 - INFO - training.fm_trainer - Eval Step 1150: loss=4.3781, ppl=79.68
12/02/2025 19:15:59 - INFO - training.fm_trainer - Step 1160/210000 (0.55%): loss=5.8272, lr=2.16e-05, step_time=1826.1ms, ETA 4d 11h
12/02/2025 19:16:17 - INFO - training.fm_trainer - Step 1170/210000 (0.56%): loss=3.1991, lr=2.16e-05, step_time=1828.9ms, ETA 4d 11h
12/02/2025 19:16:36 - INFO - training.fm_trainer - Step 1180/210000 (0.56%): loss=1.0704, lr=2.16e-05, step_time=1854.1ms, ETA 4d 11h
12/02/2025 19:16:55 - INFO - training.fm_trainer - Step 1190/210000 (0.57%): loss=4.1492, lr=2.22e-05, step_time=1857.1ms, ETA 4d 11h
12/02/2025 19:17:13 - INFO - training.fm_trainer - Step 1200/210000 (0.57%): loss=6.8438, lr=2.22e-05, step_time=1846.5ms, ETA 4d 11h
12/02/2025 19:17:13 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:17:46 - INFO - training.fm_trainer - Eval Step 1200: loss=4.4168, ppl=82.83
12/02/2025 19:18:05 - INFO - training.fm_trainer - Step 1210/210000 (0.58%): loss=1.8112, lr=2.22e-05, step_time=1867.5ms, ETA 4d 11h
12/02/2025 19:18:24 - INFO - training.fm_trainer - Step 1220/210000 (0.58%): loss=9.6005, lr=2.28e-05, step_time=1855.5ms, ETA 4d 11h
12/02/2025 19:18:42 - INFO - training.fm_trainer - Step 1230/210000 (0.59%): loss=1.3882, lr=2.28e-05, step_time=1861.4ms, ETA 4d 11h
12/02/2025 19:19:01 - INFO - training.fm_trainer - Step 1240/210000 (0.59%): loss=8.1864, lr=2.28e-05, step_time=1850.1ms, ETA 4d 11h
12/02/2025 19:19:20 - INFO - training.fm_trainer - Step 1250/210000 (0.60%): loss=1.3114, lr=2.34e-05, step_time=1869.1ms, ETA 4d 11h
12/02/2025 19:19:20 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:19:46 - INFO - training.fm_trainer - Eval Step 1250: loss=3.8990, ppl=49.36
12/02/2025 19:20:04 - INFO - training.fm_trainer - Step 1260/210000 (0.60%): loss=5.2589, lr=2.34e-05, step_time=1849.3ms, ETA 4d 11h
12/02/2025 19:20:22 - INFO - training.fm_trainer - Step 1270/210000 (0.60%): loss=0.8531, lr=2.34e-05, step_time=1859.6ms, ETA 4d 11h
12/02/2025 19:20:41 - INFO - training.fm_trainer - Step 1280/210000 (0.61%): loss=5.6338, lr=2.40e-05, grad_norm=0.62, step_time=1836.4ms, ETA 4d 11h
12/02/2025 19:21:00 - INFO - training.fm_trainer - Step 1290/210000 (0.61%): loss=1.8522, lr=2.40e-05, step_time=2045.9ms, ETA 4d 12h
12/02/2025 19:21:18 - INFO - training.fm_trainer - Step 1300/210000 (0.62%): loss=1.4481, lr=2.40e-05, step_time=1848.7ms, ETA 4d 12h
12/02/2025 19:21:18 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:21:45 - INFO - training.fm_trainer - Eval Step 1300: loss=4.4064, ppl=81.97
12/02/2025 19:22:04 - INFO - training.fm_trainer - Step 1310/210000 (0.62%): loss=0.9861, lr=2.40e-05, step_time=1838.3ms, ETA 4d 12h
12/02/2025 19:22:23 - INFO - training.fm_trainer - Step 1320/210000 (0.63%): loss=13.3378, lr=2.46e-05, step_time=1903.8ms, ETA 4d 12h
12/02/2025 19:22:42 - INFO - training.fm_trainer - Step 1330/210000 (0.63%): loss=3.4109, lr=2.46e-05, step_time=1934.4ms, ETA 4d 12h
12/02/2025 19:23:00 - INFO - training.fm_trainer - Step 1340/210000 (0.64%): loss=8.1461, lr=2.46e-05, step_time=1871.4ms, ETA 4d 12h
12/02/2025 19:23:19 - INFO - training.fm_trainer - Step 1350/210000 (0.64%): loss=0.6585, lr=2.52e-05, step_time=2029.4ms, ETA 4d 13h
12/02/2025 19:23:19 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:24:08 - INFO - training.fm_trainer - Eval Step 1350: loss=4.1195, ppl=61.53
12/02/2025 19:24:27 - INFO - training.fm_trainer - Step 1360/210000 (0.65%): loss=5.5853, lr=2.52e-05, step_time=1845.3ms, ETA 4d 13h
12/02/2025 19:24:45 - INFO - training.fm_trainer - Step 1370/210000 (0.65%): loss=8.8957, lr=2.52e-05, step_time=1837.1ms, ETA 4d 13h
12/02/2025 19:25:04 - INFO - training.fm_trainer - Step 1380/210000 (0.66%): loss=1.0591, lr=2.58e-05, step_time=1838.0ms, ETA 4d 12h
12/02/2025 19:25:23 - INFO - training.fm_trainer - Step 1390/210000 (0.66%): loss=5.7508, lr=2.58e-05, step_time=1864.3ms, ETA 4d 12h
12/02/2025 19:25:42 - INFO - training.fm_trainer - Step 1400/210000 (0.67%): loss=20.7177, lr=2.58e-05, step_time=1844.0ms, ETA 4d 12h
12/02/2025 19:25:42 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:26:09 - INFO - training.fm_trainer - Eval Step 1400: loss=4.0144, ppl=55.39
12/02/2025 19:26:28 - INFO - training.fm_trainer - Step 1410/210000 (0.67%): loss=0.5580, lr=2.64e-05, step_time=1906.6ms, ETA 4d 12h
12/02/2025 19:26:47 - INFO - training.fm_trainer - Step 1420/210000 (0.68%): loss=1.5304, lr=2.64e-05, step_time=1854.2ms, ETA 4d 12h
12/02/2025 19:27:06 - INFO - training.fm_trainer - Step 1430/210000 (0.68%): loss=1.5264, lr=2.64e-05, step_time=2006.7ms, ETA 4d 13h
12/02/2025 19:27:25 - INFO - training.fm_trainer - Step 1440/210000 (0.69%): loss=2.1024, lr=2.70e-05, grad_norm=11.65, step_time=2095.8ms, ETA 4d 14h
12/02/2025 19:27:44 - INFO - training.fm_trainer - Step 1450/210000 (0.69%): loss=1.0876, lr=2.70e-05, step_time=1842.4ms, ETA 4d 14h
12/02/2025 19:27:44 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:28:11 - INFO - training.fm_trainer - Eval Step 1450: loss=4.0267, ppl=56.08
12/02/2025 19:28:30 - INFO - training.fm_trainer - Step 1460/210000 (0.70%): loss=2.3449, lr=2.70e-05, step_time=1855.0ms, ETA 4d 13h
12/02/2025 19:28:49 - INFO - training.fm_trainer - Step 1470/210000 (0.70%): loss=0.9489, lr=2.70e-05, step_time=1812.1ms, ETA 4d 13h
12/02/2025 19:29:07 - INFO - training.fm_trainer - Step 1480/210000 (0.70%): loss=9.5063, lr=2.76e-05, step_time=1844.0ms, ETA 4d 13h
12/02/2025 19:29:26 - INFO - training.fm_trainer - Step 1490/210000 (0.71%): loss=8.3658, lr=2.76e-05, step_time=1841.1ms, ETA 4d 12h
12/02/2025 19:29:45 - INFO - training.fm_trainer - Step 1500/210000 (0.71%): loss=1.7470, lr=2.76e-05, step_time=1844.3ms, ETA 4d 12h
12/02/2025 19:29:45 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:30:12 - INFO - training.fm_trainer - Eval Step 1500: loss=4.6627, ppl=105.92
12/02/2025 19:30:12 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-1500
12/02/2025 19:30:12 - INFO - accelerate.accelerator - Saving FSDP model
12/02/2025 19:30:14 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-1500/pytorch_model_fsdp_0
12/02/2025 19:30:25 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-1500/pytorch_model_fsdp_0
12/02/2025 19:30:25 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-1500
12/02/2025 19:30:25 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/02/2025 19:30:29 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-1500/optimizer_0
12/02/2025 19:30:47 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-1500/optimizer_0
12/02/2025 19:30:47 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-1500
12/02/2025 19:30:47 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-1500/scheduler.bin
12/02/2025 19:30:47 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-1500/sampler.bin
12/02/2025 19:30:47 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-1500/sampler_1.bin
12/02/2025 19:30:47 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-1500/random_states_0.pkl
12/02/2025 19:30:47 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-1500
12/02/2025 19:30:47 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-500
12/02/2025 19:31:09 - INFO - training.fm_trainer - Step 1510/210000 (0.72%): loss=8.6490, lr=2.82e-05, step_time=1830.3ms, ETA 4d 12h
12/02/2025 19:31:28 - INFO - training.fm_trainer - Step 1520/210000 (0.72%): loss=4.8376, lr=2.82e-05, step_time=1897.3ms, ETA 4d 12h
12/02/2025 19:31:46 - INFO - training.fm_trainer - Step 1530/210000 (0.73%): loss=3.0857, lr=2.82e-05, step_time=1858.8ms, ETA 4d 12h
12/02/2025 19:32:05 - INFO - training.fm_trainer - Step 1540/210000 (0.73%): loss=2.1816, lr=2.88e-05, step_time=1924.9ms, ETA 4d 12h
12/02/2025 19:32:24 - INFO - training.fm_trainer - Step 1550/210000 (0.74%): loss=9.1770, lr=2.88e-05, step_time=1812.2ms, ETA 4d 12h
12/02/2025 19:32:24 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:32:52 - INFO - training.fm_trainer - Eval Step 1550: loss=4.2167, ppl=67.81
12/02/2025 19:33:12 - INFO - training.fm_trainer - Step 1560/210000 (0.74%): loss=2.2847, lr=2.88e-05, step_time=1812.1ms, ETA 4d 12h
12/02/2025 19:33:30 - INFO - training.fm_trainer - Step 1570/210000 (0.75%): loss=4.9297, lr=2.94e-05, step_time=1825.2ms, ETA 4d 11h
12/02/2025 19:33:49 - INFO - training.fm_trainer - Step 1580/210000 (0.75%): loss=11.5909, lr=2.94e-05, step_time=1897.7ms, ETA 4d 11h
12/02/2025 19:34:07 - INFO - training.fm_trainer - Step 1590/210000 (0.76%): loss=4.3136, lr=2.94e-05, step_time=1852.7ms, ETA 4d 11h
12/02/2025 19:34:26 - INFO - training.fm_trainer - Step 1600/210000 (0.76%): loss=4.2970, lr=3.00e-05, grad_norm=0.68, step_time=1973.9ms, ETA 4d 12h
12/02/2025 19:34:26 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:34:54 - INFO - training.fm_trainer - Eval Step 1600: loss=4.2720, ppl=71.66
12/02/2025 19:35:13 - INFO - training.fm_trainer - Step 1610/210000 (0.77%): loss=6.2548, lr=3.00e-05, step_time=1888.2ms, ETA 4d 12h
12/02/2025 19:35:32 - INFO - training.fm_trainer - Step 1620/210000 (0.77%): loss=1.6766, lr=3.00e-05, step_time=1836.5ms, ETA 4d 12h
12/02/2025 19:35:50 - INFO - training.fm_trainer - Step 1630/210000 (0.78%): loss=4.2305, lr=3.00e-05, step_time=1832.2ms, ETA 4d 12h
12/02/2025 19:36:09 - INFO - training.fm_trainer - Step 1640/210000 (0.78%): loss=2.8775, lr=3.06e-05, step_time=1829.6ms, ETA 4d 11h
12/02/2025 19:36:27 - INFO - training.fm_trainer - Step 1650/210000 (0.79%): loss=10.5647, lr=3.06e-05, step_time=1866.9ms, ETA 4d 11h
12/02/2025 19:36:27 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:36:55 - INFO - training.fm_trainer - Eval Step 1650: loss=4.0335, ppl=56.46
12/02/2025 19:37:14 - INFO - training.fm_trainer - Step 1660/210000 (0.79%): loss=4.8813, lr=3.06e-05, step_time=1862.6ms, ETA 4d 11h
12/02/2025 19:37:32 - INFO - training.fm_trainer - Step 1670/210000 (0.80%): loss=8.7030, lr=3.12e-05, step_time=1852.1ms, ETA 4d 11h
12/02/2025 19:37:51 - INFO - training.fm_trainer - Step 1680/210000 (0.80%): loss=2.6226, lr=3.12e-05, step_time=1816.2ms, ETA 4d 11h
12/02/2025 19:38:09 - INFO - training.fm_trainer - Step 1690/210000 (0.80%): loss=1.1800, lr=3.12e-05, step_time=1834.7ms, ETA 4d 11h
12/02/2025 19:38:28 - INFO - training.fm_trainer - Step 1700/210000 (0.81%): loss=1.5959, lr=3.18e-05, step_time=1815.8ms, ETA 4d 11h
12/02/2025 19:38:28 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:38:54 - INFO - training.fm_trainer - Eval Step 1700: loss=3.9886, ppl=53.98
12/02/2025 19:39:12 - INFO - training.fm_trainer - Step 1710/210000 (0.81%): loss=2.3687, lr=3.18e-05, step_time=1816.3ms, ETA 4d 10h
12/02/2025 19:39:30 - INFO - training.fm_trainer - Step 1720/210000 (0.82%): loss=12.3547, lr=3.18e-05, step_time=1831.9ms, ETA 4d 10h
12/02/2025 19:39:49 - INFO - training.fm_trainer - Step 1730/210000 (0.82%): loss=0.5651, lr=3.24e-05, step_time=1802.6ms, ETA 4d 10h
12/02/2025 19:40:07 - INFO - training.fm_trainer - Step 1740/210000 (0.83%): loss=11.2465, lr=3.24e-05, step_time=1841.1ms, ETA 4d 10h
12/02/2025 19:40:26 - INFO - training.fm_trainer - Step 1750/210000 (0.83%): loss=0.9421, lr=3.24e-05, step_time=1837.0ms, ETA 4d 10h
12/02/2025 19:40:26 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:40:53 - INFO - training.fm_trainer - Eval Step 1750: loss=3.8441, ppl=46.72
12/02/2025 19:41:12 - INFO - training.fm_trainer - Step 1760/210000 (0.84%): loss=0.9082, lr=3.30e-05, grad_norm=0.92, step_time=1867.5ms, ETA 4d 10h
12/02/2025 19:41:30 - INFO - training.fm_trainer - Step 1770/210000 (0.84%): loss=1.4427, lr=3.30e-05, step_time=1870.4ms, ETA 4d 10h
12/02/2025 19:41:49 - INFO - training.fm_trainer - Step 1780/210000 (0.85%): loss=4.7820, lr=3.30e-05, step_time=1872.4ms, ETA 4d 10h
12/02/2025 19:42:08 - INFO - training.fm_trainer - Step 1790/210000 (0.85%): loss=10.5482, lr=3.30e-05, step_time=1815.9ms, ETA 4d 10h
12/02/2025 19:42:26 - INFO - training.fm_trainer - Step 1800/210000 (0.86%): loss=0.5821, lr=3.36e-05, step_time=1814.4ms, ETA 4d 10h
12/02/2025 19:42:26 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:42:52 - INFO - training.fm_trainer - Eval Step 1800: loss=3.7111, ppl=40.90
12/02/2025 19:43:11 - INFO - training.fm_trainer - Step 1810/210000 (0.86%): loss=1.2722, lr=3.36e-05, step_time=1872.5ms, ETA 4d 10h
12/02/2025 19:43:29 - INFO - training.fm_trainer - Step 1820/210000 (0.87%): loss=5.2801, lr=3.36e-05, step_time=1939.9ms, ETA 4d 11h
12/02/2025 19:43:48 - INFO - training.fm_trainer - Step 1830/210000 (0.87%): loss=1.5566, lr=3.42e-05, step_time=1875.7ms, ETA 4d 11h
12/02/2025 19:44:07 - INFO - training.fm_trainer - Step 1840/210000 (0.88%): loss=5.9628, lr=3.42e-05, step_time=1928.9ms, ETA 4d 11h
12/02/2025 19:44:25 - INFO - training.fm_trainer - Step 1850/210000 (0.88%): loss=2.4686, lr=3.42e-05, step_time=1848.1ms, ETA 4d 11h
12/02/2025 19:44:25 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:44:53 - INFO - training.fm_trainer - Eval Step 1850: loss=3.7124, ppl=40.95
12/02/2025 19:45:12 - INFO - training.fm_trainer - Step 1860/210000 (0.89%): loss=1.5458, lr=3.48e-05, step_time=1859.8ms, ETA 4d 11h
12/02/2025 19:45:30 - INFO - training.fm_trainer - Step 1870/210000 (0.89%): loss=5.7181, lr=3.48e-05, step_time=1863.1ms, ETA 4d 11h
12/02/2025 19:45:49 - INFO - training.fm_trainer - Step 1880/210000 (0.90%): loss=0.8445, lr=3.48e-05, step_time=1823.5ms, ETA 4d 11h
12/02/2025 19:46:08 - INFO - training.fm_trainer - Step 1890/210000 (0.90%): loss=6.8778, lr=3.54e-05, step_time=1847.9ms, ETA 4d 11h
12/02/2025 19:46:26 - INFO - training.fm_trainer - Step 1900/210000 (0.90%): loss=5.8537, lr=3.54e-05, step_time=1877.4ms, ETA 4d 11h
12/02/2025 19:46:26 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:46:53 - INFO - training.fm_trainer - Eval Step 1900: loss=3.7330, ppl=41.81
12/02/2025 19:47:12 - INFO - training.fm_trainer - Step 1910/210000 (0.91%): loss=1.7220, lr=3.54e-05, step_time=1875.3ms, ETA 4d 11h
12/02/2025 19:47:31 - INFO - training.fm_trainer - Step 1920/210000 (0.91%): loss=0.7364, lr=3.60e-05, grad_norm=0.63, step_time=1865.8ms, ETA 4d 11h
12/02/2025 19:47:49 - INFO - training.fm_trainer - Step 1930/210000 (0.92%): loss=3.7300, lr=3.60e-05, step_time=1834.7ms, ETA 4d 11h
12/02/2025 19:48:08 - INFO - training.fm_trainer - Step 1940/210000 (0.92%): loss=5.6571, lr=3.60e-05, step_time=1822.0ms, ETA 4d 11h
12/02/2025 19:48:26 - INFO - training.fm_trainer - Step 1950/210000 (0.93%): loss=1.0703, lr=3.60e-05, step_time=1906.9ms, ETA 4d 11h
12/02/2025 19:48:26 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:48:52 - INFO - training.fm_trainer - Eval Step 1950: loss=3.6066, ppl=36.84
12/02/2025 19:49:11 - INFO - training.fm_trainer - Step 1960/210000 (0.93%): loss=13.0598, lr=3.66e-05, step_time=1838.5ms, ETA 4d 11h
12/02/2025 19:49:29 - INFO - training.fm_trainer - Step 1970/210000 (0.94%): loss=2.9976, lr=3.66e-05, step_time=1829.0ms, ETA 4d 11h
12/02/2025 19:49:48 - INFO - training.fm_trainer - Step 1980/210000 (0.94%): loss=0.6817, lr=3.66e-05, step_time=1832.2ms, ETA 4d 11h
12/02/2025 19:50:06 - INFO - training.fm_trainer - Step 1990/210000 (0.95%): loss=6.2107, lr=3.72e-05, step_time=1838.4ms, ETA 4d 10h
12/02/2025 19:50:25 - INFO - training.fm_trainer - Step 2000/210000 (0.95%): loss=0.8084, lr=3.72e-05, step_time=1823.5ms, ETA 4d 10h
12/02/2025 19:50:25 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:50:51 - INFO - training.fm_trainer - Eval Step 2000: loss=3.4844, ppl=32.60
12/02/2025 19:50:51 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-2000
12/02/2025 19:50:51 - INFO - accelerate.accelerator - Saving FSDP model
12/02/2025 19:50:52 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-2000/pytorch_model_fsdp_0
12/02/2025 19:51:00 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-2000/pytorch_model_fsdp_0
12/02/2025 19:51:00 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-2000
12/02/2025 19:51:00 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/02/2025 19:51:03 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-2000/optimizer_0
12/02/2025 19:51:19 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-2000/optimizer_0
12/02/2025 19:51:19 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-2000
12/02/2025 19:51:19 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-2000/scheduler.bin
12/02/2025 19:51:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-2000/sampler.bin
12/02/2025 19:51:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-2000/sampler_1.bin
12/02/2025 19:51:19 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-2000/random_states_0.pkl
12/02/2025 19:51:19 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-2000
12/02/2025 19:51:19 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-1000
12/02/2025 19:51:40 - INFO - training.fm_trainer - Step 2010/210000 (0.96%): loss=4.4230, lr=3.72e-05, step_time=1870.9ms, ETA 4d 10h
12/02/2025 19:51:59 - INFO - training.fm_trainer - Step 2020/210000 (0.96%): loss=1.3609, lr=3.78e-05, step_time=1847.0ms, ETA 4d 10h
12/02/2025 19:52:18 - INFO - training.fm_trainer - Step 2030/210000 (0.97%): loss=1.0392, lr=3.78e-05, step_time=1890.4ms, ETA 4d 11h
12/02/2025 19:52:36 - INFO - training.fm_trainer - Step 2040/210000 (0.97%): loss=1.3008, lr=3.78e-05, step_time=1878.6ms, ETA 4d 11h
12/02/2025 19:52:55 - INFO - training.fm_trainer - Step 2050/210000 (0.98%): loss=9.9087, lr=3.84e-05, step_time=1873.7ms, ETA 4d 11h
12/02/2025 19:52:55 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:53:21 - INFO - training.fm_trainer - Eval Step 2050: loss=3.4930, ppl=32.89
12/02/2025 19:53:40 - INFO - training.fm_trainer - Step 2060/210000 (0.98%): loss=0.5110, lr=3.84e-05, step_time=1891.0ms, ETA 4d 11h
12/02/2025 19:53:58 - INFO - training.fm_trainer - Step 2070/210000 (0.99%): loss=2.7301, lr=3.84e-05, step_time=1850.5ms, ETA 4d 11h
12/02/2025 19:54:17 - INFO - training.fm_trainer - Step 2080/210000 (0.99%): loss=1.2404, lr=3.90e-05, grad_norm=0.14, step_time=1853.7ms, ETA 4d 11h
12/02/2025 19:54:36 - INFO - training.fm_trainer - Step 2090/210000 (1.00%): loss=11.0743, lr=3.90e-05, step_time=1807.7ms, ETA 4d 11h
12/02/2025 19:54:54 - INFO - training.fm_trainer - Step 2100/210000 (1.00%): loss=1.5495, lr=3.90e-05, step_time=1842.2ms, ETA 4d 11h
12/02/2025 19:54:54 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:55:21 - INFO - training.fm_trainer - Eval Step 2100: loss=3.5448, ppl=34.63
12/02/2025 19:55:39 - INFO - training.fm_trainer - Step 2110/210000 (1.00%): loss=1.3925, lr=3.90e-05, step_time=1837.7ms, ETA 4d 10h
12/02/2025 19:55:58 - INFO - training.fm_trainer - Step 2120/210000 (1.01%): loss=3.9606, lr=3.96e-05, step_time=1847.4ms, ETA 4d 10h
12/02/2025 19:56:17 - INFO - training.fm_trainer - Step 2130/210000 (1.01%): loss=4.0193, lr=3.96e-05, step_time=1855.5ms, ETA 4d 10h
12/02/2025 19:56:35 - INFO - training.fm_trainer - Step 2140/210000 (1.02%): loss=2.2851, lr=3.96e-05, step_time=1906.3ms, ETA 4d 11h
12/02/2025 19:56:53 - INFO - training.fm_trainer - Step 2150/210000 (1.02%): loss=5.3243, lr=4.02e-05, step_time=1829.0ms, ETA 4d 11h
12/02/2025 19:56:53 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:57:21 - INFO - training.fm_trainer - Eval Step 2150: loss=3.6229, ppl=37.44
12/02/2025 19:57:39 - INFO - training.fm_trainer - Step 2160/210000 (1.03%): loss=2.0437, lr=4.02e-05, step_time=1826.6ms, ETA 4d 10h
12/02/2025 19:57:58 - INFO - training.fm_trainer - Step 2170/210000 (1.03%): loss=0.7477, lr=4.02e-05, step_time=1828.1ms, ETA 4d 10h
12/02/2025 19:58:16 - INFO - training.fm_trainer - Step 2180/210000 (1.04%): loss=0.8043, lr=4.08e-05, step_time=1842.4ms, ETA 4d 10h
12/02/2025 19:58:34 - INFO - training.fm_trainer - Step 2190/210000 (1.04%): loss=1.2302, lr=4.08e-05, step_time=1852.5ms, ETA 4d 10h
12/02/2025 19:58:53 - INFO - training.fm_trainer - Step 2200/210000 (1.05%): loss=1.0704, lr=4.08e-05, step_time=1810.4ms, ETA 4d 10h
12/02/2025 19:58:53 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 19:59:19 - INFO - training.fm_trainer - Eval Step 2200: loss=3.5577, ppl=35.08
12/02/2025 19:59:37 - INFO - training.fm_trainer - Step 2210/210000 (1.05%): loss=7.8332, lr=4.14e-05, step_time=1833.7ms, ETA 4d 10h
12/02/2025 19:59:55 - INFO - training.fm_trainer - Step 2220/210000 (1.06%): loss=1.4404, lr=4.14e-05, step_time=1809.8ms, ETA 4d 10h
12/02/2025 20:00:14 - INFO - training.fm_trainer - Step 2230/210000 (1.06%): loss=4.9845, lr=4.14e-05, step_time=1873.3ms, ETA 4d 10h
12/02/2025 20:00:32 - INFO - training.fm_trainer - Step 2240/210000 (1.07%): loss=0.8411, lr=4.20e-05, grad_norm=0.36, step_time=1865.2ms, ETA 4d 10h
12/02/2025 20:00:51 - INFO - training.fm_trainer - Step 2250/210000 (1.07%): loss=1.6344, lr=4.20e-05, step_time=1835.8ms, ETA 4d 10h
12/02/2025 20:00:51 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:01:17 - INFO - training.fm_trainer - Eval Step 2250: loss=3.5001, ppl=33.12
12/02/2025 20:01:35 - INFO - training.fm_trainer - Step 2260/210000 (1.08%): loss=2.2605, lr=4.20e-05, step_time=1872.1ms, ETA 4d 10h
12/02/2025 20:01:54 - INFO - training.fm_trainer - Step 2270/210000 (1.08%): loss=4.3440, lr=4.20e-05, step_time=1838.4ms, ETA 4d 10h
12/02/2025 20:02:13 - INFO - training.fm_trainer - Step 2280/210000 (1.09%): loss=0.4090, lr=4.26e-05, step_time=1927.6ms, ETA 4d 11h
12/02/2025 20:02:31 - INFO - training.fm_trainer - Step 2290/210000 (1.09%): loss=1.0091, lr=4.26e-05, step_time=1883.3ms, ETA 4d 11h
12/02/2025 20:02:50 - INFO - training.fm_trainer - Step 2300/210000 (1.10%): loss=10.2959, lr=4.26e-05, step_time=1864.7ms, ETA 4d 11h
12/02/2025 20:02:50 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:03:17 - INFO - training.fm_trainer - Eval Step 2300: loss=3.6251, ppl=37.53
12/02/2025 20:03:36 - INFO - training.fm_trainer - Step 2310/210000 (1.10%): loss=10.0881, lr=4.32e-05, step_time=1851.0ms, ETA 4d 11h
12/02/2025 20:03:55 - INFO - training.fm_trainer - Step 2320/210000 (1.10%): loss=0.9234, lr=4.32e-05, step_time=1885.5ms, ETA 4d 11h
12/02/2025 20:04:13 - INFO - training.fm_trainer - Step 2330/210000 (1.11%): loss=1.2122, lr=4.32e-05, step_time=1813.6ms, ETA 4d 11h
12/02/2025 20:04:32 - INFO - training.fm_trainer - Step 2340/210000 (1.11%): loss=1.2175, lr=4.38e-05, step_time=1818.3ms, ETA 4d 10h
12/02/2025 20:04:50 - INFO - training.fm_trainer - Step 2350/210000 (1.12%): loss=1.8322, lr=4.38e-05, step_time=1871.4ms, ETA 4d 10h
12/02/2025 20:04:50 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:05:18 - INFO - training.fm_trainer - Eval Step 2350: loss=3.5723, ppl=35.60
12/02/2025 20:05:36 - INFO - training.fm_trainer - Step 2360/210000 (1.12%): loss=4.5525, lr=4.38e-05, step_time=1834.6ms, ETA 4d 10h
12/02/2025 20:05:55 - INFO - training.fm_trainer - Step 2370/210000 (1.13%): loss=1.3795, lr=4.44e-05, step_time=1820.9ms, ETA 4d 10h
12/02/2025 20:06:14 - INFO - training.fm_trainer - Step 2380/210000 (1.13%): loss=3.2008, lr=4.44e-05, step_time=1899.1ms, ETA 4d 10h
12/02/2025 20:06:32 - INFO - training.fm_trainer - Step 2390/210000 (1.14%): loss=1.3676, lr=4.44e-05, step_time=1827.1ms, ETA 4d 10h
12/02/2025 20:06:51 - INFO - training.fm_trainer - Step 2400/210000 (1.14%): loss=8.8997, lr=4.50e-05, grad_norm=0.21, step_time=1881.0ms, ETA 4d 10h
12/02/2025 20:06:51 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:07:16 - INFO - training.fm_trainer - Eval Step 2400: loss=3.6560, ppl=38.71
12/02/2025 20:07:35 - INFO - training.fm_trainer - Step 2410/210000 (1.15%): loss=1.6573, lr=4.50e-05, step_time=1822.3ms, ETA 4d 10h
12/02/2025 20:07:53 - INFO - training.fm_trainer - Step 2420/210000 (1.15%): loss=3.8227, lr=4.50e-05, step_time=1830.0ms, ETA 4d 10h
12/02/2025 20:08:12 - INFO - training.fm_trainer - Step 2430/210000 (1.16%): loss=1.3770, lr=4.50e-05, step_time=1857.1ms, ETA 4d 10h
12/02/2025 20:08:30 - INFO - training.fm_trainer - Step 2440/210000 (1.16%): loss=1.7013, lr=4.56e-05, step_time=1849.0ms, ETA 4d 10h
12/02/2025 20:08:49 - INFO - training.fm_trainer - Step 2450/210000 (1.17%): loss=1.8912, lr=4.56e-05, step_time=1848.7ms, ETA 4d 10h
12/02/2025 20:08:49 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:09:16 - INFO - training.fm_trainer - Eval Step 2450: loss=3.8291, ppl=46.02
12/02/2025 20:09:35 - INFO - training.fm_trainer - Step 2460/210000 (1.17%): loss=3.2644, lr=4.56e-05, step_time=1831.0ms, ETA 4d 10h
12/02/2025 20:09:53 - INFO - training.fm_trainer - Step 2470/210000 (1.18%): loss=9.1436, lr=4.62e-05, step_time=1833.3ms, ETA 4d 10h
12/02/2025 20:10:11 - INFO - training.fm_trainer - Step 2480/210000 (1.18%): loss=5.2945, lr=4.62e-05, step_time=1842.1ms, ETA 4d 10h
12/02/2025 20:10:31 - INFO - training.fm_trainer - Step 2490/210000 (1.19%): loss=1.4001, lr=4.62e-05, step_time=1807.4ms, ETA 4d 10h
12/02/2025 20:10:49 - INFO - training.fm_trainer - Step 2500/210000 (1.19%): loss=6.8055, lr=4.68e-05, step_time=1810.5ms, ETA 4d 9h
12/02/2025 20:10:49 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:11:15 - INFO - training.fm_trainer - Eval Step 2500: loss=3.8287, ppl=46.00
12/02/2025 20:11:15 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-2500
12/02/2025 20:11:15 - INFO - accelerate.accelerator - Saving FSDP model
12/02/2025 20:11:16 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-2500/pytorch_model_fsdp_0
12/02/2025 20:11:28 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-2500/pytorch_model_fsdp_0
12/02/2025 20:11:28 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-2500
12/02/2025 20:11:28 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/02/2025 20:11:31 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-2500/optimizer_0
12/02/2025 20:11:47 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-2500/optimizer_0
12/02/2025 20:11:47 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-2500
12/02/2025 20:11:47 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-2500/scheduler.bin
12/02/2025 20:11:47 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-2500/sampler.bin
12/02/2025 20:11:47 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-2500/sampler_1.bin
12/02/2025 20:11:47 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-2500/random_states_0.pkl
12/02/2025 20:11:47 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-2500
12/02/2025 20:11:47 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-1500
12/02/2025 20:12:09 - INFO - training.fm_trainer - Step 2510/210000 (1.20%): loss=1.8939, lr=4.68e-05, step_time=2013.0ms, ETA 4d 10h
12/02/2025 20:12:27 - INFO - training.fm_trainer - Step 2520/210000 (1.20%): loss=3.4596, lr=4.68e-05, step_time=1844.1ms, ETA 4d 10h
12/02/2025 20:12:46 - INFO - training.fm_trainer - Step 2530/210000 (1.20%): loss=1.2607, lr=4.74e-05, step_time=1841.4ms, ETA 4d 10h
12/02/2025 20:13:04 - INFO - training.fm_trainer - Step 2540/210000 (1.21%): loss=1.2564, lr=4.74e-05, step_time=1855.3ms, ETA 4d 10h
12/02/2025 20:13:23 - INFO - training.fm_trainer - Step 2550/210000 (1.21%): loss=1.5616, lr=4.74e-05, step_time=1852.2ms, ETA 4d 10h
12/02/2025 20:13:23 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:13:49 - INFO - training.fm_trainer - Eval Step 2550: loss=3.4799, ppl=32.46
12/02/2025 20:14:08 - INFO - training.fm_trainer - Step 2560/210000 (1.22%): loss=4.3565, lr=4.80e-05, grad_norm=0.17, step_time=1859.5ms, ETA 4d 10h
12/02/2025 20:14:27 - INFO - training.fm_trainer - Step 2570/210000 (1.22%): loss=5.3926, lr=4.80e-05, step_time=1865.2ms, ETA 4d 10h
12/02/2025 20:14:45 - INFO - training.fm_trainer - Step 2580/210000 (1.23%): loss=8.7241, lr=4.80e-05, step_time=1820.9ms, ETA 4d 10h
12/02/2025 20:15:04 - INFO - training.fm_trainer - Step 2590/210000 (1.23%): loss=1.3894, lr=4.80e-05, step_time=1847.5ms, ETA 4d 10h
12/02/2025 20:15:23 - INFO - training.fm_trainer - Step 2600/210000 (1.24%): loss=4.1817, lr=4.86e-05, step_time=1886.6ms, ETA 4d 10h
12/02/2025 20:15:23 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:15:50 - INFO - training.fm_trainer - Eval Step 2600: loss=3.4721, ppl=32.20
12/02/2025 20:16:09 - INFO - training.fm_trainer - Step 2610/210000 (1.24%): loss=2.1025, lr=4.86e-05, step_time=1842.6ms, ETA 4d 10h
12/02/2025 20:16:27 - INFO - training.fm_trainer - Step 2620/210000 (1.25%): loss=4.1298, lr=4.86e-05, step_time=1848.9ms, ETA 4d 10h
12/02/2025 20:16:46 - INFO - training.fm_trainer - Step 2630/210000 (1.25%): loss=8.9742, lr=4.92e-05, step_time=1829.6ms, ETA 4d 10h
12/02/2025 20:17:04 - INFO - training.fm_trainer - Step 2640/210000 (1.26%): loss=3.1362, lr=4.92e-05, step_time=1822.8ms, ETA 4d 10h
12/02/2025 20:17:23 - INFO - training.fm_trainer - Step 2650/210000 (1.26%): loss=6.2450, lr=4.92e-05, step_time=1852.6ms, ETA 4d 10h
12/02/2025 20:17:23 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:17:49 - INFO - training.fm_trainer - Eval Step 2650: loss=3.9051, ppl=49.65
12/02/2025 20:18:07 - INFO - training.fm_trainer - Step 2660/210000 (1.27%): loss=10.2187, lr=4.98e-05, step_time=1829.1ms, ETA 4d 10h
12/02/2025 20:18:26 - INFO - training.fm_trainer - Step 2670/210000 (1.27%): loss=6.0503, lr=4.98e-05, step_time=1818.9ms, ETA 4d 10h
12/02/2025 20:18:44 - INFO - training.fm_trainer - Step 2680/210000 (1.28%): loss=0.6447, lr=4.98e-05, step_time=1835.5ms, ETA 4d 10h
12/02/2025 20:19:02 - INFO - training.fm_trainer - Step 2690/210000 (1.28%): loss=6.7438, lr=5.04e-05, step_time=1821.9ms, ETA 4d 10h
12/02/2025 20:19:21 - INFO - training.fm_trainer - Step 2700/210000 (1.29%): loss=11.5377, lr=5.04e-05, step_time=1818.6ms, ETA 4d 9h
12/02/2025 20:19:21 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:19:47 - INFO - training.fm_trainer - Eval Step 2700: loss=4.0298, ppl=56.25
12/02/2025 20:20:05 - INFO - training.fm_trainer - Step 2710/210000 (1.29%): loss=0.5623, lr=5.04e-05, step_time=1846.1ms, ETA 4d 9h
12/02/2025 20:20:24 - INFO - training.fm_trainer - Step 2720/210000 (1.30%): loss=0.3823, lr=5.10e-05, grad_norm=0.26, step_time=2006.9ms, ETA 4d 10h
12/02/2025 20:20:42 - INFO - training.fm_trainer - Step 2730/210000 (1.30%): loss=0.4341, lr=5.10e-05, step_time=2045.7ms, ETA 4d 11h
12/02/2025 20:21:01 - INFO - training.fm_trainer - Step 2740/210000 (1.30%): loss=6.7213, lr=5.10e-05, step_time=1878.6ms, ETA 4d 11h
12/02/2025 20:21:20 - INFO - training.fm_trainer - Step 2750/210000 (1.31%): loss=0.6468, lr=5.10e-05, step_time=1862.7ms, ETA 4d 11h
12/02/2025 20:21:20 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:21:47 - INFO - training.fm_trainer - Eval Step 2750: loss=3.5180, ppl=33.72
12/02/2025 20:22:05 - INFO - training.fm_trainer - Step 2760/210000 (1.31%): loss=0.8095, lr=5.16e-05, step_time=1832.7ms, ETA 4d 11h
12/02/2025 20:22:24 - INFO - training.fm_trainer - Step 2770/210000 (1.32%): loss=10.4238, lr=5.16e-05, step_time=1825.1ms, ETA 4d 11h
12/02/2025 20:22:42 - INFO - training.fm_trainer - Step 2780/210000 (1.32%): loss=4.5812, lr=5.16e-05, step_time=1835.5ms, ETA 4d 11h
12/02/2025 20:23:01 - INFO - training.fm_trainer - Step 2790/210000 (1.33%): loss=1.8075, lr=5.22e-05, step_time=1838.9ms, ETA 4d 11h
12/02/2025 20:23:20 - INFO - training.fm_trainer - Step 2800/210000 (1.33%): loss=5.2522, lr=5.22e-05, step_time=1842.2ms, ETA 4d 10h
12/02/2025 20:23:20 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:23:46 - INFO - training.fm_trainer - Eval Step 2800: loss=3.7025, ppl=40.55
12/02/2025 20:24:05 - INFO - training.fm_trainer - Step 2810/210000 (1.34%): loss=1.0506, lr=5.22e-05, step_time=1832.3ms, ETA 4d 10h
12/02/2025 20:24:24 - INFO - training.fm_trainer - Step 2820/210000 (1.34%): loss=2.6583, lr=5.28e-05, step_time=1855.8ms, ETA 4d 10h
12/02/2025 20:24:42 - INFO - training.fm_trainer - Step 2830/210000 (1.35%): loss=1.5243, lr=5.28e-05, step_time=1884.1ms, ETA 4d 10h
12/02/2025 20:25:00 - INFO - training.fm_trainer - Step 2840/210000 (1.35%): loss=2.2810, lr=5.28e-05, step_time=1827.4ms, ETA 4d 10h
12/02/2025 20:25:19 - INFO - training.fm_trainer - Step 2850/210000 (1.36%): loss=5.2414, lr=5.34e-05, step_time=1921.2ms, ETA 4d 11h
12/02/2025 20:25:19 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:25:46 - INFO - training.fm_trainer - Eval Step 2850: loss=3.6483, ppl=38.41
12/02/2025 20:26:05 - INFO - training.fm_trainer - Step 2860/210000 (1.36%): loss=9.0306, lr=5.34e-05, step_time=1874.8ms, ETA 4d 11h
12/02/2025 20:26:23 - INFO - training.fm_trainer - Step 2870/210000 (1.37%): loss=1.0141, lr=5.34e-05, step_time=1825.2ms, ETA 4d 10h
12/02/2025 20:26:42 - INFO - training.fm_trainer - Step 2880/210000 (1.37%): loss=1.8501, lr=5.40e-05, grad_norm=0.10, step_time=1843.4ms, ETA 4d 10h
12/02/2025 20:27:00 - INFO - training.fm_trainer - Step 2890/210000 (1.38%): loss=4.5427, lr=5.40e-05, step_time=1864.8ms, ETA 4d 10h
12/02/2025 20:27:19 - INFO - training.fm_trainer - Step 2900/210000 (1.38%): loss=0.7699, lr=5.40e-05, step_time=1814.2ms, ETA 4d 10h
12/02/2025 20:27:19 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:27:45 - INFO - training.fm_trainer - Eval Step 2900: loss=3.5017, ppl=33.17
12/02/2025 20:28:03 - INFO - training.fm_trainer - Step 2910/210000 (1.39%): loss=0.5923, lr=5.40e-05, step_time=1845.8ms, ETA 4d 10h
12/02/2025 20:28:21 - INFO - training.fm_trainer - Step 2920/210000 (1.39%): loss=6.5877, lr=5.46e-05, step_time=1819.3ms, ETA 4d 10h
12/02/2025 20:28:40 - INFO - training.fm_trainer - Step 2930/210000 (1.40%): loss=2.8943, lr=5.46e-05, step_time=1826.9ms, ETA 4d 10h
12/02/2025 20:28:58 - INFO - training.fm_trainer - Step 2940/210000 (1.40%): loss=5.1500, lr=5.46e-05, step_time=1933.6ms, ETA 4d 10h
12/02/2025 20:29:17 - INFO - training.fm_trainer - Step 2950/210000 (1.40%): loss=13.8092, lr=5.52e-05, step_time=1829.6ms, ETA 4d 10h
12/02/2025 20:29:17 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:29:43 - INFO - training.fm_trainer - Eval Step 2950: loss=3.6092, ppl=36.94
12/02/2025 20:30:01 - INFO - training.fm_trainer - Step 2960/210000 (1.41%): loss=7.2238, lr=5.52e-05, step_time=1846.2ms, ETA 4d 10h
12/02/2025 20:30:20 - INFO - training.fm_trainer - Step 2970/210000 (1.41%): loss=0.5399, lr=5.52e-05, step_time=1822.6ms, ETA 4d 10h
12/02/2025 20:30:38 - INFO - training.fm_trainer - Step 2980/210000 (1.42%): loss=0.8032, lr=5.58e-05, step_time=1838.7ms, ETA 4d 10h
12/02/2025 20:30:56 - INFO - training.fm_trainer - Step 2990/210000 (1.42%): loss=0.5637, lr=5.58e-05, step_time=1835.3ms, ETA 4d 10h
12/02/2025 20:31:15 - INFO - training.fm_trainer - Step 3000/210000 (1.43%): loss=2.2385, lr=5.58e-05, step_time=1878.7ms, ETA 4d 10h
12/02/2025 20:31:15 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:31:41 - INFO - training.fm_trainer - Eval Step 3000: loss=3.6352, ppl=37.91
12/02/2025 20:31:41 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-3000
12/02/2025 20:31:41 - INFO - accelerate.accelerator - Saving FSDP model
12/02/2025 20:31:43 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-3000/pytorch_model_fsdp_0
12/02/2025 20:31:51 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-3000/pytorch_model_fsdp_0
12/02/2025 20:31:51 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-3000
12/02/2025 20:31:51 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/02/2025 20:31:53 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-3000/optimizer_0
12/02/2025 20:32:12 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-3000/optimizer_0
12/02/2025 20:32:12 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-3000
12/02/2025 20:32:13 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-3000/scheduler.bin
12/02/2025 20:32:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-3000/sampler.bin
12/02/2025 20:32:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-3000/sampler_1.bin
12/02/2025 20:32:13 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-3000/random_states_0.pkl
12/02/2025 20:32:13 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-3000
12/02/2025 20:32:13 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-2000
12/02/2025 20:32:34 - INFO - training.fm_trainer - Step 3010/210000 (1.43%): loss=1.0847, lr=5.64e-05, step_time=2183.2ms, ETA 4d 12h
12/02/2025 20:32:52 - INFO - training.fm_trainer - Step 3020/210000 (1.44%): loss=5.1095, lr=5.64e-05, step_time=1818.2ms, ETA 4d 11h
12/02/2025 20:33:11 - INFO - training.fm_trainer - Step 3030/210000 (1.44%): loss=4.0236, lr=5.64e-05, step_time=1825.0ms, ETA 4d 11h
12/02/2025 20:33:29 - INFO - training.fm_trainer - Step 3040/210000 (1.45%): loss=0.9422, lr=5.70e-05, grad_norm=0.05, step_time=1846.2ms, ETA 4d 11h
12/02/2025 20:33:48 - INFO - training.fm_trainer - Step 3050/210000 (1.45%): loss=6.4260, lr=5.70e-05, step_time=1823.4ms, ETA 4d 11h
12/02/2025 20:33:48 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:34:14 - INFO - training.fm_trainer - Eval Step 3050: loss=3.6385, ppl=38.03
12/02/2025 20:34:32 - INFO - training.fm_trainer - Step 3060/210000 (1.46%): loss=8.4563, lr=5.70e-05, step_time=1833.4ms, ETA 4d 11h
12/02/2025 20:34:51 - INFO - training.fm_trainer - Step 3070/210000 (1.46%): loss=1.8092, lr=5.70e-05, step_time=1832.2ms, ETA 4d 10h
12/02/2025 20:35:09 - INFO - training.fm_trainer - Step 3080/210000 (1.47%): loss=1.1921, lr=5.76e-05, step_time=1821.7ms, ETA 4d 10h
12/02/2025 20:35:27 - INFO - training.fm_trainer - Step 3090/210000 (1.47%): loss=1.9305, lr=5.76e-05, step_time=1814.8ms, ETA 4d 10h
12/02/2025 20:35:46 - INFO - training.fm_trainer - Step 3100/210000 (1.48%): loss=1.6365, lr=5.76e-05, step_time=1827.8ms, ETA 4d 10h
12/02/2025 20:35:46 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:36:13 - INFO - training.fm_trainer - Eval Step 3100: loss=3.5375, ppl=34.38
12/02/2025 20:36:32 - INFO - training.fm_trainer - Step 3110/210000 (1.48%): loss=7.0771, lr=5.82e-05, step_time=1850.0ms, ETA 4d 10h
12/02/2025 20:36:51 - INFO - training.fm_trainer - Step 3120/210000 (1.49%): loss=1.5225, lr=5.82e-05, step_time=1831.1ms, ETA 4d 10h
12/02/2025 20:37:10 - INFO - training.fm_trainer - Step 3130/210000 (1.49%): loss=6.4888, lr=5.82e-05, step_time=1858.4ms, ETA 4d 10h
12/02/2025 20:37:29 - INFO - training.fm_trainer - Step 3140/210000 (1.50%): loss=2.6327, lr=5.88e-05, step_time=1902.1ms, ETA 4d 10h
12/02/2025 20:37:47 - INFO - training.fm_trainer - Step 3150/210000 (1.50%): loss=1.8691, lr=5.88e-05, step_time=1844.0ms, ETA 4d 10h
12/02/2025 20:37:47 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:38:13 - INFO - training.fm_trainer - Eval Step 3150: loss=3.4849, ppl=32.62
12/02/2025 20:38:32 - INFO - training.fm_trainer - Step 3160/210000 (1.50%): loss=2.0077, lr=5.88e-05, step_time=1835.7ms, ETA 4d 10h
12/02/2025 20:38:50 - INFO - training.fm_trainer - Step 3170/210000 (1.51%): loss=2.7999, lr=5.94e-05, step_time=1833.8ms, ETA 4d 10h
12/02/2025 20:39:09 - INFO - training.fm_trainer - Step 3180/210000 (1.51%): loss=1.3141, lr=5.94e-05, step_time=1822.0ms, ETA 4d 10h
12/02/2025 20:39:27 - INFO - training.fm_trainer - Step 3190/210000 (1.52%): loss=1.3425, lr=5.94e-05, step_time=1876.0ms, ETA 4d 10h
12/02/2025 20:39:46 - INFO - training.fm_trainer - Step 3200/210000 (1.52%): loss=8.4427, lr=6.00e-05, grad_norm=0.20, step_time=1836.7ms, ETA 4d 10h
12/02/2025 20:39:46 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:40:12 - INFO - training.fm_trainer - Eval Step 3200: loss=3.5442, ppl=34.61
12/02/2025 20:40:31 - INFO - training.fm_trainer - Step 3210/210000 (1.53%): loss=4.6023, lr=6.00e-05, step_time=1909.4ms, ETA 4d 10h
12/02/2025 20:40:50 - INFO - training.fm_trainer - Step 3220/210000 (1.53%): loss=0.9806, lr=6.00e-05, step_time=1954.6ms, ETA 4d 11h
12/02/2025 20:41:08 - INFO - training.fm_trainer - Step 3230/210000 (1.54%): loss=5.5662, lr=6.00e-05, step_time=1872.3ms, ETA 4d 11h
12/02/2025 20:41:27 - INFO - training.fm_trainer - Step 3240/210000 (1.54%): loss=1.7501, lr=6.06e-05, step_time=1838.5ms, ETA 4d 10h
12/02/2025 20:41:45 - INFO - training.fm_trainer - Step 3250/210000 (1.55%): loss=1.2875, lr=6.06e-05, step_time=1816.5ms, ETA 4d 10h
12/02/2025 20:41:45 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:42:12 - INFO - training.fm_trainer - Eval Step 3250: loss=3.3878, ppl=29.60
12/02/2025 20:42:31 - INFO - training.fm_trainer - Step 3260/210000 (1.55%): loss=2.0069, lr=6.06e-05, step_time=1819.4ms, ETA 4d 10h
12/02/2025 20:42:49 - INFO - training.fm_trainer - Step 3270/210000 (1.56%): loss=1.1213, lr=6.12e-05, step_time=1814.3ms, ETA 4d 10h
12/02/2025 20:43:08 - INFO - training.fm_trainer - Step 3280/210000 (1.56%): loss=4.5515, lr=6.12e-05, step_time=1818.7ms, ETA 4d 10h
12/02/2025 20:43:26 - INFO - training.fm_trainer - Step 3290/210000 (1.57%): loss=1.2310, lr=6.12e-05, step_time=1836.1ms, ETA 4d 9h
12/02/2025 20:43:44 - INFO - training.fm_trainer - Step 3300/210000 (1.57%): loss=3.8800, lr=6.18e-05, step_time=1826.8ms, ETA 4d 9h
12/02/2025 20:43:44 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:44:11 - INFO - training.fm_trainer - Eval Step 3300: loss=3.7694, ppl=43.36
12/02/2025 20:44:29 - INFO - training.fm_trainer - Step 3310/210000 (1.58%): loss=0.7904, lr=6.18e-05, step_time=2007.0ms, ETA 4d 10h
12/02/2025 20:44:48 - INFO - training.fm_trainer - Step 3320/210000 (1.58%): loss=7.6978, lr=6.18e-05, step_time=1831.3ms, ETA 4d 10h
12/02/2025 20:45:06 - INFO - training.fm_trainer - Step 3330/210000 (1.59%): loss=8.6050, lr=6.24e-05, step_time=1828.8ms, ETA 4d 10h
12/02/2025 20:45:25 - INFO - training.fm_trainer - Step 3340/210000 (1.59%): loss=1.1727, lr=6.24e-05, step_time=1811.8ms, ETA 4d 10h
12/02/2025 20:45:43 - INFO - training.fm_trainer - Step 3350/210000 (1.60%): loss=3.3291, lr=6.24e-05, step_time=1832.1ms, ETA 4d 10h
12/02/2025 20:45:43 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:46:11 - INFO - training.fm_trainer - Eval Step 3350: loss=3.9351, ppl=51.17
12/02/2025 20:46:29 - INFO - training.fm_trainer - Step 3360/210000 (1.60%): loss=1.5072, lr=6.30e-05, grad_norm=0.23, step_time=1852.5ms, ETA 4d 10h
12/02/2025 20:46:48 - INFO - training.fm_trainer - Step 3370/210000 (1.60%): loss=2.1776, lr=6.30e-05, step_time=1848.8ms, ETA 4d 10h
12/02/2025 20:47:06 - INFO - training.fm_trainer - Step 3380/210000 (1.61%): loss=1.8505, lr=6.30e-05, step_time=1828.1ms, ETA 4d 9h
12/02/2025 20:47:24 - INFO - training.fm_trainer - Step 3390/210000 (1.61%): loss=3.0566, lr=6.30e-05, step_time=1852.3ms, ETA 4d 10h
12/02/2025 20:47:43 - INFO - training.fm_trainer - Step 3400/210000 (1.62%): loss=4.1829, lr=6.36e-05, step_time=1842.1ms, ETA 4d 9h
12/02/2025 20:47:43 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:48:09 - INFO - training.fm_trainer - Eval Step 3400: loss=3.4255, ppl=30.74
12/02/2025 20:48:27 - INFO - training.fm_trainer - Step 3410/210000 (1.62%): loss=1.2643, lr=6.36e-05, step_time=1830.7ms, ETA 4d 9h
12/02/2025 20:48:46 - INFO - training.fm_trainer - Step 3420/210000 (1.63%): loss=5.3330, lr=6.36e-05, step_time=1834.3ms, ETA 4d 9h
12/02/2025 20:49:05 - INFO - training.fm_trainer - Step 3430/210000 (1.63%): loss=1.7232, lr=6.42e-05, step_time=2354.0ms, ETA 4d 12h
12/02/2025 20:49:23 - INFO - training.fm_trainer - Step 3440/210000 (1.64%): loss=4.6393, lr=6.42e-05, step_time=1839.9ms, ETA 4d 12h
12/02/2025 20:49:42 - INFO - training.fm_trainer - Step 3450/210000 (1.64%): loss=2.3337, lr=6.42e-05, step_time=1828.7ms, ETA 4d 12h
12/02/2025 20:49:42 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:50:07 - INFO - training.fm_trainer - Eval Step 3450: loss=3.4712, ppl=32.18
12/02/2025 20:50:26 - INFO - training.fm_trainer - Step 3460/210000 (1.65%): loss=3.0261, lr=6.48e-05, step_time=1857.4ms, ETA 4d 11h
12/02/2025 20:50:44 - INFO - training.fm_trainer - Step 3470/210000 (1.65%): loss=4.7267, lr=6.48e-05, step_time=1830.5ms, ETA 4d 11h
12/02/2025 20:51:03 - INFO - training.fm_trainer - Step 3480/210000 (1.66%): loss=1.7448, lr=6.48e-05, step_time=1842.2ms, ETA 4d 11h
12/02/2025 20:51:21 - INFO - training.fm_trainer - Step 3490/210000 (1.66%): loss=2.7342, lr=6.54e-05, step_time=1856.5ms, ETA 4d 11h
12/02/2025 20:51:40 - INFO - training.fm_trainer - Step 3500/210000 (1.67%): loss=4.2115, lr=6.54e-05, step_time=1830.3ms, ETA 4d 11h
12/02/2025 20:51:40 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:52:07 - INFO - training.fm_trainer - Eval Step 3500: loss=3.6459, ppl=38.32
12/02/2025 20:52:07 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-3500
12/02/2025 20:52:07 - INFO - accelerate.accelerator - Saving FSDP model
12/02/2025 20:52:08 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-3500/pytorch_model_fsdp_0
12/02/2025 20:52:17 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-3500/pytorch_model_fsdp_0
12/02/2025 20:52:17 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-3500
12/02/2025 20:52:17 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/02/2025 20:52:20 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-3500/optimizer_0
12/02/2025 20:52:36 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-3500/optimizer_0
12/02/2025 20:52:36 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-3500
12/02/2025 20:52:36 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-3500/scheduler.bin
12/02/2025 20:52:36 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-3500/sampler.bin
12/02/2025 20:52:36 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-3500/sampler_1.bin
12/02/2025 20:52:36 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-3500/random_states_0.pkl
12/02/2025 20:52:36 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-3500
12/02/2025 20:52:36 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-2500
12/02/2025 20:52:57 - INFO - training.fm_trainer - Step 3510/210000 (1.67%): loss=2.3421, lr=6.54e-05, step_time=1891.0ms, ETA 4d 11h
12/02/2025 20:53:15 - INFO - training.fm_trainer - Step 3520/210000 (1.68%): loss=1.5835, lr=6.60e-05, grad_norm=0.04, step_time=1867.7ms, ETA 4d 11h
12/02/2025 20:53:34 - INFO - training.fm_trainer - Step 3530/210000 (1.68%): loss=1.4310, lr=6.60e-05, step_time=1807.4ms, ETA 4d 10h
12/02/2025 20:53:52 - INFO - training.fm_trainer - Step 3540/210000 (1.69%): loss=0.7554, lr=6.60e-05, step_time=1813.7ms, ETA 4d 10h
12/02/2025 20:54:11 - INFO - training.fm_trainer - Step 3550/210000 (1.69%): loss=5.5973, lr=6.60e-05, step_time=1829.4ms, ETA 4d 10h
12/02/2025 20:54:11 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:54:38 - INFO - training.fm_trainer - Eval Step 3550: loss=3.5101, ppl=33.45
12/02/2025 20:54:56 - INFO - training.fm_trainer - Step 3560/210000 (1.70%): loss=3.6290, lr=6.66e-05, step_time=1810.1ms, ETA 4d 10h
12/02/2025 20:55:15 - INFO - training.fm_trainer - Step 3570/210000 (1.70%): loss=4.4265, lr=6.66e-05, step_time=1839.4ms, ETA 4d 10h
12/02/2025 20:55:33 - INFO - training.fm_trainer - Step 3580/210000 (1.70%): loss=1.0556, lr=6.66e-05, step_time=1828.4ms, ETA 4d 9h
12/02/2025 20:55:51 - INFO - training.fm_trainer - Step 3590/210000 (1.71%): loss=2.4913, lr=6.72e-05, step_time=1834.9ms, ETA 4d 9h
12/02/2025 20:56:10 - INFO - training.fm_trainer - Step 3600/210000 (1.71%): loss=1.5388, lr=6.72e-05, step_time=1854.9ms, ETA 4d 9h
12/02/2025 20:56:10 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:56:37 - INFO - training.fm_trainer - Eval Step 3600: loss=3.6071, ppl=36.86
12/02/2025 20:56:55 - INFO - training.fm_trainer - Step 3610/210000 (1.72%): loss=1.0406, lr=6.72e-05, step_time=1829.7ms, ETA 4d 9h
12/02/2025 20:57:14 - INFO - training.fm_trainer - Step 3620/210000 (1.72%): loss=1.2938, lr=6.78e-05, step_time=1835.3ms, ETA 4d 9h
12/02/2025 20:57:32 - INFO - training.fm_trainer - Step 3630/210000 (1.73%): loss=8.4751, lr=6.78e-05, step_time=1827.7ms, ETA 4d 9h
12/02/2025 20:57:50 - INFO - training.fm_trainer - Step 3640/210000 (1.73%): loss=1.2349, lr=6.78e-05, step_time=1826.1ms, ETA 4d 9h
12/02/2025 20:58:09 - INFO - training.fm_trainer - Step 3650/210000 (1.74%): loss=0.7219, lr=6.84e-05, step_time=1838.8ms, ETA 4d 9h
12/02/2025 20:58:09 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 20:58:35 - INFO - training.fm_trainer - Eval Step 3650: loss=3.3786, ppl=29.33
12/02/2025 20:58:53 - INFO - training.fm_trainer - Step 3660/210000 (1.74%): loss=1.4774, lr=6.84e-05, step_time=1861.4ms, ETA 4d 9h
12/02/2025 20:59:12 - INFO - training.fm_trainer - Step 3670/210000 (1.75%): loss=2.0394, lr=6.84e-05, step_time=1842.9ms, ETA 4d 9h
12/02/2025 20:59:30 - INFO - training.fm_trainer - Step 3680/210000 (1.75%): loss=4.0833, lr=6.90e-05, grad_norm=0.06, step_time=1862.7ms, ETA 4d 9h
12/02/2025 20:59:49 - INFO - training.fm_trainer - Step 3690/210000 (1.76%): loss=0.7919, lr=6.90e-05, step_time=1830.0ms, ETA 4d 9h
12/02/2025 21:00:07 - INFO - training.fm_trainer - Step 3700/210000 (1.76%): loss=4.9425, lr=6.90e-05, step_time=1835.0ms, ETA 4d 9h
12/02/2025 21:00:07 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:00:35 - INFO - training.fm_trainer - Eval Step 3700: loss=3.4590, ppl=31.78
12/02/2025 21:00:54 - INFO - training.fm_trainer - Step 3710/210000 (1.77%): loss=5.6699, lr=6.90e-05, step_time=1860.1ms, ETA 4d 9h
12/02/2025 21:01:12 - INFO - training.fm_trainer - Step 3720/210000 (1.77%): loss=1.2963, lr=6.96e-05, step_time=1811.3ms, ETA 4d 9h
12/02/2025 21:01:30 - INFO - training.fm_trainer - Step 3730/210000 (1.78%): loss=9.8399, lr=6.96e-05, step_time=1824.0ms, ETA 4d 9h
12/02/2025 21:01:49 - INFO - training.fm_trainer - Step 3740/210000 (1.78%): loss=2.9761, lr=6.96e-05, step_time=1843.1ms, ETA 4d 9h
12/02/2025 21:02:08 - INFO - training.fm_trainer - Step 3750/210000 (1.79%): loss=5.2124, lr=7.02e-05, step_time=1835.0ms, ETA 4d 9h
12/02/2025 21:02:08 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:02:34 - INFO - training.fm_trainer - Eval Step 3750: loss=3.6453, ppl=38.30
12/02/2025 21:02:52 - INFO - training.fm_trainer - Step 3760/210000 (1.79%): loss=3.3505, lr=7.02e-05, step_time=1837.6ms, ETA 4d 9h
12/02/2025 21:03:11 - INFO - training.fm_trainer - Step 3770/210000 (1.80%): loss=2.3290, lr=7.02e-05, step_time=1850.8ms, ETA 4d 9h
12/02/2025 21:03:29 - INFO - training.fm_trainer - Step 3780/210000 (1.80%): loss=2.6477, lr=7.08e-05, step_time=1831.2ms, ETA 4d 9h
12/02/2025 21:03:47 - INFO - training.fm_trainer - Step 3790/210000 (1.80%): loss=1.8708, lr=7.08e-05, step_time=1804.0ms, ETA 4d 9h
12/02/2025 21:04:06 - INFO - training.fm_trainer - Step 3800/210000 (1.81%): loss=1.9791, lr=7.08e-05, step_time=1902.1ms, ETA 4d 9h
12/02/2025 21:04:06 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:04:32 - INFO - training.fm_trainer - Eval Step 3800: loss=3.9016, ppl=49.48
12/02/2025 21:04:50 - INFO - training.fm_trainer - Step 3810/210000 (1.81%): loss=2.7530, lr=7.14e-05, step_time=1825.2ms, ETA 4d 9h
12/02/2025 21:05:09 - INFO - training.fm_trainer - Step 3820/210000 (1.82%): loss=3.6501, lr=7.14e-05, step_time=1828.5ms, ETA 4d 9h
12/02/2025 21:05:28 - INFO - training.fm_trainer - Step 3830/210000 (1.82%): loss=5.1616, lr=7.14e-05, step_time=1823.8ms, ETA 4d 9h
12/02/2025 21:05:46 - INFO - training.fm_trainer - Step 3840/210000 (1.83%): loss=1.7987, lr=7.20e-05, grad_norm=0.57, step_time=1887.2ms, ETA 4d 9h
12/02/2025 21:06:05 - INFO - training.fm_trainer - Step 3850/210000 (1.83%): loss=2.2765, lr=7.20e-05, step_time=1840.2ms, ETA 4d 9h
12/02/2025 21:06:05 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:06:31 - INFO - training.fm_trainer - Eval Step 3850: loss=4.0910, ppl=59.80
12/02/2025 21:06:49 - INFO - training.fm_trainer - Step 3860/210000 (1.84%): loss=2.6526, lr=7.20e-05, step_time=1855.3ms, ETA 4d 9h
12/02/2025 21:07:08 - INFO - training.fm_trainer - Step 3870/210000 (1.84%): loss=5.3535, lr=7.20e-05, step_time=1846.4ms, ETA 4d 9h
12/02/2025 21:07:26 - INFO - training.fm_trainer - Step 3880/210000 (1.85%): loss=11.7398, lr=7.26e-05, step_time=1845.5ms, ETA 4d 9h
12/02/2025 21:07:45 - INFO - training.fm_trainer - Step 3890/210000 (1.85%): loss=3.5624, lr=7.26e-05, step_time=1842.1ms, ETA 4d 9h
12/02/2025 21:08:03 - INFO - training.fm_trainer - Step 3900/210000 (1.86%): loss=5.3105, lr=7.26e-05, step_time=1830.3ms, ETA 4d 9h
12/02/2025 21:08:03 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:08:29 - INFO - training.fm_trainer - Eval Step 3900: loss=3.7893, ppl=44.22
12/02/2025 21:08:48 - INFO - training.fm_trainer - Step 3910/210000 (1.86%): loss=1.0502, lr=7.32e-05, step_time=1807.9ms, ETA 4d 9h
12/02/2025 21:09:06 - INFO - training.fm_trainer - Step 3920/210000 (1.87%): loss=1.6579, lr=7.32e-05, step_time=1843.7ms, ETA 4d 9h
12/02/2025 21:09:24 - INFO - training.fm_trainer - Step 3930/210000 (1.87%): loss=1.8658, lr=7.32e-05, step_time=1830.1ms, ETA 4d 9h
12/02/2025 21:09:43 - INFO - training.fm_trainer - Step 3940/210000 (1.88%): loss=11.1771, lr=7.38e-05, step_time=1889.5ms, ETA 4d 9h
12/02/2025 21:10:01 - INFO - training.fm_trainer - Step 3950/210000 (1.88%): loss=0.9515, lr=7.38e-05, step_time=1841.3ms, ETA 4d 9h
12/02/2025 21:10:01 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:10:27 - INFO - training.fm_trainer - Eval Step 3950: loss=3.4986, ppl=33.07
12/02/2025 21:10:46 - INFO - training.fm_trainer - Step 3960/210000 (1.89%): loss=1.9692, lr=7.38e-05, step_time=1837.4ms, ETA 4d 9h
12/02/2025 21:11:04 - INFO - training.fm_trainer - Step 3970/210000 (1.89%): loss=10.6596, lr=7.44e-05, step_time=1858.1ms, ETA 4d 9h
12/02/2025 21:11:23 - INFO - training.fm_trainer - Step 3980/210000 (1.90%): loss=4.7591, lr=7.44e-05, step_time=1812.4ms, ETA 4d 9h
12/02/2025 21:11:41 - INFO - training.fm_trainer - Step 3990/210000 (1.90%): loss=2.4072, lr=7.44e-05, step_time=1823.9ms, ETA 4d 9h
12/02/2025 21:12:00 - INFO - training.fm_trainer - Step 4000/210000 (1.90%): loss=5.9569, lr=7.50e-05, grad_norm=0.37, step_time=1847.3ms, ETA 4d 9h
12/02/2025 21:12:00 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:12:26 - INFO - training.fm_trainer - Eval Step 4000: loss=3.4822, ppl=32.53
12/02/2025 21:12:26 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-4000
12/02/2025 21:12:26 - INFO - accelerate.accelerator - Saving FSDP model
12/02/2025 21:12:27 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-4000/pytorch_model_fsdp_0
12/02/2025 21:12:35 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-4000/pytorch_model_fsdp_0
12/02/2025 21:12:35 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-4000
12/02/2025 21:12:35 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/02/2025 21:12:38 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-4000/optimizer_0
12/02/2025 21:12:54 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-4000/optimizer_0
12/02/2025 21:12:54 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-4000
12/02/2025 21:12:54 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-4000/scheduler.bin
12/02/2025 21:12:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-4000/sampler.bin
12/02/2025 21:12:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-4000/sampler_1.bin
12/02/2025 21:12:54 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-4000/random_states_0.pkl
12/02/2025 21:12:54 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-4000
12/02/2025 21:12:54 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-3000
12/02/2025 21:13:15 - INFO - training.fm_trainer - Step 4010/210000 (1.91%): loss=3.0057, lr=7.50e-05, step_time=1883.4ms, ETA 4d 9h
12/02/2025 21:13:34 - INFO - training.fm_trainer - Step 4020/210000 (1.91%): loss=1.6234, lr=7.50e-05, step_time=1841.8ms, ETA 4d 9h
12/02/2025 21:13:52 - INFO - training.fm_trainer - Step 4030/210000 (1.92%): loss=6.1252, lr=7.50e-05, step_time=1837.7ms, ETA 4d 9h
12/02/2025 21:14:10 - INFO - training.fm_trainer - Step 4040/210000 (1.92%): loss=4.6350, lr=7.56e-05, step_time=1835.1ms, ETA 4d 9h
12/02/2025 21:14:29 - INFO - training.fm_trainer - Step 4050/210000 (1.93%): loss=1.6064, lr=7.56e-05, step_time=1811.5ms, ETA 4d 9h
12/02/2025 21:14:29 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:14:56 - INFO - training.fm_trainer - Eval Step 4050: loss=3.7018, ppl=40.52
12/02/2025 21:15:14 - INFO - training.fm_trainer - Step 4060/210000 (1.93%): loss=1.3090, lr=7.56e-05, step_time=1840.1ms, ETA 4d 9h
12/02/2025 21:15:33 - INFO - training.fm_trainer - Step 4070/210000 (1.94%): loss=1.0688, lr=7.62e-05, step_time=1861.1ms, ETA 4d 9h
12/02/2025 21:15:52 - INFO - training.fm_trainer - Step 4080/210000 (1.94%): loss=3.7833, lr=7.62e-05, step_time=1824.8ms, ETA 4d 9h
12/02/2025 21:16:10 - INFO - training.fm_trainer - Step 4090/210000 (1.95%): loss=0.8773, lr=7.62e-05, step_time=1858.5ms, ETA 4d 9h
12/02/2025 21:16:29 - INFO - training.fm_trainer - Step 4100/210000 (1.95%): loss=3.2543, lr=7.68e-05, step_time=1819.5ms, ETA 4d 9h
12/02/2025 21:16:29 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:16:57 - INFO - training.fm_trainer - Eval Step 4100: loss=3.5452, ppl=34.65
12/02/2025 21:17:15 - INFO - training.fm_trainer - Step 4110/210000 (1.96%): loss=8.7374, lr=7.68e-05, step_time=1867.0ms, ETA 4d 9h
12/02/2025 21:17:34 - INFO - training.fm_trainer - Step 4120/210000 (1.96%): loss=1.6294, lr=7.68e-05, step_time=1844.7ms, ETA 4d 9h
12/02/2025 21:17:52 - INFO - training.fm_trainer - Step 4130/210000 (1.97%): loss=1.1196, lr=7.74e-05, step_time=1845.3ms, ETA 4d 9h
12/02/2025 21:18:10 - INFO - training.fm_trainer - Step 4140/210000 (1.97%): loss=6.2005, lr=7.74e-05, step_time=1827.2ms, ETA 4d 9h
12/02/2025 21:18:29 - INFO - training.fm_trainer - Step 4150/210000 (1.98%): loss=0.7100, lr=7.74e-05, step_time=1823.4ms, ETA 4d 9h
12/02/2025 21:18:29 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:18:55 - INFO - training.fm_trainer - Eval Step 4150: loss=3.6899, ppl=40.04
12/02/2025 21:19:14 - INFO - training.fm_trainer - Step 4160/210000 (1.98%): loss=3.0079, lr=7.80e-05, grad_norm=0.52, step_time=1851.0ms, ETA 4d 9h
12/02/2025 21:19:32 - INFO - training.fm_trainer - Step 4170/210000 (1.99%): loss=4.4592, lr=7.80e-05, step_time=1850.4ms, ETA 4d 9h
12/02/2025 21:19:50 - INFO - training.fm_trainer - Step 4180/210000 (1.99%): loss=1.2081, lr=7.80e-05, step_time=1823.8ms, ETA 4d 9h
12/02/2025 21:20:09 - INFO - training.fm_trainer - Step 4190/210000 (2.00%): loss=4.4610, lr=7.80e-05, step_time=1826.4ms, ETA 4d 9h
12/02/2025 21:20:27 - INFO - training.fm_trainer - Step 4200/210000 (2.00%): loss=0.9760, lr=7.86e-05, step_time=1833.2ms, ETA 4d 9h
12/02/2025 21:20:27 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:20:55 - INFO - training.fm_trainer - Eval Step 4200: loss=3.9595, ppl=52.43
12/02/2025 21:21:13 - INFO - training.fm_trainer - Step 4210/210000 (2.00%): loss=0.9875, lr=7.86e-05, step_time=1818.8ms, ETA 4d 8h
12/02/2025 21:21:32 - INFO - training.fm_trainer - Step 4220/210000 (2.01%): loss=2.0023, lr=7.86e-05, step_time=1934.5ms, ETA 4d 9h
12/02/2025 21:21:50 - INFO - training.fm_trainer - Step 4230/210000 (2.01%): loss=1.9375, lr=7.92e-05, step_time=1850.0ms, ETA 4d 9h
12/02/2025 21:22:09 - INFO - training.fm_trainer - Step 4240/210000 (2.02%): loss=1.8015, lr=7.92e-05, step_time=1822.0ms, ETA 4d 9h
12/02/2025 21:22:27 - INFO - training.fm_trainer - Step 4250/210000 (2.02%): loss=2.3990, lr=7.92e-05, step_time=1813.0ms, ETA 4d 9h
12/02/2025 21:22:27 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:22:56 - INFO - training.fm_trainer - Eval Step 4250: loss=3.8380, ppl=46.43
12/02/2025 21:23:14 - INFO - training.fm_trainer - Step 4260/210000 (2.03%): loss=2.5239, lr=7.98e-05, step_time=1818.0ms, ETA 4d 9h
12/02/2025 21:23:33 - INFO - training.fm_trainer - Step 4270/210000 (2.03%): loss=2.3035, lr=7.98e-05, step_time=1830.1ms, ETA 4d 9h
12/02/2025 21:23:51 - INFO - training.fm_trainer - Step 4280/210000 (2.04%): loss=0.8818, lr=7.98e-05, step_time=1944.8ms, ETA 4d 9h
12/02/2025 21:24:10 - INFO - training.fm_trainer - Step 4290/210000 (2.04%): loss=11.0505, lr=8.04e-05, step_time=1883.9ms, ETA 4d 9h
12/02/2025 21:24:29 - INFO - training.fm_trainer - Step 4300/210000 (2.05%): loss=2.2045, lr=8.04e-05, step_time=1852.8ms, ETA 4d 9h
12/02/2025 21:24:29 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:24:56 - INFO - training.fm_trainer - Eval Step 4300: loss=3.4370, ppl=31.09
12/02/2025 21:25:15 - INFO - training.fm_trainer - Step 4310/210000 (2.05%): loss=5.7241, lr=8.04e-05, step_time=1877.1ms, ETA 4d 9h
12/02/2025 21:25:34 - INFO - training.fm_trainer - Step 4320/210000 (2.06%): loss=4.8070, lr=8.10e-05, grad_norm=0.09, step_time=1881.9ms, ETA 4d 10h
12/02/2025 21:25:52 - INFO - training.fm_trainer - Step 4330/210000 (2.06%): loss=2.3830, lr=8.10e-05, step_time=1884.7ms, ETA 4d 10h
12/02/2025 21:26:11 - INFO - training.fm_trainer - Step 4340/210000 (2.07%): loss=7.3424, lr=8.10e-05, step_time=1909.3ms, ETA 4d 10h
12/02/2025 21:26:30 - INFO - training.fm_trainer - Step 4350/210000 (2.07%): loss=1.8488, lr=8.10e-05, step_time=1836.2ms, ETA 4d 10h
12/02/2025 21:26:30 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:26:57 - INFO - training.fm_trainer - Eval Step 4350: loss=3.4287, ppl=30.84
12/02/2025 21:27:16 - INFO - training.fm_trainer - Step 4360/210000 (2.08%): loss=5.2515, lr=8.16e-05, step_time=1846.3ms, ETA 4d 10h
12/02/2025 21:27:34 - INFO - training.fm_trainer - Step 4370/210000 (2.08%): loss=10.6275, lr=8.16e-05, step_time=1808.0ms, ETA 4d 9h
12/02/2025 21:27:53 - INFO - training.fm_trainer - Step 4380/210000 (2.09%): loss=1.3777, lr=8.16e-05, step_time=1850.1ms, ETA 4d 9h
12/02/2025 21:28:11 - INFO - training.fm_trainer - Step 4390/210000 (2.09%): loss=3.5363, lr=8.22e-05, step_time=1846.2ms, ETA 4d 9h
12/02/2025 21:28:30 - INFO - training.fm_trainer - Step 4400/210000 (2.10%): loss=5.8553, lr=8.22e-05, step_time=1823.3ms, ETA 4d 9h
12/02/2025 21:28:30 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:28:57 - INFO - training.fm_trainer - Eval Step 4400: loss=3.4327, ppl=30.96
12/02/2025 21:29:16 - INFO - training.fm_trainer - Step 4410/210000 (2.10%): loss=2.0274, lr=8.22e-05, step_time=1842.6ms, ETA 4d 9h
12/02/2025 21:29:35 - INFO - training.fm_trainer - Step 4420/210000 (2.10%): loss=0.5710, lr=8.28e-05, step_time=1842.9ms, ETA 4d 9h
12/02/2025 21:29:54 - INFO - training.fm_trainer - Step 4430/210000 (2.11%): loss=0.8919, lr=8.28e-05, step_time=1861.9ms, ETA 4d 9h
12/02/2025 21:30:12 - INFO - training.fm_trainer - Step 4440/210000 (2.11%): loss=2.0143, lr=8.28e-05, step_time=1870.2ms, ETA 4d 9h
12/02/2025 21:30:31 - INFO - training.fm_trainer - Step 4450/210000 (2.12%): loss=0.9523, lr=8.34e-05, step_time=1819.4ms, ETA 4d 9h
12/02/2025 21:30:31 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:30:58 - INFO - training.fm_trainer - Eval Step 4450: loss=3.3408, ppl=28.24
12/02/2025 21:31:17 - INFO - training.fm_trainer - Step 4460/210000 (2.12%): loss=2.9969, lr=8.34e-05, step_time=1897.3ms, ETA 4d 9h
12/02/2025 21:31:36 - INFO - training.fm_trainer - Step 4470/210000 (2.13%): loss=1.5781, lr=8.34e-05, step_time=1818.8ms, ETA 4d 9h
12/02/2025 21:31:54 - INFO - training.fm_trainer - Step 4480/210000 (2.13%): loss=1.2501, lr=8.40e-05, grad_norm=0.08, step_time=1860.1ms, ETA 4d 9h
12/02/2025 21:32:12 - INFO - training.fm_trainer - Step 4490/210000 (2.14%): loss=1.5372, lr=8.40e-05, step_time=1844.2ms, ETA 4d 9h
12/02/2025 21:32:31 - INFO - training.fm_trainer - Step 4500/210000 (2.14%): loss=1.4418, lr=8.40e-05, step_time=1806.6ms, ETA 4d 9h
12/02/2025 21:32:31 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:32:58 - INFO - training.fm_trainer - Eval Step 4500: loss=3.3067, ppl=27.30
12/02/2025 21:32:58 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-4500
12/02/2025 21:32:58 - INFO - accelerate.accelerator - Saving FSDP model
12/02/2025 21:33:00 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-4500/pytorch_model_fsdp_0
12/02/2025 21:33:09 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-4500/pytorch_model_fsdp_0
12/02/2025 21:33:09 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-4500
12/02/2025 21:33:09 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/02/2025 21:33:12 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-4500/optimizer_0
12/02/2025 21:33:30 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-4500/optimizer_0
12/02/2025 21:33:30 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-4500
12/02/2025 21:33:30 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-4500/scheduler.bin
12/02/2025 21:33:30 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-4500/sampler.bin
12/02/2025 21:33:30 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-4500/sampler_1.bin
12/02/2025 21:33:30 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-4500/random_states_0.pkl
12/02/2025 21:33:30 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-4500
12/02/2025 21:33:30 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-3500
12/02/2025 21:33:51 - INFO - training.fm_trainer - Step 4510/210000 (2.15%): loss=5.0162, lr=8.40e-05, step_time=1833.2ms, ETA 4d 9h
12/02/2025 21:34:09 - INFO - training.fm_trainer - Step 4520/210000 (2.15%): loss=1.6643, lr=8.46e-05, step_time=1966.1ms, ETA 4d 9h
12/02/2025 21:34:28 - INFO - training.fm_trainer - Step 4530/210000 (2.16%): loss=1.3403, lr=8.46e-05, step_time=1867.8ms, ETA 4d 10h
12/02/2025 21:34:46 - INFO - training.fm_trainer - Step 4540/210000 (2.16%): loss=1.7441, lr=8.46e-05, step_time=1827.8ms, ETA 4d 9h
12/02/2025 21:35:05 - INFO - training.fm_trainer - Step 4550/210000 (2.17%): loss=5.5722, lr=8.52e-05, step_time=1834.6ms, ETA 4d 9h
12/02/2025 21:35:05 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:35:32 - INFO - training.fm_trainer - Eval Step 4550: loss=3.3153, ppl=27.53
12/02/2025 21:35:51 - INFO - training.fm_trainer - Step 4560/210000 (2.17%): loss=1.0271, lr=8.52e-05, step_time=1901.7ms, ETA 4d 10h
12/02/2025 21:36:09 - INFO - training.fm_trainer - Step 4570/210000 (2.18%): loss=2.4838, lr=8.52e-05, step_time=1841.3ms, ETA 4d 9h
12/02/2025 21:36:28 - INFO - training.fm_trainer - Step 4580/210000 (2.18%): loss=5.3889, lr=8.58e-05, step_time=1806.2ms, ETA 4d 9h
12/02/2025 21:36:46 - INFO - training.fm_trainer - Step 4590/210000 (2.19%): loss=1.1753, lr=8.58e-05, step_time=1832.0ms, ETA 4d 9h
12/02/2025 21:37:05 - INFO - training.fm_trainer - Step 4600/210000 (2.19%): loss=1.0033, lr=8.58e-05, step_time=1868.6ms, ETA 4d 9h
12/02/2025 21:37:05 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:37:30 - INFO - training.fm_trainer - Eval Step 4600: loss=3.2891, ppl=26.82
12/02/2025 21:37:49 - INFO - training.fm_trainer - Step 4610/210000 (2.20%): loss=2.5716, lr=8.64e-05, step_time=1830.7ms, ETA 4d 9h
12/02/2025 21:38:07 - INFO - training.fm_trainer - Step 4620/210000 (2.20%): loss=2.5254, lr=8.64e-05, step_time=1861.5ms, ETA 4d 9h
12/02/2025 21:38:26 - INFO - training.fm_trainer - Step 4630/210000 (2.20%): loss=1.9950, lr=8.64e-05, step_time=1833.8ms, ETA 4d 9h
12/02/2025 21:38:44 - INFO - training.fm_trainer - Step 4640/210000 (2.21%): loss=11.0390, lr=8.70e-05, grad_norm=0.29, step_time=1892.9ms, ETA 4d 9h
12/02/2025 21:39:03 - INFO - training.fm_trainer - Step 4650/210000 (2.21%): loss=1.5511, lr=8.70e-05, step_time=1832.0ms, ETA 4d 9h
12/02/2025 21:39:03 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:39:29 - INFO - training.fm_trainer - Eval Step 4650: loss=3.2767, ppl=26.49
12/02/2025 21:39:47 - INFO - training.fm_trainer - Step 4660/210000 (2.22%): loss=0.9151, lr=8.70e-05, step_time=1799.3ms, ETA 4d 9h
12/02/2025 21:40:06 - INFO - training.fm_trainer - Step 4670/210000 (2.22%): loss=2.2190, lr=8.70e-05, step_time=1929.7ms, ETA 4d 9h
12/02/2025 21:40:24 - INFO - training.fm_trainer - Step 4680/210000 (2.23%): loss=2.7473, lr=8.76e-05, step_time=1812.4ms, ETA 4d 9h
12/02/2025 21:40:43 - INFO - training.fm_trainer - Step 4690/210000 (2.23%): loss=1.0411, lr=8.76e-05, step_time=1841.7ms, ETA 4d 9h
12/02/2025 21:41:01 - INFO - training.fm_trainer - Step 4700/210000 (2.24%): loss=1.5707, lr=8.76e-05, step_time=1839.0ms, ETA 4d 9h
12/02/2025 21:41:01 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:41:27 - INFO - training.fm_trainer - Eval Step 4700: loss=3.3414, ppl=28.26
12/02/2025 21:41:46 - INFO - training.fm_trainer - Step 4710/210000 (2.24%): loss=2.9585, lr=8.82e-05, step_time=1841.4ms, ETA 4d 9h
12/02/2025 21:42:04 - INFO - training.fm_trainer - Step 4720/210000 (2.25%): loss=6.1836, lr=8.82e-05, step_time=1837.9ms, ETA 4d 9h
12/02/2025 21:42:22 - INFO - training.fm_trainer - Step 4730/210000 (2.25%): loss=1.7870, lr=8.82e-05, step_time=1868.6ms, ETA 4d 9h
12/02/2025 21:42:41 - INFO - training.fm_trainer - Step 4740/210000 (2.26%): loss=4.6253, lr=8.88e-05, step_time=1883.0ms, ETA 4d 9h
12/02/2025 21:42:59 - INFO - training.fm_trainer - Step 4750/210000 (2.26%): loss=0.9107, lr=8.88e-05, step_time=1835.9ms, ETA 4d 9h
12/02/2025 21:42:59 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:43:25 - INFO - training.fm_trainer - Eval Step 4750: loss=3.6416, ppl=38.15
12/02/2025 21:43:44 - INFO - training.fm_trainer - Step 4760/210000 (2.27%): loss=5.6292, lr=8.88e-05, step_time=1878.2ms, ETA 4d 9h
12/02/2025 21:44:03 - INFO - training.fm_trainer - Step 4770/210000 (2.27%): loss=2.5088, lr=8.94e-05, step_time=1878.0ms, ETA 4d 9h
12/02/2025 21:44:22 - INFO - training.fm_trainer - Step 4780/210000 (2.28%): loss=2.0514, lr=8.94e-05, step_time=1815.8ms, ETA 4d 9h
12/02/2025 21:44:40 - INFO - training.fm_trainer - Step 4790/210000 (2.28%): loss=1.3135, lr=8.94e-05, step_time=1863.2ms, ETA 4d 9h
12/02/2025 21:44:59 - INFO - training.fm_trainer - Step 4800/210000 (2.29%): loss=3.7881, lr=9.00e-05, grad_norm=0.23, step_time=1930.8ms, ETA 4d 10h
12/02/2025 21:44:59 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:45:27 - INFO - training.fm_trainer - Eval Step 4800: loss=3.6047, ppl=36.77
12/02/2025 21:45:45 - INFO - training.fm_trainer - Step 4810/210000 (2.29%): loss=2.1854, lr=9.00e-05, step_time=1848.8ms, ETA 4d 9h
12/02/2025 21:46:04 - INFO - training.fm_trainer - Step 4820/210000 (2.30%): loss=6.7279, lr=9.00e-05, step_time=1887.3ms, ETA 4d 10h
12/02/2025 21:46:22 - INFO - training.fm_trainer - Step 4830/210000 (2.30%): loss=7.5646, lr=9.00e-05, step_time=1833.3ms, ETA 4d 9h
12/02/2025 21:46:40 - INFO - training.fm_trainer - Step 4840/210000 (2.30%): loss=6.3531, lr=9.06e-05, step_time=1827.2ms, ETA 4d 9h
12/02/2025 21:46:59 - INFO - training.fm_trainer - Step 4850/210000 (2.31%): loss=1.4491, lr=9.06e-05, step_time=1845.1ms, ETA 4d 9h
12/02/2025 21:46:59 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:47:25 - INFO - training.fm_trainer - Eval Step 4850: loss=3.4517, ppl=31.55
12/02/2025 21:47:44 - INFO - training.fm_trainer - Step 4860/210000 (2.31%): loss=1.5728, lr=9.06e-05, step_time=1847.5ms, ETA 4d 9h
12/02/2025 21:48:03 - INFO - training.fm_trainer - Step 4870/210000 (2.32%): loss=2.8964, lr=9.12e-05, step_time=1846.5ms, ETA 4d 9h
12/02/2025 21:48:21 - INFO - training.fm_trainer - Step 4880/210000 (2.32%): loss=1.5485, lr=9.12e-05, step_time=1858.2ms, ETA 4d 9h
12/02/2025 21:48:40 - INFO - training.fm_trainer - Step 4890/210000 (2.33%): loss=1.3110, lr=9.12e-05, step_time=1865.2ms, ETA 4d 9h
12/02/2025 21:48:59 - INFO - training.fm_trainer - Step 4900/210000 (2.33%): loss=1.8585, lr=9.18e-05, step_time=2072.9ms, ETA 4d 10h
12/02/2025 21:48:59 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:49:26 - INFO - training.fm_trainer - Eval Step 4900: loss=3.2705, ppl=26.33
12/02/2025 21:49:45 - INFO - training.fm_trainer - Step 4910/210000 (2.34%): loss=3.0464, lr=9.18e-05, step_time=1837.6ms, ETA 4d 10h
12/02/2025 21:50:04 - INFO - training.fm_trainer - Step 4920/210000 (2.34%): loss=2.1431, lr=9.18e-05, step_time=1814.7ms, ETA 4d 10h
12/02/2025 21:50:22 - INFO - training.fm_trainer - Step 4930/210000 (2.35%): loss=1.0781, lr=9.24e-05, step_time=1824.3ms, ETA 4d 10h
12/02/2025 21:50:41 - INFO - training.fm_trainer - Step 4940/210000 (2.35%): loss=3.7290, lr=9.24e-05, step_time=1832.7ms, ETA 4d 9h
12/02/2025 21:50:59 - INFO - training.fm_trainer - Step 4950/210000 (2.36%): loss=4.7119, lr=9.24e-05, step_time=1852.1ms, ETA 4d 9h
12/02/2025 21:50:59 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:51:25 - INFO - training.fm_trainer - Eval Step 4950: loss=3.3350, ppl=28.08
12/02/2025 21:51:43 - INFO - training.fm_trainer - Step 4960/210000 (2.36%): loss=9.3825, lr=9.30e-05, grad_norm=0.20, step_time=1878.8ms, ETA 4d 9h
12/02/2025 21:52:02 - INFO - training.fm_trainer - Step 4970/210000 (2.37%): loss=4.5436, lr=9.30e-05, step_time=1828.3ms, ETA 4d 9h
12/02/2025 21:52:20 - INFO - training.fm_trainer - Step 4980/210000 (2.37%): loss=2.4775, lr=9.30e-05, step_time=1826.7ms, ETA 4d 9h
12/02/2025 21:52:39 - INFO - training.fm_trainer - Step 4990/210000 (2.38%): loss=12.7530, lr=9.30e-05, step_time=1958.8ms, ETA 4d 10h
12/02/2025 21:52:57 - INFO - training.fm_trainer - Step 5000/210000 (2.38%): loss=2.1273, lr=9.36e-05, step_time=1834.2ms, ETA 4d 10h
12/02/2025 21:52:57 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:53:23 - INFO - training.fm_trainer - Eval Step 5000: loss=3.5614, ppl=35.21
12/02/2025 21:53:23 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-5000
12/02/2025 21:53:23 - INFO - accelerate.accelerator - Saving FSDP model
12/02/2025 21:53:24 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-5000/pytorch_model_fsdp_0
12/02/2025 21:53:32 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-5000/pytorch_model_fsdp_0
12/02/2025 21:53:32 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-5000
12/02/2025 21:53:32 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/02/2025 21:53:35 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-5000/optimizer_0
12/02/2025 21:53:51 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-5000/optimizer_0
12/02/2025 21:53:51 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-5000
12/02/2025 21:53:51 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-5000/scheduler.bin
12/02/2025 21:53:51 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-5000/sampler.bin
12/02/2025 21:53:51 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-5000/sampler_1.bin
12/02/2025 21:53:51 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-5000/random_states_0.pkl
12/02/2025 21:53:51 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-5000
12/02/2025 21:53:51 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-4000
12/02/2025 21:54:12 - INFO - training.fm_trainer - Step 5010/210000 (2.39%): loss=1.2196, lr=9.36e-05, step_time=1830.3ms, ETA 4d 9h
12/02/2025 21:54:31 - INFO - training.fm_trainer - Step 5020/210000 (2.39%): loss=0.4033, lr=9.36e-05, step_time=1858.6ms, ETA 4d 9h
12/02/2025 21:54:49 - INFO - training.fm_trainer - Step 5030/210000 (2.40%): loss=5.6002, lr=9.42e-05, step_time=1833.3ms, ETA 4d 9h
12/02/2025 21:55:08 - INFO - training.fm_trainer - Step 5040/210000 (2.40%): loss=4.7978, lr=9.42e-05, step_time=1819.7ms, ETA 4d 9h
12/02/2025 21:55:26 - INFO - training.fm_trainer - Step 5050/210000 (2.40%): loss=1.0495, lr=9.42e-05, step_time=1970.9ms, ETA 4d 10h
12/02/2025 21:55:26 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:55:53 - INFO - training.fm_trainer - Eval Step 5050: loss=3.5646, ppl=35.33
12/02/2025 21:56:12 - INFO - training.fm_trainer - Step 5060/210000 (2.41%): loss=6.5090, lr=9.48e-05, step_time=1831.0ms, ETA 4d 9h
12/02/2025 21:56:31 - INFO - training.fm_trainer - Step 5070/210000 (2.41%): loss=9.4722, lr=9.48e-05, step_time=1805.4ms, ETA 4d 9h
12/02/2025 21:56:49 - INFO - training.fm_trainer - Step 5080/210000 (2.42%): loss=0.8383, lr=9.48e-05, step_time=1822.1ms, ETA 4d 9h
12/02/2025 21:57:08 - INFO - training.fm_trainer - Step 5090/210000 (2.42%): loss=1.6289, lr=9.54e-05, step_time=1835.4ms, ETA 4d 9h
12/02/2025 21:57:26 - INFO - training.fm_trainer - Step 5100/210000 (2.43%): loss=2.3928, lr=9.54e-05, step_time=1967.9ms, ETA 4d 9h
12/02/2025 21:57:26 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 21:57:57 - INFO - training.fm_trainer - Eval Step 5100: loss=3.3240, ppl=27.77
12/02/2025 21:58:15 - INFO - training.fm_trainer - Step 5110/210000 (2.43%): loss=0.6610, lr=9.54e-05, step_time=1816.6ms, ETA 4d 9h
12/02/2025 21:58:34 - INFO - training.fm_trainer - Step 5120/210000 (2.44%): loss=0.9926, lr=9.60e-05, grad_norm=0.06, step_time=1841.1ms, ETA 4d 9h
12/02/2025 21:58:53 - INFO - training.fm_trainer - Step 5130/210000 (2.44%): loss=0.6905, lr=9.60e-05, step_time=1855.3ms, ETA 4d 9h
12/02/2025 21:59:12 - INFO - training.fm_trainer - Step 5140/210000 (2.45%): loss=1.1756, lr=9.60e-05, step_time=2008.7ms, ETA 4d 10h
12/02/2025 21:59:33 - INFO - training.fm_trainer - Step 5150/210000 (2.45%): loss=3.1766, lr=9.60e-05, step_time=1811.1ms, ETA 4d 10h
12/02/2025 21:59:33 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:00:00 - INFO - training.fm_trainer - Eval Step 5150: loss=3.3355, ppl=28.09
12/02/2025 22:00:18 - INFO - training.fm_trainer - Step 5160/210000 (2.46%): loss=8.9990, lr=9.66e-05, step_time=1813.0ms, ETA 4d 9h
12/02/2025 22:00:37 - INFO - training.fm_trainer - Step 5170/210000 (2.46%): loss=3.5153, lr=9.66e-05, step_time=1823.4ms, ETA 4d 9h
12/02/2025 22:00:56 - INFO - training.fm_trainer - Step 5180/210000 (2.47%): loss=0.9638, lr=9.66e-05, step_time=1870.0ms, ETA 4d 9h
12/02/2025 22:01:14 - INFO - training.fm_trainer - Step 5190/210000 (2.47%): loss=3.5310, lr=9.72e-05, step_time=1855.6ms, ETA 4d 9h
12/02/2025 22:01:33 - INFO - training.fm_trainer - Step 5200/210000 (2.48%): loss=1.0860, lr=9.72e-05, step_time=1921.1ms, ETA 4d 10h
12/02/2025 22:01:33 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:01:59 - INFO - training.fm_trainer - Eval Step 5200: loss=3.2488, ppl=25.76
12/02/2025 22:02:17 - INFO - training.fm_trainer - Step 5210/210000 (2.48%): loss=1.7110, lr=9.72e-05, step_time=1823.5ms, ETA 4d 9h
12/02/2025 22:02:36 - INFO - training.fm_trainer - Step 5220/210000 (2.49%): loss=2.9542, lr=9.78e-05, step_time=1833.3ms, ETA 4d 9h
12/02/2025 22:02:55 - INFO - training.fm_trainer - Step 5230/210000 (2.49%): loss=1.1472, lr=9.78e-05, step_time=1940.0ms, ETA 4d 10h
12/02/2025 22:03:13 - INFO - training.fm_trainer - Step 5240/210000 (2.50%): loss=2.8802, lr=9.78e-05, step_time=1841.1ms, ETA 4d 9h
12/02/2025 22:03:31 - INFO - training.fm_trainer - Step 5250/210000 (2.50%): loss=1.2245, lr=9.84e-05, step_time=1843.6ms, ETA 4d 9h
12/02/2025 22:03:31 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:03:57 - INFO - training.fm_trainer - Eval Step 5250: loss=3.2673, ppl=26.24
12/02/2025 22:04:16 - INFO - training.fm_trainer - Step 5260/210000 (2.50%): loss=5.0786, lr=9.84e-05, step_time=1823.8ms, ETA 4d 9h
12/02/2025 22:04:34 - INFO - training.fm_trainer - Step 5270/210000 (2.51%): loss=0.8473, lr=9.84e-05, step_time=1836.4ms, ETA 4d 9h
12/02/2025 22:04:53 - INFO - training.fm_trainer - Step 5280/210000 (2.51%): loss=3.9115, lr=9.90e-05, grad_norm=0.10, step_time=1901.1ms, ETA 4d 9h
12/02/2025 22:05:11 - INFO - training.fm_trainer - Step 5290/210000 (2.52%): loss=0.8184, lr=9.90e-05, step_time=1815.6ms, ETA 4d 9h
12/02/2025 22:05:30 - INFO - training.fm_trainer - Step 5300/210000 (2.52%): loss=0.9507, lr=9.90e-05, step_time=1839.8ms, ETA 4d 9h
12/02/2025 22:05:30 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:05:57 - INFO - training.fm_trainer - Eval Step 5300: loss=3.2744, ppl=26.43
12/02/2025 22:06:15 - INFO - training.fm_trainer - Step 5310/210000 (2.53%): loss=0.9950, lr=9.90e-05, step_time=1826.5ms, ETA 4d 9h
12/02/2025 22:06:33 - INFO - training.fm_trainer - Step 5320/210000 (2.53%): loss=2.5203, lr=9.96e-05, step_time=1840.0ms, ETA 4d 9h
12/02/2025 22:06:52 - INFO - training.fm_trainer - Step 5330/210000 (2.54%): loss=3.4075, lr=9.96e-05, step_time=1859.6ms, ETA 4d 9h
12/02/2025 22:07:10 - INFO - training.fm_trainer - Step 5340/210000 (2.54%): loss=0.8021, lr=9.96e-05, step_time=1845.1ms, ETA 4d 9h
12/02/2025 22:07:29 - INFO - training.fm_trainer - Step 5350/210000 (2.55%): loss=7.2521, lr=1.00e-04, step_time=1925.9ms, ETA 4d 9h
12/02/2025 22:07:29 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:07:55 - INFO - training.fm_trainer - Eval Step 5350: loss=3.2258, ppl=25.17
12/02/2025 22:08:15 - INFO - training.fm_trainer - Step 5360/210000 (2.55%): loss=0.6336, lr=1.00e-04, step_time=1952.7ms, ETA 4d 10h
12/02/2025 22:08:33 - INFO - training.fm_trainer - Step 5370/210000 (2.56%): loss=1.4112, lr=1.00e-04, step_time=1854.7ms, ETA 4d 10h
12/02/2025 22:08:52 - INFO - training.fm_trainer - Step 5380/210000 (2.56%): loss=2.2499, lr=1.01e-04, step_time=1825.6ms, ETA 4d 9h
12/02/2025 22:09:10 - INFO - training.fm_trainer - Step 5390/210000 (2.57%): loss=0.9482, lr=1.01e-04, step_time=1816.1ms, ETA 4d 9h
12/02/2025 22:09:28 - INFO - training.fm_trainer - Step 5400/210000 (2.57%): loss=1.9824, lr=1.01e-04, step_time=1830.3ms, ETA 4d 9h
12/02/2025 22:09:28 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:09:54 - INFO - training.fm_trainer - Eval Step 5400: loss=3.2462, ppl=25.69
12/02/2025 22:10:12 - INFO - training.fm_trainer - Step 5410/210000 (2.58%): loss=1.7309, lr=1.01e-04, step_time=1846.6ms, ETA 4d 9h
12/02/2025 22:10:31 - INFO - training.fm_trainer - Step 5420/210000 (2.58%): loss=1.4901, lr=1.01e-04, step_time=1831.0ms, ETA 4d 9h
12/02/2025 22:10:49 - INFO - training.fm_trainer - Step 5430/210000 (2.59%): loss=0.4992, lr=1.01e-04, step_time=1823.7ms, ETA 4d 9h
12/02/2025 22:11:08 - INFO - training.fm_trainer - Step 5440/210000 (2.59%): loss=0.8746, lr=1.02e-04, grad_norm=0.06, step_time=1979.3ms, ETA 4d 9h
12/02/2025 22:11:26 - INFO - training.fm_trainer - Step 5450/210000 (2.60%): loss=1.0413, lr=1.02e-04, step_time=1842.3ms, ETA 4d 9h
12/02/2025 22:11:26 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:11:53 - INFO - training.fm_trainer - Eval Step 5450: loss=3.3571, ppl=28.71
12/02/2025 22:12:11 - INFO - training.fm_trainer - Step 5460/210000 (2.60%): loss=0.5882, lr=1.02e-04, step_time=1812.7ms, ETA 4d 9h
12/02/2025 22:12:29 - INFO - training.fm_trainer - Step 5470/210000 (2.60%): loss=2.8705, lr=1.02e-04, step_time=1860.6ms, ETA 4d 9h
12/02/2025 22:12:48 - INFO - training.fm_trainer - Step 5480/210000 (2.61%): loss=9.6878, lr=1.03e-04, step_time=1818.8ms, ETA 4d 9h
12/02/2025 22:13:06 - INFO - training.fm_trainer - Step 5490/210000 (2.61%): loss=3.0653, lr=1.03e-04, step_time=1822.3ms, ETA 4d 9h
12/02/2025 22:13:25 - INFO - training.fm_trainer - Step 5500/210000 (2.62%): loss=1.4148, lr=1.03e-04, step_time=1839.6ms, ETA 4d 8h
12/02/2025 22:13:25 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:13:51 - INFO - training.fm_trainer - Eval Step 5500: loss=3.4604, ppl=31.83
12/02/2025 22:13:51 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-5500
12/02/2025 22:13:51 - INFO - accelerate.accelerator - Saving FSDP model
12/02/2025 22:13:52 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-5500/pytorch_model_fsdp_0
12/02/2025 22:13:59 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-5500/pytorch_model_fsdp_0
12/02/2025 22:13:59 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-5500
12/02/2025 22:13:59 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/02/2025 22:14:02 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-5500/optimizer_0
12/02/2025 22:14:18 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-5500/optimizer_0
12/02/2025 22:14:18 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-5500
12/02/2025 22:14:18 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-5500/scheduler.bin
12/02/2025 22:14:18 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-5500/sampler.bin
12/02/2025 22:14:18 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-5500/sampler_1.bin
12/02/2025 22:14:18 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-5500/random_states_0.pkl
12/02/2025 22:14:18 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-5500
12/02/2025 22:14:18 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-4500
12/02/2025 22:14:39 - INFO - training.fm_trainer - Step 5510/210000 (2.62%): loss=9.0635, lr=1.03e-04, step_time=1872.4ms, ETA 4d 9h
12/02/2025 22:14:58 - INFO - training.fm_trainer - Step 5520/210000 (2.63%): loss=0.7210, lr=1.03e-04, step_time=1854.7ms, ETA 4d 9h
12/02/2025 22:15:16 - INFO - training.fm_trainer - Step 5530/210000 (2.63%): loss=0.3841, lr=1.03e-04, step_time=1843.9ms, ETA 4d 9h
12/02/2025 22:15:35 - INFO - training.fm_trainer - Step 5540/210000 (2.64%): loss=0.4644, lr=1.04e-04, step_time=1835.8ms, ETA 4d 9h
12/02/2025 22:15:53 - INFO - training.fm_trainer - Step 5550/210000 (2.64%): loss=3.1645, lr=1.04e-04, step_time=1877.8ms, ETA 4d 9h
12/02/2025 22:15:53 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:16:21 - INFO - training.fm_trainer - Eval Step 5550: loss=3.3318, ppl=27.99
12/02/2025 22:16:39 - INFO - training.fm_trainer - Step 5560/210000 (2.65%): loss=12.2454, lr=1.04e-04, step_time=1850.9ms, ETA 4d 9h
12/02/2025 22:16:58 - INFO - training.fm_trainer - Step 5570/210000 (2.65%): loss=1.6455, lr=1.04e-04, step_time=1846.1ms, ETA 4d 9h
12/02/2025 22:17:16 - INFO - training.fm_trainer - Step 5580/210000 (2.66%): loss=2.4863, lr=1.04e-04, step_time=1813.7ms, ETA 4d 8h
12/02/2025 22:17:35 - INFO - training.fm_trainer - Step 5590/210000 (2.66%): loss=0.4972, lr=1.04e-04, step_time=1839.4ms, ETA 4d 8h
12/02/2025 22:17:53 - INFO - training.fm_trainer - Step 5600/210000 (2.67%): loss=2.3687, lr=1.05e-04, grad_norm=0.04, step_time=1917.1ms, ETA 4d 9h
12/02/2025 22:17:53 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:18:19 - INFO - training.fm_trainer - Eval Step 5600: loss=3.2503, ppl=25.80
12/02/2025 22:18:37 - INFO - training.fm_trainer - Step 5610/210000 (2.67%): loss=0.7505, lr=1.05e-04, step_time=1859.2ms, ETA 4d 9h
12/02/2025 22:18:56 - INFO - training.fm_trainer - Step 5620/210000 (2.68%): loss=1.5936, lr=1.05e-04, step_time=1862.0ms, ETA 4d 9h
12/02/2025 22:19:14 - INFO - training.fm_trainer - Step 5630/210000 (2.68%): loss=3.1723, lr=1.05e-04, step_time=1845.6ms, ETA 4d 9h
12/02/2025 22:19:33 - INFO - training.fm_trainer - Step 5640/210000 (2.69%): loss=4.9517, lr=1.06e-04, step_time=1819.8ms, ETA 4d 9h
12/02/2025 22:19:51 - INFO - training.fm_trainer - Step 5650/210000 (2.69%): loss=4.9720, lr=1.06e-04, step_time=1844.2ms, ETA 4d 9h
12/02/2025 22:19:51 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:20:17 - INFO - training.fm_trainer - Eval Step 5650: loss=3.2599, ppl=26.05
12/02/2025 22:20:36 - INFO - training.fm_trainer - Step 5660/210000 (2.70%): loss=1.5445, lr=1.06e-04, step_time=1891.9ms, ETA 4d 9h
12/02/2025 22:20:54 - INFO - training.fm_trainer - Step 5670/210000 (2.70%): loss=8.8743, lr=1.06e-04, step_time=1834.8ms, ETA 4d 9h
12/02/2025 22:21:13 - INFO - training.fm_trainer - Step 5680/210000 (2.70%): loss=1.5419, lr=1.06e-04, step_time=1818.2ms, ETA 4d 8h
12/02/2025 22:21:31 - INFO - training.fm_trainer - Step 5690/210000 (2.71%): loss=4.7129, lr=1.06e-04, step_time=1816.5ms, ETA 4d 8h
12/02/2025 22:21:50 - INFO - training.fm_trainer - Step 5700/210000 (2.71%): loss=1.3177, lr=1.07e-04, step_time=1811.7ms, ETA 4d 8h
12/02/2025 22:21:50 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:22:16 - INFO - training.fm_trainer - Eval Step 5700: loss=3.2899, ppl=26.84
12/02/2025 22:22:34 - INFO - training.fm_trainer - Step 5710/210000 (2.72%): loss=2.6441, lr=1.07e-04, step_time=1830.8ms, ETA 4d 8h
12/02/2025 22:22:52 - INFO - training.fm_trainer - Step 5720/210000 (2.72%): loss=2.5902, lr=1.07e-04, step_time=1849.6ms, ETA 4d 8h
12/02/2025 22:23:11 - INFO - training.fm_trainer - Step 5730/210000 (2.73%): loss=0.8798, lr=1.07e-04, step_time=1810.2ms, ETA 4d 8h
12/02/2025 22:23:29 - INFO - training.fm_trainer - Step 5740/210000 (2.73%): loss=1.3519, lr=1.07e-04, step_time=1885.7ms, ETA 4d 8h
12/02/2025 22:23:48 - INFO - training.fm_trainer - Step 5750/210000 (2.74%): loss=7.7722, lr=1.07e-04, step_time=1865.4ms, ETA 4d 8h
12/02/2025 22:23:48 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:24:15 - INFO - training.fm_trainer - Eval Step 5750: loss=3.2732, ppl=26.40
12/02/2025 22:24:34 - INFO - training.fm_trainer - Step 5760/210000 (2.74%): loss=4.0753, lr=1.08e-04, grad_norm=0.03, step_time=1854.9ms, ETA 4d 8h
12/02/2025 22:24:52 - INFO - training.fm_trainer - Step 5770/210000 (2.75%): loss=2.1200, lr=1.08e-04, step_time=1816.7ms, ETA 4d 8h
12/02/2025 22:25:10 - INFO - training.fm_trainer - Step 5780/210000 (2.75%): loss=1.2252, lr=1.08e-04, step_time=1841.7ms, ETA 4d 8h
12/02/2025 22:25:29 - INFO - training.fm_trainer - Step 5790/210000 (2.76%): loss=1.1950, lr=1.08e-04, step_time=1811.2ms, ETA 4d 8h
12/02/2025 22:25:47 - INFO - training.fm_trainer - Step 5800/210000 (2.76%): loss=4.6538, lr=1.09e-04, step_time=1831.9ms, ETA 4d 8h
12/02/2025 22:25:47 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:26:13 - INFO - training.fm_trainer - Eval Step 5800: loss=3.2508, ppl=25.81
12/02/2025 22:26:32 - INFO - training.fm_trainer - Step 5810/210000 (2.77%): loss=3.2321, lr=1.09e-04, step_time=1822.1ms, ETA 4d 8h
12/02/2025 22:26:50 - INFO - training.fm_trainer - Step 5820/210000 (2.77%): loss=1.0073, lr=1.09e-04, step_time=1812.9ms, ETA 4d 8h
12/02/2025 22:27:09 - INFO - training.fm_trainer - Step 5830/210000 (2.78%): loss=1.5980, lr=1.09e-04, step_time=1812.0ms, ETA 4d 7h
12/02/2025 22:27:27 - INFO - training.fm_trainer - Step 5840/210000 (2.78%): loss=4.3227, lr=1.09e-04, step_time=1823.8ms, ETA 4d 7h
12/02/2025 22:27:45 - INFO - training.fm_trainer - Step 5850/210000 (2.79%): loss=5.0328, lr=1.09e-04, step_time=1843.4ms, ETA 4d 7h
12/02/2025 22:27:45 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:28:13 - INFO - training.fm_trainer - Eval Step 5850: loss=3.3101, ppl=27.39
12/02/2025 22:28:31 - INFO - training.fm_trainer - Step 5860/210000 (2.79%): loss=1.7372, lr=1.10e-04, step_time=1822.0ms, ETA 4d 7h
12/02/2025 22:28:49 - INFO - training.fm_trainer - Step 5870/210000 (2.80%): loss=1.5487, lr=1.10e-04, step_time=1834.4ms, ETA 4d 7h
12/02/2025 22:29:08 - INFO - training.fm_trainer - Step 5880/210000 (2.80%): loss=1.0297, lr=1.10e-04, step_time=1835.1ms, ETA 4d 7h
12/02/2025 22:29:26 - INFO - training.fm_trainer - Step 5890/210000 (2.80%): loss=1.0922, lr=1.10e-04, step_time=1853.5ms, ETA 4d 8h
12/02/2025 22:29:44 - INFO - training.fm_trainer - Step 5900/210000 (2.81%): loss=3.4724, lr=1.10e-04, step_time=1822.0ms, ETA 4d 7h
12/02/2025 22:29:44 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:30:10 - INFO - training.fm_trainer - Eval Step 5900: loss=3.2638, ppl=26.15
12/02/2025 22:30:29 - INFO - training.fm_trainer - Step 5910/210000 (2.81%): loss=4.3061, lr=1.10e-04, step_time=1843.1ms, ETA 4d 7h
12/02/2025 22:30:47 - INFO - training.fm_trainer - Step 5920/210000 (2.82%): loss=1.6218, lr=1.11e-04, grad_norm=0.06, step_time=1832.6ms, ETA 4d 7h
12/02/2025 22:31:06 - INFO - training.fm_trainer - Step 5930/210000 (2.82%): loss=1.3443, lr=1.11e-04, step_time=1837.7ms, ETA 4d 7h
12/02/2025 22:31:24 - INFO - training.fm_trainer - Step 5940/210000 (2.83%): loss=4.5188, lr=1.11e-04, step_time=1849.3ms, ETA 4d 8h
12/02/2025 22:31:43 - INFO - training.fm_trainer - Step 5950/210000 (2.83%): loss=0.8414, lr=1.11e-04, step_time=1828.1ms, ETA 4d 8h
12/02/2025 22:31:43 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:32:10 - INFO - training.fm_trainer - Eval Step 5950: loss=3.2378, ppl=25.48
12/02/2025 22:32:29 - INFO - training.fm_trainer - Step 5960/210000 (2.84%): loss=2.7501, lr=1.12e-04, step_time=1822.3ms, ETA 4d 7h
12/02/2025 22:32:47 - INFO - training.fm_trainer - Step 5970/210000 (2.84%): loss=1.7916, lr=1.12e-04, step_time=1835.1ms, ETA 4d 7h
12/02/2025 22:33:05 - INFO - training.fm_trainer - Step 5980/210000 (2.85%): loss=0.4294, lr=1.12e-04, step_time=1813.6ms, ETA 4d 7h
12/02/2025 22:33:24 - INFO - training.fm_trainer - Step 5990/210000 (2.85%): loss=3.8239, lr=1.12e-04, step_time=1804.6ms, ETA 4d 7h
12/02/2025 22:33:42 - INFO - training.fm_trainer - Step 6000/210000 (2.86%): loss=0.5593, lr=1.12e-04, step_time=1833.8ms, ETA 4d 7h
12/02/2025 22:33:42 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:34:09 - INFO - training.fm_trainer - Eval Step 6000: loss=3.4549, ppl=31.66
12/02/2025 22:34:09 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-6000
12/02/2025 22:34:09 - INFO - accelerate.accelerator - Saving FSDP model
12/02/2025 22:34:10 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-6000/pytorch_model_fsdp_0
12/02/2025 22:34:20 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-6000/pytorch_model_fsdp_0
12/02/2025 22:34:20 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-6000
12/02/2025 22:34:20 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/02/2025 22:34:22 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-6000/optimizer_0
12/02/2025 22:34:39 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-6000/optimizer_0
12/02/2025 22:34:40 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-6000
12/02/2025 22:34:40 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-6000/scheduler.bin
12/02/2025 22:34:40 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-6000/sampler.bin
12/02/2025 22:34:40 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-6000/sampler_1.bin
12/02/2025 22:34:40 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-6000/random_states_0.pkl
12/02/2025 22:34:40 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-6000
12/02/2025 22:34:40 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-5000
12/02/2025 22:35:01 - INFO - training.fm_trainer - Step 6010/210000 (2.86%): loss=1.2081, lr=1.12e-04, step_time=1855.4ms, ETA 4d 7h
12/02/2025 22:35:20 - INFO - training.fm_trainer - Step 6020/210000 (2.87%): loss=10.2799, lr=1.13e-04, step_time=1868.9ms, ETA 4d 8h
12/02/2025 22:35:38 - INFO - training.fm_trainer - Step 6030/210000 (2.87%): loss=1.5197, lr=1.13e-04, step_time=1819.5ms, ETA 4d 7h
12/02/2025 22:35:57 - INFO - training.fm_trainer - Step 6040/210000 (2.88%): loss=5.9366, lr=1.13e-04, step_time=1822.6ms, ETA 4d 7h
12/02/2025 22:36:15 - INFO - training.fm_trainer - Step 6050/210000 (2.88%): loss=4.0290, lr=1.13e-04, step_time=1823.0ms, ETA 4d 7h
12/02/2025 22:36:15 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:36:42 - INFO - training.fm_trainer - Eval Step 6050: loss=3.6449, ppl=38.28
12/02/2025 22:37:00 - INFO - training.fm_trainer - Step 6060/210000 (2.89%): loss=1.4063, lr=1.13e-04, step_time=1867.0ms, ETA 4d 7h
12/02/2025 22:37:19 - INFO - training.fm_trainer - Step 6070/210000 (2.89%): loss=1.4987, lr=1.13e-04, step_time=1832.5ms, ETA 4d 7h
12/02/2025 22:37:37 - INFO - training.fm_trainer - Step 6080/210000 (2.90%): loss=2.1441, lr=1.14e-04, grad_norm=0.06, step_time=1861.4ms, ETA 4d 8h
12/02/2025 22:37:55 - INFO - training.fm_trainer - Step 6090/210000 (2.90%): loss=6.6636, lr=1.14e-04, step_time=1826.8ms, ETA 4d 8h
12/02/2025 22:38:14 - INFO - training.fm_trainer - Step 6100/210000 (2.90%): loss=1.6213, lr=1.14e-04, step_time=1833.8ms, ETA 4d 8h
12/02/2025 22:38:14 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:38:40 - INFO - training.fm_trainer - Eval Step 6100: loss=3.6007, ppl=36.62
12/02/2025 22:38:58 - INFO - training.fm_trainer - Step 6110/210000 (2.91%): loss=2.3518, lr=1.14e-04, step_time=1837.0ms, ETA 4d 8h
12/02/2025 22:39:17 - INFO - training.fm_trainer - Step 6120/210000 (2.91%): loss=5.4848, lr=1.15e-04, step_time=1832.5ms, ETA 4d 7h
12/02/2025 22:39:36 - INFO - training.fm_trainer - Step 6130/210000 (2.92%): loss=8.0531, lr=1.15e-04, step_time=1831.9ms, ETA 4d 7h
12/02/2025 22:39:54 - INFO - training.fm_trainer - Step 6140/210000 (2.92%): loss=2.1114, lr=1.15e-04, step_time=1824.7ms, ETA 4d 7h
12/02/2025 22:40:13 - INFO - training.fm_trainer - Step 6150/210000 (2.93%): loss=6.9180, lr=1.15e-04, step_time=1873.7ms, ETA 4d 8h
12/02/2025 22:40:13 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:40:39 - INFO - training.fm_trainer - Eval Step 6150: loss=3.3899, ppl=29.66
12/02/2025 22:40:57 - INFO - training.fm_trainer - Step 6160/210000 (2.93%): loss=2.9727, lr=1.15e-04, step_time=1819.4ms, ETA 4d 7h
12/02/2025 22:41:16 - INFO - training.fm_trainer - Step 6170/210000 (2.94%): loss=1.1000, lr=1.15e-04, step_time=1823.0ms, ETA 4d 7h
12/02/2025 22:41:34 - INFO - training.fm_trainer - Step 6180/210000 (2.94%): loss=1.0539, lr=1.16e-04, step_time=1841.1ms, ETA 4d 7h
12/02/2025 22:41:52 - INFO - training.fm_trainer - Step 6190/210000 (2.95%): loss=5.0215, lr=1.16e-04, step_time=1824.2ms, ETA 4d 7h
12/02/2025 22:42:11 - INFO - training.fm_trainer - Step 6200/210000 (2.95%): loss=2.5835, lr=1.16e-04, step_time=1823.0ms, ETA 4d 7h
12/02/2025 22:42:11 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:42:38 - INFO - training.fm_trainer - Eval Step 6200: loss=3.3601, ppl=28.79
12/02/2025 22:42:56 - INFO - training.fm_trainer - Step 6210/210000 (2.96%): loss=0.7219, lr=1.16e-04, step_time=1832.5ms, ETA 4d 7h
12/02/2025 22:43:15 - INFO - training.fm_trainer - Step 6220/210000 (2.96%): loss=2.0153, lr=1.16e-04, step_time=1834.7ms, ETA 4d 7h
12/02/2025 22:43:33 - INFO - training.fm_trainer - Step 6230/210000 (2.97%): loss=2.9405, lr=1.16e-04, step_time=1827.0ms, ETA 4d 7h
12/02/2025 22:43:52 - INFO - training.fm_trainer - Step 6240/210000 (2.97%): loss=1.4837, lr=1.17e-04, grad_norm=0.08, step_time=1874.2ms, ETA 4d 7h
12/02/2025 22:44:10 - INFO - training.fm_trainer - Step 6250/210000 (2.98%): loss=2.0778, lr=1.17e-04, step_time=1839.7ms, ETA 4d 7h
12/02/2025 22:44:10 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:44:37 - INFO - training.fm_trainer - Eval Step 6250: loss=3.2142, ppl=24.88
12/02/2025 22:44:56 - INFO - training.fm_trainer - Step 6260/210000 (2.98%): loss=4.6634, lr=1.17e-04, step_time=1855.8ms, ETA 4d 8h
12/02/2025 22:45:14 - INFO - training.fm_trainer - Step 6270/210000 (2.99%): loss=0.8988, lr=1.17e-04, step_time=1831.2ms, ETA 4d 8h
12/02/2025 22:45:33 - INFO - training.fm_trainer - Step 6280/210000 (2.99%): loss=4.1526, lr=1.18e-04, step_time=1828.7ms, ETA 4d 7h
12/02/2025 22:45:51 - INFO - training.fm_trainer - Step 6290/210000 (3.00%): loss=7.5551, lr=1.18e-04, step_time=1864.7ms, ETA 4d 8h
12/02/2025 22:46:09 - INFO - training.fm_trainer - Step 6300/210000 (3.00%): loss=8.6511, lr=1.18e-04, step_time=1815.7ms, ETA 4d 7h
12/02/2025 22:46:09 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:46:35 - INFO - training.fm_trainer - Eval Step 6300: loss=3.2889, ppl=26.81
12/02/2025 22:46:54 - INFO - training.fm_trainer - Step 6310/210000 (3.00%): loss=1.9480, lr=1.18e-04, step_time=1937.2ms, ETA 4d 8h
12/02/2025 22:47:12 - INFO - training.fm_trainer - Step 6320/210000 (3.01%): loss=0.9286, lr=1.18e-04, step_time=1908.1ms, ETA 4d 8h
12/02/2025 22:47:31 - INFO - training.fm_trainer - Step 6330/210000 (3.01%): loss=2.1710, lr=1.18e-04, step_time=1826.1ms, ETA 4d 8h
12/02/2025 22:47:49 - INFO - training.fm_trainer - Step 6340/210000 (3.02%): loss=1.0057, lr=1.19e-04, step_time=1843.0ms, ETA 4d 8h
12/02/2025 22:48:07 - INFO - training.fm_trainer - Step 6350/210000 (3.02%): loss=0.8281, lr=1.19e-04, step_time=1832.7ms, ETA 4d 8h
12/02/2025 22:48:07 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:48:33 - INFO - training.fm_trainer - Eval Step 6350: loss=3.2449, ppl=25.66
12/02/2025 22:48:52 - INFO - training.fm_trainer - Step 6360/210000 (3.03%): loss=1.1184, lr=1.19e-04, step_time=1821.8ms, ETA 4d 8h
12/02/2025 22:49:10 - INFO - training.fm_trainer - Step 6370/210000 (3.03%): loss=0.4491, lr=1.19e-04, step_time=1819.9ms, ETA 4d 8h
12/02/2025 22:49:28 - INFO - training.fm_trainer - Step 6380/210000 (3.04%): loss=0.3982, lr=1.19e-04, step_time=1809.2ms, ETA 4d 8h
12/02/2025 22:49:47 - INFO - training.fm_trainer - Step 6390/210000 (3.04%): loss=5.5545, lr=1.19e-04, step_time=1807.5ms, ETA 4d 7h
12/02/2025 22:50:05 - INFO - training.fm_trainer - Step 6400/210000 (3.05%): loss=0.5007, lr=1.20e-04, grad_norm=0.06, step_time=1895.0ms, ETA 4d 8h
12/02/2025 22:50:05 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:50:31 - INFO - training.fm_trainer - Eval Step 6400: loss=3.3683, ppl=29.03
12/02/2025 22:50:49 - INFO - training.fm_trainer - Step 6410/210000 (3.05%): loss=1.3244, lr=1.20e-04, step_time=1831.4ms, ETA 4d 8h
12/02/2025 22:51:08 - INFO - training.fm_trainer - Step 6420/210000 (3.06%): loss=0.7917, lr=1.20e-04, step_time=1842.6ms, ETA 4d 8h
12/02/2025 22:51:27 - INFO - training.fm_trainer - Step 6430/210000 (3.06%): loss=10.3525, lr=1.20e-04, step_time=1822.4ms, ETA 4d 8h
12/02/2025 22:51:45 - INFO - training.fm_trainer - Step 6440/210000 (3.07%): loss=5.7731, lr=1.21e-04, step_time=1855.3ms, ETA 4d 8h
12/02/2025 22:52:03 - INFO - training.fm_trainer - Step 6450/210000 (3.07%): loss=0.5204, lr=1.21e-04, step_time=1828.5ms, ETA 4d 8h
12/02/2025 22:52:03 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:52:29 - INFO - training.fm_trainer - Eval Step 6450: loss=3.4903, ppl=32.80
12/02/2025 22:52:48 - INFO - training.fm_trainer - Step 6460/210000 (3.08%): loss=2.7801, lr=1.21e-04, step_time=1823.5ms, ETA 4d 7h
12/02/2025 22:53:06 - INFO - training.fm_trainer - Step 6470/210000 (3.08%): loss=2.6968, lr=1.21e-04, step_time=1906.1ms, ETA 4d 8h
12/02/2025 22:53:24 - INFO - training.fm_trainer - Step 6480/210000 (3.09%): loss=1.0612, lr=1.21e-04, step_time=1827.4ms, ETA 4d 8h
12/02/2025 22:53:43 - INFO - training.fm_trainer - Step 6490/210000 (3.09%): loss=1.0643, lr=1.21e-04, step_time=1830.1ms, ETA 4d 8h
12/02/2025 22:54:01 - INFO - training.fm_trainer - Step 6500/210000 (3.10%): loss=10.4783, lr=1.22e-04, step_time=1835.4ms, ETA 4d 8h
12/02/2025 22:54:01 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:54:27 - INFO - training.fm_trainer - Eval Step 6500: loss=3.3796, ppl=29.36
12/02/2025 22:54:27 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-6500
12/02/2025 22:54:27 - INFO - accelerate.accelerator - Saving FSDP model
12/02/2025 22:54:28 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-6500/pytorch_model_fsdp_0
12/02/2025 22:54:36 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-6500/pytorch_model_fsdp_0
12/02/2025 22:54:36 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-6500
12/02/2025 22:54:36 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/02/2025 22:54:39 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-6500/optimizer_0
12/02/2025 22:54:55 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-6500/optimizer_0
12/02/2025 22:54:55 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-6500
12/02/2025 22:54:55 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-6500/scheduler.bin
12/02/2025 22:54:55 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-6500/sampler.bin
12/02/2025 22:54:55 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-6500/sampler_1.bin
12/02/2025 22:54:55 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-6500/random_states_0.pkl
12/02/2025 22:54:55 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-6500
12/02/2025 22:54:55 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-5500
12/02/2025 22:55:16 - INFO - training.fm_trainer - Step 6510/210000 (3.10%): loss=5.8624, lr=1.22e-04, step_time=1934.2ms, ETA 4d 8h
12/02/2025 22:55:35 - INFO - training.fm_trainer - Step 6520/210000 (3.10%): loss=1.2362, lr=1.22e-04, step_time=1847.7ms, ETA 4d 8h
12/02/2025 22:55:53 - INFO - training.fm_trainer - Step 6530/210000 (3.11%): loss=8.3565, lr=1.22e-04, step_time=1807.5ms, ETA 4d 8h
12/02/2025 22:56:12 - INFO - training.fm_trainer - Step 6540/210000 (3.11%): loss=1.0822, lr=1.22e-04, step_time=1861.1ms, ETA 4d 8h
12/02/2025 22:56:30 - INFO - training.fm_trainer - Step 6550/210000 (3.12%): loss=0.4495, lr=1.22e-04, step_time=1816.3ms, ETA 4d 8h
12/02/2025 22:56:30 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:56:56 - INFO - training.fm_trainer - Eval Step 6550: loss=3.2859, ppl=26.73
12/02/2025 22:57:15 - INFO - training.fm_trainer - Step 6560/210000 (3.12%): loss=5.2389, lr=1.23e-04, grad_norm=0.05, step_time=1850.5ms, ETA 4d 8h
12/02/2025 22:57:33 - INFO - training.fm_trainer - Step 6570/210000 (3.13%): loss=1.3697, lr=1.23e-04, step_time=1843.1ms, ETA 4d 8h
12/02/2025 22:57:51 - INFO - training.fm_trainer - Step 6580/210000 (3.13%): loss=0.9815, lr=1.23e-04, step_time=1871.7ms, ETA 4d 8h
12/02/2025 22:58:10 - INFO - training.fm_trainer - Step 6590/210000 (3.14%): loss=4.5813, lr=1.23e-04, step_time=1973.5ms, ETA 4d 9h
12/02/2025 22:58:28 - INFO - training.fm_trainer - Step 6600/210000 (3.14%): loss=3.3710, lr=1.24e-04, step_time=1930.2ms, ETA 4d 9h
12/02/2025 22:58:28 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 22:58:56 - INFO - training.fm_trainer - Eval Step 6600: loss=3.3424, ppl=28.29
12/02/2025 22:59:14 - INFO - training.fm_trainer - Step 6610/210000 (3.15%): loss=2.4408, lr=1.24e-04, step_time=1844.0ms, ETA 4d 9h
12/02/2025 22:59:33 - INFO - training.fm_trainer - Step 6620/210000 (3.15%): loss=3.9790, lr=1.24e-04, step_time=1826.0ms, ETA 4d 9h
12/02/2025 22:59:51 - INFO - training.fm_trainer - Step 6630/210000 (3.16%): loss=1.5183, lr=1.24e-04, step_time=1868.1ms, ETA 4d 9h
12/02/2025 23:00:10 - INFO - training.fm_trainer - Step 6640/210000 (3.16%): loss=1.7927, lr=1.24e-04, step_time=1844.6ms, ETA 4d 9h
12/02/2025 23:00:28 - INFO - training.fm_trainer - Step 6650/210000 (3.17%): loss=7.6046, lr=1.24e-04, step_time=1835.7ms, ETA 4d 8h
12/02/2025 23:00:28 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:00:55 - INFO - training.fm_trainer - Eval Step 6650: loss=3.4675, ppl=32.06
12/02/2025 23:01:13 - INFO - training.fm_trainer - Step 6660/210000 (3.17%): loss=1.4858, lr=1.25e-04, step_time=1857.5ms, ETA 4d 8h
12/02/2025 23:01:31 - INFO - training.fm_trainer - Step 6670/210000 (3.18%): loss=4.5890, lr=1.25e-04, step_time=1822.3ms, ETA 4d 8h
12/02/2025 23:01:50 - INFO - training.fm_trainer - Step 6680/210000 (3.18%): loss=4.1807, lr=1.25e-04, step_time=1817.7ms, ETA 4d 8h
12/02/2025 23:02:08 - INFO - training.fm_trainer - Step 6690/210000 (3.19%): loss=1.7799, lr=1.25e-04, step_time=1855.5ms, ETA 4d 8h
12/02/2025 23:02:27 - INFO - training.fm_trainer - Step 6700/210000 (3.19%): loss=7.9529, lr=1.25e-04, step_time=1850.8ms, ETA 4d 8h
12/02/2025 23:02:27 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:02:53 - INFO - training.fm_trainer - Eval Step 6700: loss=3.5633, ppl=35.28
12/02/2025 23:03:11 - INFO - training.fm_trainer - Step 6710/210000 (3.20%): loss=1.3789, lr=1.25e-04, step_time=1831.1ms, ETA 4d 8h
12/02/2025 23:03:30 - INFO - training.fm_trainer - Step 6720/210000 (3.20%): loss=7.3439, lr=1.26e-04, grad_norm=0.18, step_time=2027.6ms, ETA 4d 9h
12/02/2025 23:03:48 - INFO - training.fm_trainer - Step 6730/210000 (3.20%): loss=1.5779, lr=1.26e-04, step_time=1829.0ms, ETA 4d 9h
12/02/2025 23:04:06 - INFO - training.fm_trainer - Step 6740/210000 (3.21%): loss=4.7442, lr=1.26e-04, step_time=1819.1ms, ETA 4d 8h
12/02/2025 23:04:25 - INFO - training.fm_trainer - Step 6750/210000 (3.21%): loss=3.7195, lr=1.26e-04, step_time=1826.1ms, ETA 4d 8h
12/02/2025 23:04:25 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:04:51 - INFO - training.fm_trainer - Eval Step 6750: loss=3.5855, ppl=36.07
12/02/2025 23:05:09 - INFO - training.fm_trainer - Step 6760/210000 (3.22%): loss=1.3545, lr=1.27e-04, step_time=1834.4ms, ETA 4d 8h
12/02/2025 23:05:28 - INFO - training.fm_trainer - Step 6770/210000 (3.22%): loss=1.8484, lr=1.27e-04, step_time=1827.6ms, ETA 4d 8h
12/02/2025 23:05:47 - INFO - training.fm_trainer - Step 6780/210000 (3.23%): loss=5.6817, lr=1.27e-04, step_time=1805.5ms, ETA 4d 8h
12/02/2025 23:06:05 - INFO - training.fm_trainer - Step 6790/210000 (3.23%): loss=1.4288, lr=1.27e-04, step_time=1860.5ms, ETA 4d 8h
12/02/2025 23:06:24 - INFO - training.fm_trainer - Step 6800/210000 (3.24%): loss=1.1131, lr=1.27e-04, step_time=1825.8ms, ETA 4d 8h
12/02/2025 23:06:24 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:06:49 - INFO - training.fm_trainer - Eval Step 6800: loss=3.3250, ppl=27.80
12/02/2025 23:07:08 - INFO - training.fm_trainer - Step 6810/210000 (3.24%): loss=1.4878, lr=1.27e-04, step_time=1826.9ms, ETA 4d 8h
12/02/2025 23:07:26 - INFO - training.fm_trainer - Step 6820/210000 (3.25%): loss=2.8037, lr=1.28e-04, step_time=1900.7ms, ETA 4d 8h
12/02/2025 23:07:45 - INFO - training.fm_trainer - Step 6830/210000 (3.25%): loss=5.2891, lr=1.28e-04, step_time=1807.2ms, ETA 4d 8h
12/02/2025 23:08:03 - INFO - training.fm_trainer - Step 6840/210000 (3.26%): loss=2.4781, lr=1.28e-04, step_time=1833.4ms, ETA 4d 8h
12/02/2025 23:08:22 - INFO - training.fm_trainer - Step 6850/210000 (3.26%): loss=1.4597, lr=1.28e-04, step_time=1825.7ms, ETA 4d 7h
12/02/2025 23:08:22 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:08:48 - INFO - training.fm_trainer - Eval Step 6850: loss=3.2328, ppl=25.35
12/02/2025 23:09:06 - INFO - training.fm_trainer - Step 6860/210000 (3.27%): loss=0.7374, lr=1.28e-04, step_time=1831.3ms, ETA 4d 7h
12/02/2025 23:09:24 - INFO - training.fm_trainer - Step 6870/210000 (3.27%): loss=1.4872, lr=1.28e-04, step_time=1801.2ms, ETA 4d 7h
12/02/2025 23:09:43 - INFO - training.fm_trainer - Step 6880/210000 (3.28%): loss=1.9110, lr=1.29e-04, grad_norm=0.07, step_time=1908.0ms, ETA 4d 8h
12/02/2025 23:10:01 - INFO - training.fm_trainer - Step 6890/210000 (3.28%): loss=9.4104, lr=1.29e-04, step_time=1836.8ms, ETA 4d 8h
12/02/2025 23:10:19 - INFO - training.fm_trainer - Step 6900/210000 (3.29%): loss=0.9624, lr=1.29e-04, step_time=1828.5ms, ETA 4d 7h
12/02/2025 23:10:19 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:10:45 - INFO - training.fm_trainer - Eval Step 6900: loss=3.3293, ppl=27.92
12/02/2025 23:11:03 - INFO - training.fm_trainer - Step 6910/210000 (3.29%): loss=1.4105, lr=1.29e-04, step_time=1827.1ms, ETA 4d 7h
12/02/2025 23:11:22 - INFO - training.fm_trainer - Step 6920/210000 (3.30%): loss=0.3978, lr=1.30e-04, step_time=1829.5ms, ETA 4d 7h
12/02/2025 23:11:40 - INFO - training.fm_trainer - Step 6930/210000 (3.30%): loss=2.0467, lr=1.30e-04, step_time=1870.5ms, ETA 4d 7h
12/02/2025 23:11:59 - INFO - training.fm_trainer - Step 6940/210000 (3.30%): loss=0.2916, lr=1.30e-04, step_time=1937.0ms, ETA 4d 8h
12/02/2025 23:12:17 - INFO - training.fm_trainer - Step 6950/210000 (3.31%): loss=6.7016, lr=1.30e-04, step_time=1831.9ms, ETA 4d 8h
12/02/2025 23:12:17 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:12:44 - INFO - training.fm_trainer - Eval Step 6950: loss=3.4987, ppl=33.07
12/02/2025 23:13:03 - INFO - training.fm_trainer - Step 6960/210000 (3.31%): loss=2.9485, lr=1.30e-04, step_time=1817.7ms, ETA 4d 8h
12/02/2025 23:13:21 - INFO - training.fm_trainer - Step 6970/210000 (3.32%): loss=0.6563, lr=1.30e-04, step_time=1840.5ms, ETA 4d 8h
12/02/2025 23:13:40 - INFO - training.fm_trainer - Step 6980/210000 (3.32%): loss=2.9183, lr=1.31e-04, step_time=1873.6ms, ETA 4d 8h
12/02/2025 23:13:58 - INFO - training.fm_trainer - Step 6990/210000 (3.33%): loss=3.2672, lr=1.31e-04, step_time=1834.8ms, ETA 4d 8h
12/02/2025 23:14:17 - INFO - training.fm_trainer - Step 7000/210000 (3.33%): loss=1.6810, lr=1.31e-04, step_time=1818.0ms, ETA 4d 8h
12/02/2025 23:14:17 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:14:43 - INFO - training.fm_trainer - Eval Step 7000: loss=3.5692, ppl=35.49
12/02/2025 23:14:43 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-7000
12/02/2025 23:14:43 - INFO - accelerate.accelerator - Saving FSDP model
12/02/2025 23:14:44 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-7000/pytorch_model_fsdp_0
12/02/2025 23:14:52 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-7000/pytorch_model_fsdp_0
12/02/2025 23:14:52 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-7000
12/02/2025 23:14:52 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/02/2025 23:14:55 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-7000/optimizer_0
12/02/2025 23:15:10 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-7000/optimizer_0
12/02/2025 23:15:11 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-7000
12/02/2025 23:15:11 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-7000/scheduler.bin
12/02/2025 23:15:11 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-7000/sampler.bin
12/02/2025 23:15:11 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-7000/sampler_1.bin
12/02/2025 23:15:11 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-7000/random_states_0.pkl
12/02/2025 23:15:11 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-7000
12/02/2025 23:15:11 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-6000
12/02/2025 23:15:31 - INFO - training.fm_trainer - Step 7010/210000 (3.34%): loss=5.1647, lr=1.31e-04, step_time=1839.8ms, ETA 4d 7h
12/02/2025 23:15:49 - INFO - training.fm_trainer - Step 7020/210000 (3.34%): loss=5.2952, lr=1.31e-04, step_time=1851.5ms, ETA 4d 8h
12/02/2025 23:16:07 - INFO - training.fm_trainer - Step 7030/210000 (3.35%): loss=1.8764, lr=1.31e-04, step_time=1829.9ms, ETA 4d 7h
12/02/2025 23:16:26 - INFO - training.fm_trainer - Step 7040/210000 (3.35%): loss=1.1109, lr=1.32e-04, grad_norm=0.08, step_time=1851.6ms, ETA 4d 7h
12/02/2025 23:16:44 - INFO - training.fm_trainer - Step 7050/210000 (3.36%): loss=1.7722, lr=1.32e-04, step_time=1813.6ms, ETA 4d 7h
12/02/2025 23:16:44 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:17:10 - INFO - training.fm_trainer - Eval Step 7050: loss=3.4013, ppl=30.00
12/02/2025 23:17:29 - INFO - training.fm_trainer - Step 7060/210000 (3.36%): loss=4.4765, lr=1.32e-04, step_time=1852.8ms, ETA 4d 7h
12/02/2025 23:17:47 - INFO - training.fm_trainer - Step 7070/210000 (3.37%): loss=8.7925, lr=1.32e-04, step_time=1835.8ms, ETA 4d 7h
12/02/2025 23:18:06 - INFO - training.fm_trainer - Step 7080/210000 (3.37%): loss=2.2674, lr=1.33e-04, step_time=1822.2ms, ETA 4d 7h
12/02/2025 23:18:24 - INFO - training.fm_trainer - Step 7090/210000 (3.38%): loss=4.6665, lr=1.33e-04, step_time=1831.9ms, ETA 4d 7h
12/02/2025 23:18:43 - INFO - training.fm_trainer - Step 7100/210000 (3.38%): loss=1.8818, lr=1.33e-04, step_time=1824.7ms, ETA 4d 7h
12/02/2025 23:18:43 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:19:09 - INFO - training.fm_trainer - Eval Step 7100: loss=3.3459, ppl=28.39
12/02/2025 23:19:27 - INFO - training.fm_trainer - Step 7110/210000 (3.39%): loss=2.0661, lr=1.33e-04, step_time=1835.6ms, ETA 4d 7h
12/02/2025 23:19:46 - INFO - training.fm_trainer - Step 7120/210000 (3.39%): loss=0.7574, lr=1.33e-04, step_time=1840.1ms, ETA 4d 7h
12/02/2025 23:20:04 - INFO - training.fm_trainer - Step 7130/210000 (3.40%): loss=0.9646, lr=1.33e-04, step_time=1814.7ms, ETA 4d 7h
12/02/2025 23:20:23 - INFO - training.fm_trainer - Step 7140/210000 (3.40%): loss=0.4685, lr=1.34e-04, step_time=1853.1ms, ETA 4d 7h
12/02/2025 23:20:41 - INFO - training.fm_trainer - Step 7150/210000 (3.40%): loss=2.2237, lr=1.34e-04, step_time=1843.8ms, ETA 4d 7h
12/02/2025 23:20:41 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:21:07 - INFO - training.fm_trainer - Eval Step 7150: loss=3.2569, ppl=25.97
12/02/2025 23:21:25 - INFO - training.fm_trainer - Step 7160/210000 (3.41%): loss=0.7485, lr=1.34e-04, step_time=1829.9ms, ETA 4d 7h
12/02/2025 23:21:44 - INFO - training.fm_trainer - Step 7170/210000 (3.41%): loss=3.5491, lr=1.34e-04, step_time=1825.3ms, ETA 4d 7h
12/02/2025 23:22:02 - INFO - training.fm_trainer - Step 7180/210000 (3.42%): loss=1.6989, lr=1.34e-04, step_time=2014.9ms, ETA 4d 8h
12/02/2025 23:22:21 - INFO - training.fm_trainer - Step 7190/210000 (3.42%): loss=1.1421, lr=1.34e-04, step_time=1851.8ms, ETA 4d 8h
12/02/2025 23:22:39 - INFO - training.fm_trainer - Step 7200/210000 (3.43%): loss=1.1198, lr=1.35e-04, grad_norm=0.04, step_time=1848.4ms, ETA 4d 8h
12/02/2025 23:22:39 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:23:05 - INFO - training.fm_trainer - Eval Step 7200: loss=3.3391, ppl=28.19
12/02/2025 23:23:24 - INFO - training.fm_trainer - Step 7210/210000 (3.43%): loss=0.8456, lr=1.35e-04, step_time=1821.4ms, ETA 4d 8h
12/02/2025 23:23:42 - INFO - training.fm_trainer - Step 7220/210000 (3.44%): loss=1.6772, lr=1.35e-04, step_time=1823.8ms, ETA 4d 8h
12/02/2025 23:24:00 - INFO - training.fm_trainer - Step 7230/210000 (3.44%): loss=1.4933, lr=1.35e-04, step_time=1806.9ms, ETA 4d 7h
12/02/2025 23:24:19 - INFO - training.fm_trainer - Step 7240/210000 (3.45%): loss=1.0479, lr=1.36e-04, step_time=1840.6ms, ETA 4d 7h
12/02/2025 23:24:37 - INFO - training.fm_trainer - Step 7250/210000 (3.45%): loss=0.7672, lr=1.36e-04, step_time=1823.0ms, ETA 4d 7h
12/02/2025 23:24:37 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:25:04 - INFO - training.fm_trainer - Eval Step 7250: loss=3.4068, ppl=30.17
12/02/2025 23:25:22 - INFO - training.fm_trainer - Step 7260/210000 (3.46%): loss=2.8887, lr=1.36e-04, step_time=1807.7ms, ETA 4d 7h
12/02/2025 23:25:41 - INFO - training.fm_trainer - Step 7270/210000 (3.46%): loss=2.0218, lr=1.36e-04, step_time=1823.7ms, ETA 4d 7h
12/02/2025 23:25:59 - INFO - training.fm_trainer - Step 7280/210000 (3.47%): loss=5.5143, lr=1.36e-04, step_time=1830.0ms, ETA 4d 7h
12/02/2025 23:26:18 - INFO - training.fm_trainer - Step 7290/210000 (3.47%): loss=6.8802, lr=1.36e-04, step_time=1822.1ms, ETA 4d 7h
12/02/2025 23:26:36 - INFO - training.fm_trainer - Step 7300/210000 (3.48%): loss=4.4198, lr=1.37e-04, step_time=1858.6ms, ETA 4d 7h
12/02/2025 23:26:36 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:27:02 - INFO - training.fm_trainer - Eval Step 7300: loss=3.5019, ppl=33.18
12/02/2025 23:27:20 - INFO - training.fm_trainer - Step 7310/210000 (3.48%): loss=11.7124, lr=1.37e-04, step_time=1828.1ms, ETA 4d 7h
12/02/2025 23:27:39 - INFO - training.fm_trainer - Step 7320/210000 (3.49%): loss=4.8236, lr=1.37e-04, step_time=1815.7ms, ETA 4d 7h
12/02/2025 23:27:57 - INFO - training.fm_trainer - Step 7330/210000 (3.49%): loss=9.9869, lr=1.37e-04, step_time=1818.6ms, ETA 4d 7h
12/02/2025 23:28:15 - INFO - training.fm_trainer - Step 7340/210000 (3.50%): loss=1.7380, lr=1.37e-04, step_time=1847.5ms, ETA 4d 7h
12/02/2025 23:28:34 - INFO - training.fm_trainer - Step 7350/210000 (3.50%): loss=2.3064, lr=1.37e-04, step_time=1813.2ms, ETA 4d 7h
12/02/2025 23:28:34 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:29:00 - INFO - training.fm_trainer - Eval Step 7350: loss=3.5005, ppl=33.13
12/02/2025 23:29:18 - INFO - training.fm_trainer - Step 7360/210000 (3.50%): loss=2.6900, lr=1.38e-04, grad_norm=0.14, step_time=1907.4ms, ETA 4d 7h
12/02/2025 23:29:37 - INFO - training.fm_trainer - Step 7370/210000 (3.51%): loss=7.3045, lr=1.38e-04, step_time=1826.2ms, ETA 4d 7h
12/02/2025 23:29:55 - INFO - training.fm_trainer - Step 7380/210000 (3.51%): loss=7.1888, lr=1.38e-04, step_time=1844.9ms, ETA 4d 7h
12/02/2025 23:30:14 - INFO - training.fm_trainer - Step 7390/210000 (3.52%): loss=1.8667, lr=1.38e-04, step_time=1927.4ms, ETA 4d 7h
12/02/2025 23:30:33 - INFO - training.fm_trainer - Step 7400/210000 (3.52%): loss=7.9441, lr=1.39e-04, step_time=1902.6ms, ETA 4d 8h
12/02/2025 23:30:33 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:31:01 - INFO - training.fm_trainer - Eval Step 7400: loss=3.3087, ppl=27.35
12/02/2025 23:31:21 - INFO - training.fm_trainer - Step 7410/210000 (3.53%): loss=2.8992, lr=1.39e-04, step_time=1854.1ms, ETA 4d 8h
12/02/2025 23:31:39 - INFO - training.fm_trainer - Step 7420/210000 (3.53%): loss=1.1425, lr=1.39e-04, step_time=1900.7ms, ETA 4d 8h
12/02/2025 23:31:58 - INFO - training.fm_trainer - Step 7430/210000 (3.54%): loss=3.5417, lr=1.39e-04, step_time=1889.2ms, ETA 4d 8h
12/02/2025 23:32:16 - INFO - training.fm_trainer - Step 7440/210000 (3.54%): loss=0.8098, lr=1.39e-04, step_time=1829.2ms, ETA 4d 8h
12/02/2025 23:32:35 - INFO - training.fm_trainer - Step 7450/210000 (3.55%): loss=0.7780, lr=1.39e-04, step_time=1853.2ms, ETA 4d 8h
12/02/2025 23:32:35 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:33:03 - INFO - training.fm_trainer - Eval Step 7450: loss=3.2286, ppl=25.24
12/02/2025 23:33:21 - INFO - training.fm_trainer - Step 7460/210000 (3.55%): loss=1.0362, lr=1.40e-04, step_time=1841.0ms, ETA 4d 8h
12/02/2025 23:33:40 - INFO - training.fm_trainer - Step 7470/210000 (3.56%): loss=1.1146, lr=1.40e-04, step_time=1812.8ms, ETA 4d 8h
12/02/2025 23:33:58 - INFO - training.fm_trainer - Step 7480/210000 (3.56%): loss=1.5716, lr=1.40e-04, step_time=1836.7ms, ETA 4d 8h
12/02/2025 23:34:16 - INFO - training.fm_trainer - Step 7490/210000 (3.57%): loss=2.5968, lr=1.40e-04, step_time=1855.9ms, ETA 4d 8h
12/02/2025 23:34:35 - INFO - training.fm_trainer - Step 7500/210000 (3.57%): loss=2.9154, lr=1.40e-04, step_time=1847.7ms, ETA 4d 8h
12/02/2025 23:34:35 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:35:01 - INFO - training.fm_trainer - Eval Step 7500: loss=3.2978, ppl=27.05
12/02/2025 23:35:01 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-7500
12/02/2025 23:35:01 - INFO - accelerate.accelerator - Saving FSDP model
12/02/2025 23:35:02 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-7500/pytorch_model_fsdp_0
12/02/2025 23:35:09 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-7500/pytorch_model_fsdp_0
12/02/2025 23:35:09 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-7500
12/02/2025 23:35:09 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/02/2025 23:35:12 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-7500/optimizer_0
12/02/2025 23:35:27 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-7500/optimizer_0
12/02/2025 23:35:27 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-7500
12/02/2025 23:35:27 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-7500/scheduler.bin
12/02/2025 23:35:27 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-7500/sampler.bin
12/02/2025 23:35:27 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-7500/sampler_1.bin
12/02/2025 23:35:27 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-7500/random_states_0.pkl
12/02/2025 23:35:27 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-7500
12/02/2025 23:35:27 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-6500
12/02/2025 23:35:47 - INFO - training.fm_trainer - Step 7510/210000 (3.58%): loss=1.7441, lr=1.40e-04, step_time=1819.5ms, ETA 4d 7h
12/02/2025 23:36:05 - INFO - training.fm_trainer - Step 7520/210000 (3.58%): loss=4.4643, lr=1.41e-04, grad_norm=0.11, step_time=1860.4ms, ETA 4d 7h
12/02/2025 23:36:24 - INFO - training.fm_trainer - Step 7530/210000 (3.59%): loss=1.9156, lr=1.41e-04, step_time=1830.6ms, ETA 4d 7h
12/02/2025 23:36:42 - INFO - training.fm_trainer - Step 7540/210000 (3.59%): loss=1.1795, lr=1.41e-04, step_time=1828.5ms, ETA 4d 7h
12/02/2025 23:37:01 - INFO - training.fm_trainer - Step 7550/210000 (3.60%): loss=1.8100, lr=1.41e-04, step_time=1830.5ms, ETA 4d 7h
12/02/2025 23:37:01 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:37:27 - INFO - training.fm_trainer - Eval Step 7550: loss=3.2933, ppl=26.93
12/02/2025 23:37:45 - INFO - training.fm_trainer - Step 7560/210000 (3.60%): loss=4.6926, lr=1.42e-04, step_time=1832.9ms, ETA 4d 7h
12/02/2025 23:38:04 - INFO - training.fm_trainer - Step 7570/210000 (3.60%): loss=1.8503, lr=1.42e-04, step_time=1842.7ms, ETA 4d 7h
12/02/2025 23:38:23 - INFO - training.fm_trainer - Step 7580/210000 (3.61%): loss=6.2357, lr=1.42e-04, step_time=1842.6ms, ETA 4d 7h
12/02/2025 23:38:41 - INFO - training.fm_trainer - Step 7590/210000 (3.61%): loss=0.7629, lr=1.42e-04, step_time=1815.9ms, ETA 4d 7h
12/02/2025 23:39:00 - INFO - training.fm_trainer - Step 7600/210000 (3.62%): loss=1.1595, lr=1.42e-04, step_time=1895.3ms, ETA 4d 7h
12/02/2025 23:39:00 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:39:27 - INFO - training.fm_trainer - Eval Step 7600: loss=3.2254, ppl=25.16
12/02/2025 23:39:46 - INFO - training.fm_trainer - Step 7610/210000 (3.62%): loss=4.9961, lr=1.42e-04, step_time=1842.4ms, ETA 4d 7h
12/02/2025 23:40:04 - INFO - training.fm_trainer - Step 7620/210000 (3.63%): loss=4.9477, lr=1.43e-04, step_time=1951.4ms, ETA 4d 8h
12/02/2025 23:40:23 - INFO - training.fm_trainer - Step 7630/210000 (3.63%): loss=0.8432, lr=1.43e-04, step_time=1838.6ms, ETA 4d 8h
12/02/2025 23:40:41 - INFO - training.fm_trainer - Step 7640/210000 (3.64%): loss=5.6317, lr=1.43e-04, step_time=1807.3ms, ETA 4d 7h
12/02/2025 23:41:00 - INFO - training.fm_trainer - Step 7650/210000 (3.64%): loss=1.0709, lr=1.43e-04, step_time=1823.8ms, ETA 4d 7h
12/02/2025 23:41:00 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:41:26 - INFO - training.fm_trainer - Eval Step 7650: loss=3.2477, ppl=25.73
12/02/2025 23:41:44 - INFO - training.fm_trainer - Step 7660/210000 (3.65%): loss=1.5104, lr=1.43e-04, step_time=1827.7ms, ETA 4d 7h
12/02/2025 23:42:03 - INFO - training.fm_trainer - Step 7670/210000 (3.65%): loss=0.6096, lr=1.43e-04, step_time=1808.3ms, ETA 4d 7h
12/02/2025 23:42:21 - INFO - training.fm_trainer - Step 7680/210000 (3.66%): loss=0.9948, lr=1.44e-04, grad_norm=0.14, step_time=1891.0ms, ETA 4d 7h
12/02/2025 23:42:40 - INFO - training.fm_trainer - Step 7690/210000 (3.66%): loss=1.5357, lr=1.44e-04, step_time=1832.9ms, ETA 4d 7h
12/02/2025 23:42:58 - INFO - training.fm_trainer - Step 7700/210000 (3.67%): loss=2.8865, lr=1.44e-04, step_time=1855.8ms, ETA 4d 7h
12/02/2025 23:42:58 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:43:24 - INFO - training.fm_trainer - Eval Step 7700: loss=3.2659, ppl=26.20
12/02/2025 23:43:42 - INFO - training.fm_trainer - Step 7710/210000 (3.67%): loss=1.7109, lr=1.44e-04, step_time=1821.6ms, ETA 4d 7h
12/02/2025 23:44:01 - INFO - training.fm_trainer - Step 7720/210000 (3.68%): loss=0.9532, lr=1.45e-04, step_time=1817.9ms, ETA 4d 7h
12/02/2025 23:44:19 - INFO - training.fm_trainer - Step 7730/210000 (3.68%): loss=1.1086, lr=1.45e-04, step_time=1824.2ms, ETA 4d 7h
12/02/2025 23:44:38 - INFO - training.fm_trainer - Step 7740/210000 (3.69%): loss=6.3339, lr=1.45e-04, step_time=2015.8ms, ETA 4d 8h
12/02/2025 23:44:57 - INFO - training.fm_trainer - Step 7750/210000 (3.69%): loss=5.6726, lr=1.45e-04, step_time=1818.2ms, ETA 4d 8h
12/02/2025 23:44:57 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:45:23 - INFO - training.fm_trainer - Eval Step 7750: loss=3.3310, ppl=27.97
12/02/2025 23:45:42 - INFO - training.fm_trainer - Step 7760/210000 (3.70%): loss=9.0132, lr=1.45e-04, step_time=2429.5ms, ETA 4d 11h
12/02/2025 23:46:00 - INFO - training.fm_trainer - Step 7770/210000 (3.70%): loss=6.9306, lr=1.45e-04, step_time=1870.5ms, ETA 4d 11h
12/02/2025 23:46:19 - INFO - training.fm_trainer - Step 7780/210000 (3.70%): loss=1.3827, lr=1.46e-04, step_time=1825.5ms, ETA 4d 10h
12/02/2025 23:46:37 - INFO - training.fm_trainer - Step 7790/210000 (3.71%): loss=0.9664, lr=1.46e-04, step_time=1829.9ms, ETA 4d 10h
12/02/2025 23:46:56 - INFO - training.fm_trainer - Step 7800/210000 (3.71%): loss=5.8897, lr=1.46e-04, step_time=1840.2ms, ETA 4d 9h
12/02/2025 23:46:56 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:47:23 - INFO - training.fm_trainer - Eval Step 7800: loss=3.3035, ppl=27.21
12/02/2025 23:47:42 - INFO - training.fm_trainer - Step 7810/210000 (3.72%): loss=0.8714, lr=1.46e-04, step_time=1834.3ms, ETA 4d 9h
12/02/2025 23:48:00 - INFO - training.fm_trainer - Step 7820/210000 (3.72%): loss=3.6202, lr=1.46e-04, step_time=1831.9ms, ETA 4d 9h
12/02/2025 23:48:19 - INFO - training.fm_trainer - Step 7830/210000 (3.73%): loss=10.1517, lr=1.46e-04, step_time=1848.8ms, ETA 4d 9h
12/02/2025 23:48:37 - INFO - training.fm_trainer - Step 7840/210000 (3.73%): loss=1.0583, lr=1.47e-04, grad_norm=0.03, step_time=1864.3ms, ETA 4d 9h
12/02/2025 23:48:55 - INFO - training.fm_trainer - Step 7850/210000 (3.74%): loss=1.4644, lr=1.47e-04, step_time=1827.9ms, ETA 4d 8h
12/02/2025 23:48:55 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:49:21 - INFO - training.fm_trainer - Eval Step 7850: loss=3.2402, ppl=25.54
12/02/2025 23:49:40 - INFO - training.fm_trainer - Step 7860/210000 (3.74%): loss=0.9656, lr=1.47e-04, step_time=1842.4ms, ETA 4d 8h
12/02/2025 23:49:59 - INFO - training.fm_trainer - Step 7870/210000 (3.75%): loss=0.6882, lr=1.47e-04, step_time=1903.4ms, ETA 4d 8h
12/02/2025 23:50:17 - INFO - training.fm_trainer - Step 7880/210000 (3.75%): loss=1.0752, lr=1.48e-04, step_time=1856.4ms, ETA 4d 8h
12/02/2025 23:50:36 - INFO - training.fm_trainer - Step 7890/210000 (3.76%): loss=2.1224, lr=1.48e-04, step_time=1837.6ms, ETA 4d 8h
12/02/2025 23:50:54 - INFO - training.fm_trainer - Step 7900/210000 (3.76%): loss=3.3240, lr=1.48e-04, step_time=1804.9ms, ETA 4d 8h
12/02/2025 23:50:54 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:51:20 - INFO - training.fm_trainer - Eval Step 7900: loss=3.2104, ppl=24.79
12/02/2025 23:51:39 - INFO - training.fm_trainer - Step 7910/210000 (3.77%): loss=0.8160, lr=1.48e-04, step_time=1835.7ms, ETA 4d 8h
12/02/2025 23:51:57 - INFO - training.fm_trainer - Step 7920/210000 (3.77%): loss=1.5090, lr=1.48e-04, step_time=1816.3ms, ETA 4d 7h
12/02/2025 23:52:16 - INFO - training.fm_trainer - Step 7930/210000 (3.78%): loss=1.5182, lr=1.48e-04, step_time=2055.1ms, ETA 4d 9h
12/02/2025 23:52:34 - INFO - training.fm_trainer - Step 7940/210000 (3.78%): loss=0.6664, lr=1.49e-04, step_time=1849.9ms, ETA 4d 8h
12/02/2025 23:52:53 - INFO - training.fm_trainer - Step 7950/210000 (3.79%): loss=0.7790, lr=1.49e-04, step_time=1834.3ms, ETA 4d 8h
12/02/2025 23:52:53 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:53:19 - INFO - training.fm_trainer - Eval Step 7950: loss=3.2367, ppl=25.45
12/02/2025 23:53:37 - INFO - training.fm_trainer - Step 7960/210000 (3.79%): loss=2.9454, lr=1.49e-04, step_time=1831.8ms, ETA 4d 8h
12/02/2025 23:53:56 - INFO - training.fm_trainer - Step 7970/210000 (3.80%): loss=0.7907, lr=1.49e-04, step_time=1807.8ms, ETA 4d 8h
12/02/2025 23:54:14 - INFO - training.fm_trainer - Step 7980/210000 (3.80%): loss=0.7954, lr=1.49e-04, step_time=1826.1ms, ETA 4d 8h
12/02/2025 23:54:32 - INFO - training.fm_trainer - Step 7990/210000 (3.80%): loss=0.9346, lr=1.49e-04, step_time=1843.9ms, ETA 4d 8h
12/02/2025 23:54:51 - INFO - training.fm_trainer - Step 8000/210000 (3.81%): loss=1.6071, lr=1.50e-04, grad_norm=0.02, step_time=1822.7ms, ETA 4d 7h
12/02/2025 23:54:51 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:55:17 - INFO - training.fm_trainer - Eval Step 8000: loss=3.3266, ppl=27.84
12/02/2025 23:55:17 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-8000
12/02/2025 23:55:17 - INFO - accelerate.accelerator - Saving FSDP model
12/02/2025 23:55:18 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-8000/pytorch_model_fsdp_0
12/02/2025 23:55:25 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-8000/pytorch_model_fsdp_0
12/02/2025 23:55:25 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-8000
12/02/2025 23:55:25 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/02/2025 23:55:28 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-8000/optimizer_0
12/02/2025 23:55:44 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-8000/optimizer_0
12/02/2025 23:55:44 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-8000
12/02/2025 23:55:44 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-8000/scheduler.bin
12/02/2025 23:55:44 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-8000/sampler.bin
12/02/2025 23:55:44 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-8000/sampler_1.bin
12/02/2025 23:55:44 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-8000/random_states_0.pkl
12/02/2025 23:55:44 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-8000
12/02/2025 23:55:44 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-7000
12/02/2025 23:56:06 - INFO - training.fm_trainer - Step 8010/210000 (3.81%): loss=1.0849, lr=1.50e-04, step_time=1885.6ms, ETA 4d 8h
12/02/2025 23:56:25 - INFO - training.fm_trainer - Step 8020/210000 (3.82%): loss=1.2976, lr=1.50e-04, step_time=1891.1ms, ETA 4d 8h
12/02/2025 23:56:43 - INFO - training.fm_trainer - Step 8030/210000 (3.82%): loss=4.4962, lr=1.50e-04, step_time=1817.2ms, ETA 4d 7h
12/02/2025 23:57:02 - INFO - training.fm_trainer - Step 8040/210000 (3.83%): loss=5.7402, lr=1.51e-04, step_time=1832.6ms, ETA 4d 7h
12/02/2025 23:57:20 - INFO - training.fm_trainer - Step 8050/210000 (3.83%): loss=2.9980, lr=1.51e-04, step_time=1842.4ms, ETA 4d 7h
12/02/2025 23:57:20 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:57:46 - INFO - training.fm_trainer - Eval Step 8050: loss=3.3331, ppl=28.03
12/02/2025 23:58:05 - INFO - training.fm_trainer - Step 8060/210000 (3.84%): loss=0.8523, lr=1.51e-04, step_time=1815.5ms, ETA 4d 7h
12/02/2025 23:58:23 - INFO - training.fm_trainer - Step 8070/210000 (3.84%): loss=3.7529, lr=1.51e-04, step_time=1838.7ms, ETA 4d 7h
12/02/2025 23:58:42 - INFO - training.fm_trainer - Step 8080/210000 (3.85%): loss=2.9957, lr=1.51e-04, step_time=1866.0ms, ETA 4d 7h
12/02/2025 23:59:00 - INFO - training.fm_trainer - Step 8090/210000 (3.85%): loss=1.4229, lr=1.51e-04, step_time=1829.9ms, ETA 4d 7h
12/02/2025 23:59:19 - INFO - training.fm_trainer - Step 8100/210000 (3.86%): loss=1.1232, lr=1.52e-04, step_time=1845.2ms, ETA 4d 7h
12/02/2025 23:59:19 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/02/2025 23:59:45 - INFO - training.fm_trainer - Eval Step 8100: loss=3.3121, ppl=27.44
12/03/2025 00:00:05 - INFO - training.fm_trainer - Step 8110/210000 (3.86%): loss=2.5136, lr=1.52e-04, step_time=1869.1ms, ETA 4d 7h
12/03/2025 00:00:23 - INFO - training.fm_trainer - Step 8120/210000 (3.87%): loss=5.5208, lr=1.52e-04, step_time=1812.9ms, ETA 4d 7h
12/03/2025 00:00:42 - INFO - training.fm_trainer - Step 8130/210000 (3.87%): loss=1.6906, lr=1.52e-04, step_time=1839.1ms, ETA 4d 7h
12/03/2025 00:01:00 - INFO - training.fm_trainer - Step 8140/210000 (3.88%): loss=0.6726, lr=1.52e-04, step_time=1828.2ms, ETA 4d 7h
12/03/2025 00:01:19 - INFO - training.fm_trainer - Step 8150/210000 (3.88%): loss=5.1629, lr=1.52e-04, step_time=1824.0ms, ETA 4d 7h
12/03/2025 00:01:19 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:01:45 - INFO - training.fm_trainer - Eval Step 8150: loss=3.2370, ppl=25.46
12/03/2025 00:02:03 - INFO - training.fm_trainer - Step 8160/210000 (3.89%): loss=3.4082, lr=1.53e-04, grad_norm=0.05, step_time=1841.6ms, ETA 4d 7h
12/03/2025 00:02:21 - INFO - training.fm_trainer - Step 8170/210000 (3.89%): loss=1.8768, lr=1.53e-04, step_time=1809.4ms, ETA 4d 7h
12/03/2025 00:02:40 - INFO - training.fm_trainer - Step 8180/210000 (3.90%): loss=2.3821, lr=1.53e-04, step_time=1812.5ms, ETA 4d 6h
12/03/2025 00:02:58 - INFO - training.fm_trainer - Step 8190/210000 (3.90%): loss=1.7499, lr=1.53e-04, step_time=1814.5ms, ETA 4d 6h
12/03/2025 00:03:16 - INFO - training.fm_trainer - Step 8200/210000 (3.90%): loss=1.1444, lr=1.54e-04, step_time=1834.1ms, ETA 4d 6h
12/03/2025 00:03:16 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:03:42 - INFO - training.fm_trainer - Eval Step 8200: loss=3.2287, ppl=25.25
12/03/2025 00:04:00 - INFO - training.fm_trainer - Step 8210/210000 (3.91%): loss=1.1624, lr=1.54e-04, step_time=1848.6ms, ETA 4d 6h
12/03/2025 00:04:19 - INFO - training.fm_trainer - Step 8220/210000 (3.91%): loss=4.3196, lr=1.54e-04, step_time=1922.8ms, ETA 4d 7h
12/03/2025 00:04:37 - INFO - training.fm_trainer - Step 8230/210000 (3.92%): loss=3.6334, lr=1.54e-04, step_time=1844.5ms, ETA 4d 7h
12/03/2025 00:04:56 - INFO - training.fm_trainer - Step 8240/210000 (3.92%): loss=1.1391, lr=1.54e-04, step_time=1974.9ms, ETA 4d 8h
12/03/2025 00:05:14 - INFO - training.fm_trainer - Step 8250/210000 (3.93%): loss=2.9448, lr=1.54e-04, step_time=1926.7ms, ETA 4d 8h
12/03/2025 00:05:14 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:05:40 - INFO - training.fm_trainer - Eval Step 8250: loss=3.2750, ppl=26.44
12/03/2025 00:05:59 - INFO - training.fm_trainer - Step 8260/210000 (3.93%): loss=1.5645, lr=1.55e-04, step_time=1813.4ms, ETA 4d 8h
12/03/2025 00:06:17 - INFO - training.fm_trainer - Step 8270/210000 (3.94%): loss=0.8028, lr=1.55e-04, step_time=1900.2ms, ETA 4d 8h
12/03/2025 00:06:36 - INFO - training.fm_trainer - Step 8280/210000 (3.94%): loss=3.8492, lr=1.55e-04, step_time=1839.1ms, ETA 4d 8h
12/03/2025 00:06:54 - INFO - training.fm_trainer - Step 8290/210000 (3.95%): loss=7.1270, lr=1.55e-04, step_time=1829.1ms, ETA 4d 8h
12/03/2025 00:07:13 - INFO - training.fm_trainer - Step 8300/210000 (3.95%): loss=8.7980, lr=1.55e-04, step_time=1825.0ms, ETA 4d 7h
12/03/2025 00:07:13 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:07:39 - INFO - training.fm_trainer - Eval Step 8300: loss=3.2258, ppl=25.17
12/03/2025 00:07:57 - INFO - training.fm_trainer - Step 8310/210000 (3.96%): loss=2.2258, lr=1.55e-04, step_time=1822.9ms, ETA 4d 7h
12/03/2025 00:08:16 - INFO - training.fm_trainer - Step 8320/210000 (3.96%): loss=3.6174, lr=1.56e-04, grad_norm=0.03, step_time=2171.9ms, ETA 4d 9h
12/03/2025 00:08:34 - INFO - training.fm_trainer - Step 8330/210000 (3.97%): loss=1.1908, lr=1.56e-04, step_time=1820.7ms, ETA 4d 9h
12/03/2025 00:08:53 - INFO - training.fm_trainer - Step 8340/210000 (3.97%): loss=0.7493, lr=1.56e-04, step_time=1851.5ms, ETA 4d 8h
12/03/2025 00:09:11 - INFO - training.fm_trainer - Step 8350/210000 (3.98%): loss=0.4775, lr=1.56e-04, step_time=1825.4ms, ETA 4d 8h
12/03/2025 00:09:11 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:09:38 - INFO - training.fm_trainer - Eval Step 8350: loss=3.2694, ppl=26.30
12/03/2025 00:09:57 - INFO - training.fm_trainer - Step 8360/210000 (3.98%): loss=2.9161, lr=1.57e-04, step_time=1831.0ms, ETA 4d 8h
12/03/2025 00:10:15 - INFO - training.fm_trainer - Step 8370/210000 (3.99%): loss=0.9992, lr=1.57e-04, step_time=1864.8ms, ETA 4d 8h
12/03/2025 00:10:34 - INFO - training.fm_trainer - Step 8380/210000 (3.99%): loss=4.2434, lr=1.57e-04, step_time=1853.0ms, ETA 4d 8h
12/03/2025 00:10:52 - INFO - training.fm_trainer - Step 8390/210000 (4.00%): loss=8.4918, lr=1.57e-04, step_time=1822.5ms, ETA 4d 8h
12/03/2025 00:11:11 - INFO - training.fm_trainer - Step 8400/210000 (4.00%): loss=0.6806, lr=1.57e-04, step_time=1873.1ms, ETA 4d 8h
12/03/2025 00:11:11 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:11:38 - INFO - training.fm_trainer - Eval Step 8400: loss=3.4619, ppl=31.88
12/03/2025 00:11:57 - INFO - training.fm_trainer - Step 8410/210000 (4.00%): loss=1.5105, lr=1.57e-04, step_time=1815.7ms, ETA 4d 7h
12/03/2025 00:12:15 - INFO - training.fm_trainer - Step 8420/210000 (4.01%): loss=4.2388, lr=1.58e-04, step_time=1835.9ms, ETA 4d 7h
12/03/2025 00:12:33 - INFO - training.fm_trainer - Step 8430/210000 (4.01%): loss=5.0676, lr=1.58e-04, step_time=1858.5ms, ETA 4d 7h
12/03/2025 00:12:52 - INFO - training.fm_trainer - Step 8440/210000 (4.02%): loss=1.0675, lr=1.58e-04, step_time=1840.1ms, ETA 4d 7h
12/03/2025 00:13:10 - INFO - training.fm_trainer - Step 8450/210000 (4.02%): loss=6.2980, lr=1.58e-04, step_time=1822.8ms, ETA 4d 7h
12/03/2025 00:13:10 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:13:37 - INFO - training.fm_trainer - Eval Step 8450: loss=3.5475, ppl=34.73
12/03/2025 00:13:55 - INFO - training.fm_trainer - Step 8460/210000 (4.03%): loss=0.6164, lr=1.58e-04, step_time=1832.3ms, ETA 4d 7h
12/03/2025 00:14:14 - INFO - training.fm_trainer - Step 8470/210000 (4.03%): loss=1.6764, lr=1.58e-04, step_time=1868.2ms, ETA 4d 7h
12/03/2025 00:14:33 - INFO - training.fm_trainer - Step 8480/210000 (4.04%): loss=0.9700, lr=1.59e-04, grad_norm=0.13, step_time=2120.9ms, ETA 4d 9h
12/03/2025 00:14:52 - INFO - training.fm_trainer - Step 8490/210000 (4.04%): loss=5.6362, lr=1.59e-04, step_time=1821.3ms, ETA 4d 8h
12/03/2025 00:15:10 - INFO - training.fm_trainer - Step 8500/210000 (4.05%): loss=2.1940, lr=1.59e-04, step_time=1816.7ms, ETA 4d 8h
12/03/2025 00:15:10 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:15:36 - INFO - training.fm_trainer - Eval Step 8500: loss=3.4777, ppl=32.39
12/03/2025 00:15:36 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-8500
12/03/2025 00:15:36 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 00:15:37 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-8500/pytorch_model_fsdp_0
12/03/2025 00:15:45 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-8500/pytorch_model_fsdp_0
12/03/2025 00:15:45 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-8500
12/03/2025 00:15:45 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 00:15:48 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-8500/optimizer_0
12/03/2025 00:16:03 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-8500/optimizer_0
12/03/2025 00:16:04 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-8500
12/03/2025 00:16:04 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-8500/scheduler.bin
12/03/2025 00:16:04 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-8500/sampler.bin
12/03/2025 00:16:04 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-8500/sampler_1.bin
12/03/2025 00:16:04 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-8500/random_states_0.pkl
12/03/2025 00:16:04 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-8500
12/03/2025 00:16:04 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-7500
12/03/2025 00:16:26 - INFO - training.fm_trainer - Step 8510/210000 (4.05%): loss=1.0787, lr=1.59e-04, step_time=1860.4ms, ETA 4d 8h
12/03/2025 00:16:44 - INFO - training.fm_trainer - Step 8520/210000 (4.06%): loss=1.7273, lr=1.60e-04, step_time=1829.0ms, ETA 4d 8h
12/03/2025 00:17:03 - INFO - training.fm_trainer - Step 8530/210000 (4.06%): loss=1.9248, lr=1.60e-04, step_time=1821.4ms, ETA 4d 7h
12/03/2025 00:17:21 - INFO - training.fm_trainer - Step 8540/210000 (4.07%): loss=1.3728, lr=1.60e-04, step_time=1949.0ms, ETA 4d 8h
12/03/2025 00:17:40 - INFO - training.fm_trainer - Step 8550/210000 (4.07%): loss=4.0882, lr=1.60e-04, step_time=1830.1ms, ETA 4d 8h
12/03/2025 00:17:40 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:18:08 - INFO - training.fm_trainer - Eval Step 8550: loss=3.2803, ppl=26.58
12/03/2025 00:18:26 - INFO - training.fm_trainer - Step 8560/210000 (4.08%): loss=0.5214, lr=1.60e-04, step_time=1845.5ms, ETA 4d 8h
12/03/2025 00:18:45 - INFO - training.fm_trainer - Step 8570/210000 (4.08%): loss=8.2830, lr=1.60e-04, step_time=1806.2ms, ETA 4d 7h
12/03/2025 00:19:03 - INFO - training.fm_trainer - Step 8580/210000 (4.09%): loss=1.6979, lr=1.61e-04, step_time=1825.7ms, ETA 4d 7h
12/03/2025 00:19:22 - INFO - training.fm_trainer - Step 8590/210000 (4.09%): loss=0.8989, lr=1.61e-04, step_time=1831.6ms, ETA 4d 7h
12/03/2025 00:19:40 - INFO - training.fm_trainer - Step 8600/210000 (4.10%): loss=2.5036, lr=1.61e-04, step_time=1840.8ms, ETA 4d 7h
12/03/2025 00:19:40 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:20:06 - INFO - training.fm_trainer - Eval Step 8600: loss=3.2401, ppl=25.54
12/03/2025 00:20:25 - INFO - training.fm_trainer - Step 8610/210000 (4.10%): loss=4.5569, lr=1.61e-04, step_time=1827.2ms, ETA 4d 7h
12/03/2025 00:20:43 - INFO - training.fm_trainer - Step 8620/210000 (4.10%): loss=1.8251, lr=1.61e-04, step_time=1814.1ms, ETA 4d 7h
12/03/2025 00:21:01 - INFO - training.fm_trainer - Step 8630/210000 (4.11%): loss=0.9646, lr=1.61e-04, step_time=1805.6ms, ETA 4d 6h
12/03/2025 00:21:20 - INFO - training.fm_trainer - Step 8640/210000 (4.11%): loss=7.5914, lr=1.62e-04, grad_norm=0.08, step_time=1851.7ms, ETA 4d 7h
12/03/2025 00:21:38 - INFO - training.fm_trainer - Step 8650/210000 (4.12%): loss=1.1700, lr=1.62e-04, step_time=1891.3ms, ETA 4d 7h
12/03/2025 00:21:38 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:22:04 - INFO - training.fm_trainer - Eval Step 8650: loss=3.3409, ppl=28.25
12/03/2025 00:22:23 - INFO - training.fm_trainer - Step 8660/210000 (4.12%): loss=2.1806, lr=1.62e-04, step_time=1873.4ms, ETA 4d 7h
12/03/2025 00:22:41 - INFO - training.fm_trainer - Step 8670/210000 (4.13%): loss=4.8112, lr=1.62e-04, step_time=1927.7ms, ETA 4d 7h
12/03/2025 00:22:59 - INFO - training.fm_trainer - Step 8680/210000 (4.13%): loss=1.2784, lr=1.63e-04, step_time=1826.5ms, ETA 4d 7h
12/03/2025 00:23:18 - INFO - training.fm_trainer - Step 8690/210000 (4.14%): loss=2.1579, lr=1.63e-04, step_time=1847.5ms, ETA 4d 7h
12/03/2025 00:23:36 - INFO - training.fm_trainer - Step 8700/210000 (4.14%): loss=1.2738, lr=1.63e-04, step_time=1838.4ms, ETA 4d 7h
12/03/2025 00:23:36 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:24:02 - INFO - training.fm_trainer - Eval Step 8700: loss=3.3835, ppl=29.47
12/03/2025 00:24:20 - INFO - training.fm_trainer - Step 8710/210000 (4.15%): loss=1.5823, lr=1.63e-04, step_time=1807.3ms, ETA 4d 7h
12/03/2025 00:24:39 - INFO - training.fm_trainer - Step 8720/210000 (4.15%): loss=3.6748, lr=1.63e-04, step_time=1837.5ms, ETA 4d 7h
12/03/2025 00:24:58 - INFO - training.fm_trainer - Step 8730/210000 (4.16%): loss=8.4256, lr=1.63e-04, step_time=1833.0ms, ETA 4d 7h
12/03/2025 00:25:16 - INFO - training.fm_trainer - Step 8740/210000 (4.16%): loss=1.1042, lr=1.64e-04, step_time=1834.2ms, ETA 4d 7h
12/03/2025 00:25:35 - INFO - training.fm_trainer - Step 8750/210000 (4.17%): loss=1.4891, lr=1.64e-04, step_time=1883.8ms, ETA 4d 7h
12/03/2025 00:25:35 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:26:02 - INFO - training.fm_trainer - Eval Step 8750: loss=3.2917, ppl=26.89
12/03/2025 00:26:20 - INFO - training.fm_trainer - Step 8760/210000 (4.17%): loss=1.2987, lr=1.64e-04, step_time=1865.0ms, ETA 4d 7h
12/03/2025 00:26:39 - INFO - training.fm_trainer - Step 8770/210000 (4.18%): loss=7.1558, lr=1.64e-04, step_time=1819.2ms, ETA 4d 7h
12/03/2025 00:26:57 - INFO - training.fm_trainer - Step 8780/210000 (4.18%): loss=1.7399, lr=1.64e-04, step_time=1835.0ms, ETA 4d 7h
12/03/2025 00:27:16 - INFO - training.fm_trainer - Step 8790/210000 (4.19%): loss=6.0404, lr=1.64e-04, step_time=1847.9ms, ETA 4d 7h
12/03/2025 00:27:34 - INFO - training.fm_trainer - Step 8800/210000 (4.19%): loss=0.9554, lr=1.65e-04, grad_norm=0.09, step_time=1893.8ms, ETA 4d 7h
12/03/2025 00:27:34 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:28:00 - INFO - training.fm_trainer - Eval Step 8800: loss=3.2544, ppl=25.90
12/03/2025 00:28:19 - INFO - training.fm_trainer - Step 8810/210000 (4.20%): loss=4.8941, lr=1.65e-04, step_time=1820.3ms, ETA 4d 7h
12/03/2025 00:28:37 - INFO - training.fm_trainer - Step 8820/210000 (4.20%): loss=2.6967, lr=1.65e-04, step_time=1844.2ms, ETA 4d 7h
12/03/2025 00:28:56 - INFO - training.fm_trainer - Step 8830/210000 (4.20%): loss=1.1856, lr=1.65e-04, step_time=1810.3ms, ETA 4d 7h
12/03/2025 00:29:14 - INFO - training.fm_trainer - Step 8840/210000 (4.21%): loss=1.3499, lr=1.66e-04, step_time=1829.1ms, ETA 4d 6h
12/03/2025 00:29:33 - INFO - training.fm_trainer - Step 8850/210000 (4.21%): loss=9.4818, lr=1.66e-04, step_time=1839.2ms, ETA 4d 6h
12/03/2025 00:29:33 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:30:00 - INFO - training.fm_trainer - Eval Step 8850: loss=3.2653, ppl=26.19
12/03/2025 00:30:18 - INFO - training.fm_trainer - Step 8860/210000 (4.22%): loss=1.6402, lr=1.66e-04, step_time=1818.9ms, ETA 4d 6h
12/03/2025 00:30:37 - INFO - training.fm_trainer - Step 8870/210000 (4.22%): loss=8.7886, lr=1.66e-04, step_time=1840.8ms, ETA 4d 6h
12/03/2025 00:30:55 - INFO - training.fm_trainer - Step 8880/210000 (4.23%): loss=9.1780, lr=1.66e-04, step_time=1825.7ms, ETA 4d 6h
12/03/2025 00:31:14 - INFO - training.fm_trainer - Step 8890/210000 (4.23%): loss=2.4618, lr=1.66e-04, step_time=1976.2ms, ETA 4d 7h
12/03/2025 00:31:32 - INFO - training.fm_trainer - Step 8900/210000 (4.24%): loss=4.1288, lr=1.67e-04, step_time=1851.4ms, ETA 4d 7h
12/03/2025 00:31:32 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:31:59 - INFO - training.fm_trainer - Eval Step 8900: loss=3.2636, ppl=26.14
12/03/2025 00:32:18 - INFO - training.fm_trainer - Step 8910/210000 (4.24%): loss=1.3341, lr=1.67e-04, step_time=1832.0ms, ETA 4d 7h
12/03/2025 00:32:36 - INFO - training.fm_trainer - Step 8920/210000 (4.25%): loss=1.9577, lr=1.67e-04, step_time=1847.0ms, ETA 4d 7h
12/03/2025 00:32:54 - INFO - training.fm_trainer - Step 8930/210000 (4.25%): loss=1.0990, lr=1.67e-04, step_time=1828.2ms, ETA 4d 7h
12/03/2025 00:33:13 - INFO - training.fm_trainer - Step 8940/210000 (4.26%): loss=1.4492, lr=1.67e-04, step_time=1804.6ms, ETA 4d 6h
12/03/2025 00:33:31 - INFO - training.fm_trainer - Step 8950/210000 (4.26%): loss=0.6474, lr=1.67e-04, step_time=1858.1ms, ETA 4d 7h
12/03/2025 00:33:31 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:33:57 - INFO - training.fm_trainer - Eval Step 8950: loss=3.2971, ppl=27.03
12/03/2025 00:34:16 - INFO - training.fm_trainer - Step 8960/210000 (4.27%): loss=1.9922, lr=1.68e-04, grad_norm=0.01, step_time=1860.6ms, ETA 4d 7h
12/03/2025 00:34:34 - INFO - training.fm_trainer - Step 8970/210000 (4.27%): loss=9.7454, lr=1.68e-04, step_time=1849.5ms, ETA 4d 7h
12/03/2025 00:34:53 - INFO - training.fm_trainer - Step 8980/210000 (4.28%): loss=1.8266, lr=1.68e-04, step_time=1838.9ms, ETA 4d 7h
12/03/2025 00:35:11 - INFO - training.fm_trainer - Step 8990/210000 (4.28%): loss=0.8993, lr=1.68e-04, step_time=1986.3ms, ETA 4d 7h
12/03/2025 00:35:30 - INFO - training.fm_trainer - Step 9000/210000 (4.29%): loss=8.2134, lr=1.69e-04, step_time=1840.3ms, ETA 4d 7h
12/03/2025 00:35:30 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:35:56 - INFO - training.fm_trainer - Eval Step 9000: loss=3.2775, ppl=26.51
12/03/2025 00:35:56 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-9000
12/03/2025 00:35:56 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 00:35:57 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-9000/pytorch_model_fsdp_0
12/03/2025 00:36:05 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-9000/pytorch_model_fsdp_0
12/03/2025 00:36:05 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-9000
12/03/2025 00:36:05 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 00:36:09 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-9000/optimizer_0
12/03/2025 00:36:26 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-9000/optimizer_0
12/03/2025 00:36:26 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-9000
12/03/2025 00:36:26 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-9000/scheduler.bin
12/03/2025 00:36:26 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-9000/sampler.bin
12/03/2025 00:36:26 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-9000/sampler_1.bin
12/03/2025 00:36:26 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-9000/random_states_0.pkl
12/03/2025 00:36:26 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-9000
12/03/2025 00:36:26 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-8000
12/03/2025 00:36:47 - INFO - training.fm_trainer - Step 9010/210000 (4.29%): loss=1.1223, lr=1.69e-04, step_time=1863.2ms, ETA 4d 7h
12/03/2025 00:37:06 - INFO - training.fm_trainer - Step 9020/210000 (4.30%): loss=0.9150, lr=1.69e-04, step_time=1830.4ms, ETA 4d 7h
12/03/2025 00:37:24 - INFO - training.fm_trainer - Step 9030/210000 (4.30%): loss=4.7961, lr=1.69e-04, step_time=1840.4ms, ETA 4d 7h
12/03/2025 00:37:43 - INFO - training.fm_trainer - Step 9040/210000 (4.30%): loss=0.5655, lr=1.69e-04, step_time=1834.1ms, ETA 4d 7h
12/03/2025 00:38:01 - INFO - training.fm_trainer - Step 9050/210000 (4.31%): loss=1.1388, lr=1.69e-04, step_time=1875.8ms, ETA 4d 7h
12/03/2025 00:38:01 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:38:28 - INFO - training.fm_trainer - Eval Step 9050: loss=3.2503, ppl=25.80
12/03/2025 00:38:47 - INFO - training.fm_trainer - Step 9060/210000 (4.31%): loss=4.4660, lr=1.70e-04, step_time=1900.3ms, ETA 4d 7h
12/03/2025 00:39:05 - INFO - training.fm_trainer - Step 9070/210000 (4.32%): loss=0.9603, lr=1.70e-04, step_time=1841.9ms, ETA 4d 7h
12/03/2025 00:39:24 - INFO - training.fm_trainer - Step 9080/210000 (4.32%): loss=1.3411, lr=1.70e-04, step_time=1854.1ms, ETA 4d 7h
12/03/2025 00:39:42 - INFO - training.fm_trainer - Step 9090/210000 (4.33%): loss=7.1903, lr=1.70e-04, step_time=1831.6ms, ETA 4d 7h
12/03/2025 00:40:01 - INFO - training.fm_trainer - Step 9100/210000 (4.33%): loss=3.5109, lr=1.70e-04, step_time=1845.8ms, ETA 4d 7h
12/03/2025 00:40:01 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:40:28 - INFO - training.fm_trainer - Eval Step 9100: loss=3.2159, ppl=24.93
12/03/2025 00:40:47 - INFO - training.fm_trainer - Step 9110/210000 (4.34%): loss=1.5904, lr=1.70e-04, step_time=1846.8ms, ETA 4d 7h
12/03/2025 00:41:05 - INFO - training.fm_trainer - Step 9120/210000 (4.34%): loss=1.1205, lr=1.71e-04, grad_norm=0.05, step_time=1876.2ms, ETA 4d 7h
12/03/2025 00:41:24 - INFO - training.fm_trainer - Step 9130/210000 (4.35%): loss=1.2166, lr=1.71e-04, step_time=1833.5ms, ETA 4d 7h
12/03/2025 00:41:42 - INFO - training.fm_trainer - Step 9140/210000 (4.35%): loss=3.6541, lr=1.71e-04, step_time=1838.7ms, ETA 4d 7h
12/03/2025 00:42:01 - INFO - training.fm_trainer - Step 9150/210000 (4.36%): loss=3.0118, lr=1.71e-04, step_time=1849.9ms, ETA 4d 7h
12/03/2025 00:42:01 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:42:28 - INFO - training.fm_trainer - Eval Step 9150: loss=3.2477, ppl=25.73
12/03/2025 00:42:46 - INFO - training.fm_trainer - Step 9160/210000 (4.36%): loss=1.0048, lr=1.72e-04, step_time=1864.2ms, ETA 4d 7h
12/03/2025 00:43:05 - INFO - training.fm_trainer - Step 9170/210000 (4.37%): loss=1.0323, lr=1.72e-04, step_time=1823.1ms, ETA 4d 7h
12/03/2025 00:43:23 - INFO - training.fm_trainer - Step 9180/210000 (4.37%): loss=1.3012, lr=1.72e-04, step_time=1839.5ms, ETA 4d 7h
12/03/2025 00:43:41 - INFO - training.fm_trainer - Step 9190/210000 (4.38%): loss=6.2370, lr=1.72e-04, step_time=1819.9ms, ETA 4d 6h
12/03/2025 00:44:00 - INFO - training.fm_trainer - Step 9200/210000 (4.38%): loss=2.7329, lr=1.72e-04, step_time=1833.8ms, ETA 4d 6h
12/03/2025 00:44:00 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:44:26 - INFO - training.fm_trainer - Eval Step 9200: loss=3.2153, ppl=24.91
12/03/2025 00:44:44 - INFO - training.fm_trainer - Step 9210/210000 (4.39%): loss=8.5813, lr=1.72e-04, step_time=1847.1ms, ETA 4d 6h
12/03/2025 00:45:03 - INFO - training.fm_trainer - Step 9220/210000 (4.39%): loss=0.8517, lr=1.73e-04, step_time=1836.3ms, ETA 4d 6h
12/03/2025 00:45:22 - INFO - training.fm_trainer - Step 9230/210000 (4.40%): loss=2.2064, lr=1.73e-04, step_time=1850.7ms, ETA 4d 6h
12/03/2025 00:45:40 - INFO - training.fm_trainer - Step 9240/210000 (4.40%): loss=2.4914, lr=1.73e-04, step_time=1820.6ms, ETA 4d 6h
12/03/2025 00:45:58 - INFO - training.fm_trainer - Step 9250/210000 (4.40%): loss=0.8342, lr=1.73e-04, step_time=1813.2ms, ETA 4d 6h
12/03/2025 00:45:58 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:46:25 - INFO - training.fm_trainer - Eval Step 9250: loss=3.2058, ppl=24.68
12/03/2025 00:46:43 - INFO - training.fm_trainer - Step 9260/210000 (4.41%): loss=0.7613, lr=1.73e-04, step_time=1877.3ms, ETA 4d 6h
12/03/2025 00:47:02 - INFO - training.fm_trainer - Step 9270/210000 (4.41%): loss=1.2780, lr=1.73e-04, step_time=1832.8ms, ETA 4d 6h
12/03/2025 00:47:20 - INFO - training.fm_trainer - Step 9280/210000 (4.42%): loss=3.5517, lr=1.74e-04, grad_norm=0.02, step_time=1875.3ms, ETA 4d 6h
12/03/2025 00:47:38 - INFO - training.fm_trainer - Step 9290/210000 (4.42%): loss=6.4093, lr=1.74e-04, step_time=1824.5ms, ETA 4d 6h
12/03/2025 00:47:57 - INFO - training.fm_trainer - Step 9300/210000 (4.43%): loss=0.8919, lr=1.74e-04, step_time=1823.4ms, ETA 4d 6h
12/03/2025 00:47:57 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:48:23 - INFO - training.fm_trainer - Eval Step 9300: loss=3.2103, ppl=24.79
12/03/2025 00:48:41 - INFO - training.fm_trainer - Step 9310/210000 (4.43%): loss=1.3680, lr=1.74e-04, step_time=1852.4ms, ETA 4d 6h
12/03/2025 00:49:00 - INFO - training.fm_trainer - Step 9320/210000 (4.44%): loss=0.6269, lr=1.75e-04, step_time=1823.3ms, ETA 4d 6h
12/03/2025 00:49:18 - INFO - training.fm_trainer - Step 9330/210000 (4.44%): loss=1.2378, lr=1.75e-04, step_time=1827.0ms, ETA 4d 6h
12/03/2025 00:49:36 - INFO - training.fm_trainer - Step 9340/210000 (4.45%): loss=3.0068, lr=1.75e-04, step_time=1821.7ms, ETA 4d 6h
12/03/2025 00:49:55 - INFO - training.fm_trainer - Step 9350/210000 (4.45%): loss=0.4189, lr=1.75e-04, step_time=1947.7ms, ETA 4d 7h
12/03/2025 00:49:55 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:50:21 - INFO - training.fm_trainer - Eval Step 9350: loss=3.3325, ppl=28.01
12/03/2025 00:50:39 - INFO - training.fm_trainer - Step 9360/210000 (4.46%): loss=1.0274, lr=1.75e-04, step_time=1816.6ms, ETA 4d 6h
12/03/2025 00:50:57 - INFO - training.fm_trainer - Step 9370/210000 (4.46%): loss=1.4753, lr=1.75e-04, step_time=1829.7ms, ETA 4d 6h
12/03/2025 00:51:16 - INFO - training.fm_trainer - Step 9380/210000 (4.47%): loss=4.8971, lr=1.76e-04, step_time=1808.6ms, ETA 4d 6h
12/03/2025 00:51:34 - INFO - training.fm_trainer - Step 9390/210000 (4.47%): loss=7.4852, lr=1.76e-04, step_time=1833.0ms, ETA 4d 6h
12/03/2025 00:51:53 - INFO - training.fm_trainer - Step 9400/210000 (4.48%): loss=3.6128, lr=1.76e-04, step_time=1886.9ms, ETA 4d 6h
12/03/2025 00:51:53 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:52:20 - INFO - training.fm_trainer - Eval Step 9400: loss=3.3407, ppl=28.24
12/03/2025 00:52:39 - INFO - training.fm_trainer - Step 9410/210000 (4.48%): loss=4.2545, lr=1.76e-04, step_time=1848.5ms, ETA 4d 6h
12/03/2025 00:52:57 - INFO - training.fm_trainer - Step 9420/210000 (4.49%): loss=2.7726, lr=1.76e-04, step_time=1820.4ms, ETA 4d 6h
12/03/2025 00:53:15 - INFO - training.fm_trainer - Step 9430/210000 (4.49%): loss=9.8274, lr=1.76e-04, step_time=1895.1ms, ETA 4d 6h
12/03/2025 00:53:34 - INFO - training.fm_trainer - Step 9440/210000 (4.50%): loss=3.6778, lr=1.77e-04, grad_norm=0.02, step_time=2029.5ms, ETA 4d 7h
12/03/2025 00:53:52 - INFO - training.fm_trainer - Step 9450/210000 (4.50%): loss=0.9088, lr=1.77e-04, step_time=1838.8ms, ETA 4d 7h
12/03/2025 00:53:52 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:54:18 - INFO - training.fm_trainer - Eval Step 9450: loss=3.2966, ppl=27.02
12/03/2025 00:54:37 - INFO - training.fm_trainer - Step 9460/210000 (4.50%): loss=4.4989, lr=1.77e-04, step_time=1809.9ms, ETA 4d 7h
12/03/2025 00:54:55 - INFO - training.fm_trainer - Step 9470/210000 (4.51%): loss=4.7143, lr=1.77e-04, step_time=1855.2ms, ETA 4d 7h
12/03/2025 00:55:14 - INFO - training.fm_trainer - Step 9480/210000 (4.51%): loss=1.8428, lr=1.78e-04, step_time=1815.5ms, ETA 4d 7h
12/03/2025 00:55:32 - INFO - training.fm_trainer - Step 9490/210000 (4.52%): loss=5.8042, lr=1.78e-04, step_time=1829.7ms, ETA 4d 7h
12/03/2025 00:55:51 - INFO - training.fm_trainer - Step 9500/210000 (4.52%): loss=2.3275, lr=1.78e-04, step_time=1878.2ms, ETA 4d 7h
12/03/2025 00:55:51 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:56:17 - INFO - training.fm_trainer - Eval Step 9500: loss=3.2835, ppl=26.67
12/03/2025 00:56:17 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-9500
12/03/2025 00:56:17 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 00:56:18 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-9500/pytorch_model_fsdp_0
12/03/2025 00:56:27 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-9500/pytorch_model_fsdp_0
12/03/2025 00:56:27 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-9500
12/03/2025 00:56:27 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 00:56:30 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-9500/optimizer_0
12/03/2025 00:56:49 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-9500/optimizer_0
12/03/2025 00:56:49 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-9500
12/03/2025 00:56:49 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-9500/scheduler.bin
12/03/2025 00:56:49 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-9500/sampler.bin
12/03/2025 00:56:49 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-9500/sampler_1.bin
12/03/2025 00:56:49 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-9500/random_states_0.pkl
12/03/2025 00:56:49 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-9500
12/03/2025 00:56:49 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-8500
12/03/2025 00:57:09 - INFO - training.fm_trainer - Step 9510/210000 (4.53%): loss=2.1280, lr=1.78e-04, step_time=1833.0ms, ETA 4d 7h
12/03/2025 00:57:28 - INFO - training.fm_trainer - Step 9520/210000 (4.53%): loss=3.9212, lr=1.78e-04, step_time=1865.1ms, ETA 4d 7h
12/03/2025 00:57:46 - INFO - training.fm_trainer - Step 9530/210000 (4.54%): loss=4.6886, lr=1.78e-04, step_time=1826.2ms, ETA 4d 7h
12/03/2025 00:58:05 - INFO - training.fm_trainer - Step 9540/210000 (4.54%): loss=1.6712, lr=1.79e-04, step_time=1842.6ms, ETA 4d 6h
12/03/2025 00:58:23 - INFO - training.fm_trainer - Step 9550/210000 (4.55%): loss=3.7680, lr=1.79e-04, step_time=1823.5ms, ETA 4d 6h
12/03/2025 00:58:23 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 00:58:51 - INFO - training.fm_trainer - Eval Step 9550: loss=3.2360, ppl=25.43
12/03/2025 00:59:09 - INFO - training.fm_trainer - Step 9560/210000 (4.55%): loss=2.0286, lr=1.79e-04, step_time=1860.5ms, ETA 4d 6h
12/03/2025 00:59:28 - INFO - training.fm_trainer - Step 9570/210000 (4.56%): loss=1.3507, lr=1.79e-04, step_time=1966.1ms, ETA 4d 7h
12/03/2025 00:59:47 - INFO - training.fm_trainer - Step 9580/210000 (4.56%): loss=1.7723, lr=1.79e-04, step_time=1824.1ms, ETA 4d 7h
12/03/2025 01:00:05 - INFO - training.fm_trainer - Step 9590/210000 (4.57%): loss=1.6361, lr=1.79e-04, step_time=1836.6ms, ETA 4d 7h
12/03/2025 01:00:23 - INFO - training.fm_trainer - Step 9600/210000 (4.57%): loss=5.7664, lr=1.80e-04, grad_norm=0.05, step_time=1889.1ms, ETA 4d 7h
12/03/2025 01:00:23 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:00:49 - INFO - training.fm_trainer - Eval Step 9600: loss=3.4618, ppl=31.87
12/03/2025 01:01:08 - INFO - training.fm_trainer - Step 9610/210000 (4.58%): loss=5.5454, lr=1.80e-04, step_time=1814.0ms, ETA 4d 7h
12/03/2025 01:01:26 - INFO - training.fm_trainer - Step 9620/210000 (4.58%): loss=2.4098, lr=1.80e-04, step_time=1822.4ms, ETA 4d 6h
12/03/2025 01:01:45 - INFO - training.fm_trainer - Step 9630/210000 (4.59%): loss=3.2689, lr=1.80e-04, step_time=1827.6ms, ETA 4d 6h
12/03/2025 01:02:03 - INFO - training.fm_trainer - Step 9640/210000 (4.59%): loss=4.3604, lr=1.81e-04, step_time=1877.4ms, ETA 4d 7h
12/03/2025 01:02:21 - INFO - training.fm_trainer - Step 9650/210000 (4.60%): loss=9.6868, lr=1.81e-04, step_time=1840.4ms, ETA 4d 6h
12/03/2025 01:02:21 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:02:49 - INFO - training.fm_trainer - Eval Step 9650: loss=3.4794, ppl=32.44
12/03/2025 01:03:07 - INFO - training.fm_trainer - Step 9660/210000 (4.60%): loss=5.7772, lr=1.81e-04, step_time=1840.4ms, ETA 4d 6h
12/03/2025 01:03:26 - INFO - training.fm_trainer - Step 9670/210000 (4.60%): loss=4.4806, lr=1.81e-04, step_time=1807.2ms, ETA 4d 6h
12/03/2025 01:03:44 - INFO - training.fm_trainer - Step 9680/210000 (4.61%): loss=1.9479, lr=1.81e-04, step_time=1826.1ms, ETA 4d 6h
12/03/2025 01:04:03 - INFO - training.fm_trainer - Step 9690/210000 (4.61%): loss=1.8161, lr=1.81e-04, step_time=1827.3ms, ETA 4d 6h
12/03/2025 01:04:22 - INFO - training.fm_trainer - Step 9700/210000 (4.62%): loss=2.1185, lr=1.82e-04, step_time=1937.7ms, ETA 4d 6h
12/03/2025 01:04:22 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:04:48 - INFO - training.fm_trainer - Eval Step 9700: loss=3.2865, ppl=26.75
12/03/2025 01:05:06 - INFO - training.fm_trainer - Step 9710/210000 (4.62%): loss=1.8001, lr=1.82e-04, step_time=1821.4ms, ETA 4d 6h
12/03/2025 01:05:24 - INFO - training.fm_trainer - Step 9720/210000 (4.63%): loss=1.7182, lr=1.82e-04, step_time=1801.6ms, ETA 4d 6h
12/03/2025 01:05:43 - INFO - training.fm_trainer - Step 9730/210000 (4.63%): loss=1.0488, lr=1.82e-04, step_time=1824.1ms, ETA 4d 6h
12/03/2025 01:06:01 - INFO - training.fm_trainer - Step 9740/210000 (4.64%): loss=1.5329, lr=1.82e-04, step_time=1841.2ms, ETA 4d 6h
12/03/2025 01:06:20 - INFO - training.fm_trainer - Step 9750/210000 (4.64%): loss=1.2777, lr=1.82e-04, step_time=1818.5ms, ETA 4d 6h
12/03/2025 01:06:20 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:06:46 - INFO - training.fm_trainer - Eval Step 9750: loss=3.2535, ppl=25.88
12/03/2025 01:07:04 - INFO - training.fm_trainer - Step 9760/210000 (4.65%): loss=1.8798, lr=1.83e-04, grad_norm=0.02, step_time=1864.0ms, ETA 4d 6h
12/03/2025 01:07:23 - INFO - training.fm_trainer - Step 9770/210000 (4.65%): loss=1.4952, lr=1.83e-04, step_time=1829.9ms, ETA 4d 6h
12/03/2025 01:07:41 - INFO - training.fm_trainer - Step 9780/210000 (4.66%): loss=0.8758, lr=1.83e-04, step_time=1810.9ms, ETA 4d 6h
12/03/2025 01:07:59 - INFO - training.fm_trainer - Step 9790/210000 (4.66%): loss=1.4259, lr=1.83e-04, step_time=1822.4ms, ETA 4d 6h
12/03/2025 01:08:18 - INFO - training.fm_trainer - Step 9800/210000 (4.67%): loss=0.6328, lr=1.84e-04, step_time=1814.1ms, ETA 4d 5h
12/03/2025 01:08:18 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:08:44 - INFO - training.fm_trainer - Eval Step 9800: loss=3.3718, ppl=29.13
12/03/2025 01:09:02 - INFO - training.fm_trainer - Step 9810/210000 (4.67%): loss=1.7216, lr=1.84e-04, step_time=1858.6ms, ETA 4d 6h
12/03/2025 01:09:20 - INFO - training.fm_trainer - Step 9820/210000 (4.68%): loss=1.6767, lr=1.84e-04, step_time=1841.6ms, ETA 4d 6h
12/03/2025 01:09:39 - INFO - training.fm_trainer - Step 9830/210000 (4.68%): loss=7.6294, lr=1.84e-04, step_time=1848.0ms, ETA 4d 6h
12/03/2025 01:09:57 - INFO - training.fm_trainer - Step 9840/210000 (4.69%): loss=2.7196, lr=1.84e-04, step_time=1853.8ms, ETA 4d 6h
12/03/2025 01:10:16 - INFO - training.fm_trainer - Step 9850/210000 (4.69%): loss=0.3415, lr=1.84e-04, step_time=1816.4ms, ETA 4d 6h
12/03/2025 01:10:16 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:10:42 - INFO - training.fm_trainer - Eval Step 9850: loss=3.4400, ppl=31.19
12/03/2025 01:11:00 - INFO - training.fm_trainer - Step 9860/210000 (4.70%): loss=0.4464, lr=1.85e-04, step_time=1851.7ms, ETA 4d 6h
12/03/2025 01:11:18 - INFO - training.fm_trainer - Step 9870/210000 (4.70%): loss=1.4291, lr=1.85e-04, step_time=1870.9ms, ETA 4d 6h
12/03/2025 01:11:37 - INFO - training.fm_trainer - Step 9880/210000 (4.70%): loss=1.2228, lr=1.85e-04, step_time=1817.0ms, ETA 4d 6h
12/03/2025 01:11:55 - INFO - training.fm_trainer - Step 9890/210000 (4.71%): loss=6.6958, lr=1.85e-04, step_time=1913.1ms, ETA 4d 6h
12/03/2025 01:12:14 - INFO - training.fm_trainer - Step 9900/210000 (4.71%): loss=5.4969, lr=1.85e-04, step_time=1820.5ms, ETA 4d 6h
12/03/2025 01:12:14 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:12:40 - INFO - training.fm_trainer - Eval Step 9900: loss=3.3835, ppl=29.48
12/03/2025 01:12:58 - INFO - training.fm_trainer - Step 9910/210000 (4.72%): loss=4.1006, lr=1.85e-04, step_time=1829.5ms, ETA 4d 6h
12/03/2025 01:13:17 - INFO - training.fm_trainer - Step 9920/210000 (4.72%): loss=5.4413, lr=1.86e-04, grad_norm=0.06, step_time=1836.3ms, ETA 4d 6h
12/03/2025 01:13:35 - INFO - training.fm_trainer - Step 9930/210000 (4.73%): loss=2.7485, lr=1.86e-04, step_time=1839.1ms, ETA 4d 6h
12/03/2025 01:13:53 - INFO - training.fm_trainer - Step 9940/210000 (4.73%): loss=1.0359, lr=1.86e-04, step_time=1809.4ms, ETA 4d 6h
12/03/2025 01:14:12 - INFO - training.fm_trainer - Step 9950/210000 (4.74%): loss=2.0053, lr=1.86e-04, step_time=1808.7ms, ETA 4d 5h
12/03/2025 01:14:12 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:14:38 - INFO - training.fm_trainer - Eval Step 9950: loss=3.3161, ppl=27.55
12/03/2025 01:14:56 - INFO - training.fm_trainer - Step 9960/210000 (4.74%): loss=2.6675, lr=1.87e-04, step_time=1837.7ms, ETA 4d 6h
12/03/2025 01:15:15 - INFO - training.fm_trainer - Step 9970/210000 (4.75%): loss=5.2396, lr=1.87e-04, step_time=1837.6ms, ETA 4d 6h
12/03/2025 01:15:33 - INFO - training.fm_trainer - Step 9980/210000 (4.75%): loss=1.5167, lr=1.87e-04, step_time=1948.8ms, ETA 4d 6h
12/03/2025 01:15:51 - INFO - training.fm_trainer - Step 9990/210000 (4.76%): loss=7.6876, lr=1.87e-04, step_time=2013.4ms, ETA 4d 7h
12/03/2025 01:16:10 - INFO - training.fm_trainer - Step 10000/210000 (4.76%): loss=2.9242, lr=1.87e-04, step_time=1844.0ms, ETA 4d 7h
12/03/2025 01:16:10 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:16:36 - INFO - training.fm_trainer - Eval Step 10000: loss=3.2131, ppl=24.86
12/03/2025 01:16:36 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-10000
12/03/2025 01:16:36 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 01:16:37 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-10000/pytorch_model_fsdp_0
12/03/2025 01:16:44 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-10000/pytorch_model_fsdp_0
12/03/2025 01:16:44 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-10000
12/03/2025 01:16:44 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 01:16:47 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-10000/optimizer_0
12/03/2025 01:17:05 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-10000/optimizer_0
12/03/2025 01:17:05 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-10000
12/03/2025 01:17:05 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-10000/scheduler.bin
12/03/2025 01:17:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-10000/sampler.bin
12/03/2025 01:17:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-10000/sampler_1.bin
12/03/2025 01:17:05 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-10000/random_states_0.pkl
12/03/2025 01:17:05 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-10000
12/03/2025 01:17:05 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-9000
12/03/2025 01:17:27 - INFO - training.fm_trainer - Step 10010/210000 (4.77%): loss=0.9189, lr=1.87e-04, step_time=2104.9ms, ETA 4d 8h
12/03/2025 01:17:46 - INFO - training.fm_trainer - Step 10020/210000 (4.77%): loss=3.5971, lr=1.88e-04, step_time=1829.6ms, ETA 4d 8h
12/03/2025 01:18:04 - INFO - training.fm_trainer - Step 10030/210000 (4.78%): loss=1.1268, lr=1.88e-04, step_time=1833.5ms, ETA 4d 8h
12/03/2025 01:18:23 - INFO - training.fm_trainer - Step 10040/210000 (4.78%): loss=4.6726, lr=1.88e-04, step_time=1858.4ms, ETA 4d 8h
12/03/2025 01:18:41 - INFO - training.fm_trainer - Step 10050/210000 (4.79%): loss=5.3488, lr=1.88e-04, step_time=1919.3ms, ETA 4d 8h
12/03/2025 01:18:41 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:19:07 - INFO - training.fm_trainer - Eval Step 10050: loss=3.3212, ppl=27.69
12/03/2025 01:19:26 - INFO - training.fm_trainer - Step 10060/210000 (4.79%): loss=1.4188, lr=1.88e-04, step_time=1829.6ms, ETA 4d 8h
12/03/2025 01:19:44 - INFO - training.fm_trainer - Step 10070/210000 (4.80%): loss=0.9599, lr=1.88e-04, step_time=1808.2ms, ETA 4d 7h
12/03/2025 01:20:03 - INFO - training.fm_trainer - Step 10080/210000 (4.80%): loss=4.9126, lr=1.89e-04, grad_norm=0.07, step_time=1867.0ms, ETA 4d 7h
12/03/2025 01:20:22 - INFO - training.fm_trainer - Step 10090/210000 (4.80%): loss=2.6266, lr=1.89e-04, step_time=1851.9ms, ETA 4d 7h
12/03/2025 01:20:40 - INFO - training.fm_trainer - Step 10100/210000 (4.81%): loss=1.6872, lr=1.89e-04, step_time=1839.7ms, ETA 4d 7h
12/03/2025 01:20:40 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:21:06 - INFO - training.fm_trainer - Eval Step 10100: loss=3.2808, ppl=26.60
12/03/2025 01:21:24 - INFO - training.fm_trainer - Step 10110/210000 (4.81%): loss=1.2477, lr=1.89e-04, step_time=1811.3ms, ETA 4d 7h
12/03/2025 01:21:43 - INFO - training.fm_trainer - Step 10120/210000 (4.82%): loss=2.3909, lr=1.90e-04, step_time=1836.8ms, ETA 4d 7h
12/03/2025 01:22:01 - INFO - training.fm_trainer - Step 10130/210000 (4.82%): loss=8.6167, lr=1.90e-04, step_time=1821.3ms, ETA 4d 6h
12/03/2025 01:22:20 - INFO - training.fm_trainer - Step 10140/210000 (4.83%): loss=1.6648, lr=1.90e-04, step_time=1904.1ms, ETA 4d 7h
12/03/2025 01:22:38 - INFO - training.fm_trainer - Step 10150/210000 (4.83%): loss=0.7089, lr=1.90e-04, step_time=1810.7ms, ETA 4d 6h
12/03/2025 01:22:38 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:23:06 - INFO - training.fm_trainer - Eval Step 10150: loss=3.2429, ppl=25.61
12/03/2025 01:23:24 - INFO - training.fm_trainer - Step 10160/210000 (4.84%): loss=2.8184, lr=1.90e-04, step_time=1877.1ms, ETA 4d 6h
12/03/2025 01:23:43 - INFO - training.fm_trainer - Step 10170/210000 (4.84%): loss=1.1939, lr=1.90e-04, step_time=1820.5ms, ETA 4d 6h
12/03/2025 01:24:02 - INFO - training.fm_trainer - Step 10180/210000 (4.85%): loss=8.4289, lr=1.91e-04, step_time=1817.4ms, ETA 4d 6h
12/03/2025 01:24:20 - INFO - training.fm_trainer - Step 10190/210000 (4.85%): loss=5.7101, lr=1.91e-04, step_time=1803.2ms, ETA 4d 6h
12/03/2025 01:24:39 - INFO - training.fm_trainer - Step 10200/210000 (4.86%): loss=10.2811, lr=1.91e-04, step_time=1827.6ms, ETA 4d 6h
12/03/2025 01:24:39 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:25:05 - INFO - training.fm_trainer - Eval Step 10200: loss=3.2351, ppl=25.41
12/03/2025 01:25:23 - INFO - training.fm_trainer - Step 10210/210000 (4.86%): loss=2.3148, lr=1.91e-04, step_time=1855.8ms, ETA 4d 6h
12/03/2025 01:25:41 - INFO - training.fm_trainer - Step 10220/210000 (4.87%): loss=3.6437, lr=1.91e-04, step_time=1879.8ms, ETA 4d 6h
12/03/2025 01:26:00 - INFO - training.fm_trainer - Step 10230/210000 (4.87%): loss=4.3065, lr=1.91e-04, step_time=1828.8ms, ETA 4d 6h
12/03/2025 01:26:18 - INFO - training.fm_trainer - Step 10240/210000 (4.88%): loss=2.6667, lr=1.92e-04, grad_norm=0.01, step_time=1869.2ms, ETA 4d 6h
12/03/2025 01:26:37 - INFO - training.fm_trainer - Step 10250/210000 (4.88%): loss=2.2349, lr=1.92e-04, step_time=2537.0ms, ETA 4d 10h
12/03/2025 01:26:37 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:27:03 - INFO - training.fm_trainer - Eval Step 10250: loss=3.3105, ppl=27.40
12/03/2025 01:27:21 - INFO - training.fm_trainer - Step 10260/210000 (4.89%): loss=7.0921, lr=1.92e-04, step_time=1809.8ms, ETA 4d 9h
12/03/2025 01:27:40 - INFO - training.fm_trainer - Step 10270/210000 (4.89%): loss=1.1066, lr=1.92e-04, step_time=1878.6ms, ETA 4d 9h
12/03/2025 01:27:59 - INFO - training.fm_trainer - Step 10280/210000 (4.90%): loss=0.9020, lr=1.93e-04, step_time=1886.2ms, ETA 4d 9h
12/03/2025 01:28:17 - INFO - training.fm_trainer - Step 10290/210000 (4.90%): loss=0.5561, lr=1.93e-04, step_time=1830.1ms, ETA 4d 9h
12/03/2025 01:28:36 - INFO - training.fm_trainer - Step 10300/210000 (4.90%): loss=3.0958, lr=1.93e-04, step_time=1830.2ms, ETA 4d 8h
12/03/2025 01:28:36 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:29:03 - INFO - training.fm_trainer - Eval Step 10300: loss=3.3358, ppl=28.10
12/03/2025 01:29:21 - INFO - training.fm_trainer - Step 10310/210000 (4.91%): loss=1.2842, lr=1.93e-04, step_time=1825.0ms, ETA 4d 8h
12/03/2025 01:29:40 - INFO - training.fm_trainer - Step 10320/210000 (4.91%): loss=5.2025, lr=1.93e-04, step_time=1813.1ms, ETA 4d 7h
12/03/2025 01:29:59 - INFO - training.fm_trainer - Step 10330/210000 (4.92%): loss=0.4209, lr=1.93e-04, step_time=1834.3ms, ETA 4d 7h
12/03/2025 01:30:17 - INFO - training.fm_trainer - Step 10340/210000 (4.92%): loss=1.1092, lr=1.94e-04, step_time=1841.7ms, ETA 4d 7h
12/03/2025 01:30:36 - INFO - training.fm_trainer - Step 10350/210000 (4.93%): loss=0.7224, lr=1.94e-04, step_time=1845.1ms, ETA 4d 7h
12/03/2025 01:30:36 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:31:01 - INFO - training.fm_trainer - Eval Step 10350: loss=3.2926, ppl=26.91
12/03/2025 01:31:20 - INFO - training.fm_trainer - Step 10360/210000 (4.93%): loss=0.9958, lr=1.94e-04, step_time=1845.8ms, ETA 4d 7h
12/03/2025 01:31:38 - INFO - training.fm_trainer - Step 10370/210000 (4.94%): loss=4.8925, lr=1.94e-04, step_time=1820.4ms, ETA 4d 7h
12/03/2025 01:31:56 - INFO - training.fm_trainer - Step 10380/210000 (4.94%): loss=2.6648, lr=1.94e-04, step_time=1842.9ms, ETA 4d 7h
12/03/2025 01:32:15 - INFO - training.fm_trainer - Step 10390/210000 (4.95%): loss=1.4116, lr=1.94e-04, step_time=1834.8ms, ETA 4d 6h
12/03/2025 01:32:33 - INFO - training.fm_trainer - Step 10400/210000 (4.95%): loss=6.8386, lr=1.95e-04, grad_norm=0.04, step_time=1853.6ms, ETA 4d 6h
12/03/2025 01:32:33 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:33:00 - INFO - training.fm_trainer - Eval Step 10400: loss=3.2723, ppl=26.37
12/03/2025 01:33:19 - INFO - training.fm_trainer - Step 10410/210000 (4.96%): loss=0.7899, lr=1.95e-04, step_time=1826.0ms, ETA 4d 6h
12/03/2025 01:33:37 - INFO - training.fm_trainer - Step 10420/210000 (4.96%): loss=1.2213, lr=1.95e-04, step_time=1829.1ms, ETA 4d 6h
12/03/2025 01:33:56 - INFO - training.fm_trainer - Step 10430/210000 (4.97%): loss=1.1509, lr=1.95e-04, step_time=1839.9ms, ETA 4d 6h
12/03/2025 01:34:14 - INFO - training.fm_trainer - Step 10440/210000 (4.97%): loss=0.6043, lr=1.96e-04, step_time=1815.9ms, ETA 4d 6h
12/03/2025 01:34:32 - INFO - training.fm_trainer - Step 10450/210000 (4.98%): loss=5.0203, lr=1.96e-04, step_time=1817.5ms, ETA 4d 6h
12/03/2025 01:34:32 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:34:58 - INFO - training.fm_trainer - Eval Step 10450: loss=3.2406, ppl=25.55
12/03/2025 01:35:17 - INFO - training.fm_trainer - Step 10460/210000 (4.98%): loss=1.0228, lr=1.96e-04, step_time=1868.6ms, ETA 4d 6h
12/03/2025 01:35:35 - INFO - training.fm_trainer - Step 10470/210000 (4.99%): loss=2.3970, lr=1.96e-04, step_time=1827.2ms, ETA 4d 6h
12/03/2025 01:35:53 - INFO - training.fm_trainer - Step 10480/210000 (4.99%): loss=2.9465, lr=1.96e-04, step_time=1815.8ms, ETA 4d 6h
12/03/2025 01:36:12 - INFO - training.fm_trainer - Step 10490/210000 (5.00%): loss=0.8691, lr=1.96e-04, step_time=1832.2ms, ETA 4d 5h
12/03/2025 01:36:30 - INFO - training.fm_trainer - Step 10500/210000 (5.00%): loss=4.3315, lr=1.97e-04, step_time=1834.1ms, ETA 4d 5h
12/03/2025 01:36:30 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:36:56 - INFO - training.fm_trainer - Eval Step 10500: loss=3.2519, ppl=25.84
12/03/2025 01:36:57 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-10500
12/03/2025 01:36:57 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 01:36:57 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-10500/pytorch_model_fsdp_0
12/03/2025 01:37:05 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-10500/pytorch_model_fsdp_0
12/03/2025 01:37:05 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-10500
12/03/2025 01:37:05 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 01:37:08 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-10500/optimizer_0
12/03/2025 01:37:23 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-10500/optimizer_0
12/03/2025 01:37:23 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-10500
12/03/2025 01:37:23 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-10500/scheduler.bin
12/03/2025 01:37:23 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-10500/sampler.bin
12/03/2025 01:37:23 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-10500/sampler_1.bin
12/03/2025 01:37:23 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-10500/random_states_0.pkl
12/03/2025 01:37:23 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-10500
12/03/2025 01:37:23 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-9500
12/03/2025 01:37:44 - INFO - training.fm_trainer - Step 10510/210000 (5.00%): loss=6.4237, lr=1.97e-04, step_time=1854.4ms, ETA 4d 6h
12/03/2025 01:38:03 - INFO - training.fm_trainer - Step 10520/210000 (5.01%): loss=4.6247, lr=1.97e-04, step_time=1843.2ms, ETA 4d 6h
12/03/2025 01:38:22 - INFO - training.fm_trainer - Step 10530/210000 (5.01%): loss=1.7542, lr=1.97e-04, step_time=1838.2ms, ETA 4d 5h
12/03/2025 01:38:40 - INFO - training.fm_trainer - Step 10540/210000 (5.02%): loss=0.9220, lr=1.97e-04, step_time=1804.9ms, ETA 4d 5h
12/03/2025 01:38:59 - INFO - training.fm_trainer - Step 10550/210000 (5.02%): loss=1.1187, lr=1.97e-04, step_time=1814.2ms, ETA 4d 5h
12/03/2025 01:38:59 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:39:24 - INFO - training.fm_trainer - Eval Step 10550: loss=3.2289, ppl=25.25
12/03/2025 01:39:43 - INFO - training.fm_trainer - Step 10560/210000 (5.03%): loss=1.4759, lr=1.98e-04, grad_norm=0.01, step_time=1841.9ms, ETA 4d 5h
12/03/2025 01:40:01 - INFO - training.fm_trainer - Step 10570/210000 (5.03%): loss=1.1355, lr=1.98e-04, step_time=1859.9ms, ETA 4d 5h
12/03/2025 01:40:20 - INFO - training.fm_trainer - Step 10580/210000 (5.04%): loss=5.7984, lr=1.98e-04, step_time=1829.4ms, ETA 4d 5h
12/03/2025 01:40:38 - INFO - training.fm_trainer - Step 10590/210000 (5.04%): loss=1.9431, lr=1.98e-04, step_time=1815.5ms, ETA 4d 5h
12/03/2025 01:40:57 - INFO - training.fm_trainer - Step 10600/210000 (5.05%): loss=0.6439, lr=1.99e-04, step_time=1864.1ms, ETA 4d 5h
12/03/2025 01:40:57 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:41:23 - INFO - training.fm_trainer - Eval Step 10600: loss=3.2266, ppl=25.19
12/03/2025 01:41:41 - INFO - training.fm_trainer - Step 10610/210000 (5.05%): loss=1.4076, lr=1.99e-04, step_time=1847.9ms, ETA 4d 5h
12/03/2025 01:41:59 - INFO - training.fm_trainer - Step 10620/210000 (5.06%): loss=0.8863, lr=1.99e-04, step_time=1811.6ms, ETA 4d 5h
12/03/2025 01:42:18 - INFO - training.fm_trainer - Step 10630/210000 (5.06%): loss=2.3743, lr=1.99e-04, step_time=1817.6ms, ETA 4d 5h
12/03/2025 01:42:36 - INFO - training.fm_trainer - Step 10640/210000 (5.07%): loss=1.2250, lr=1.99e-04, step_time=1847.8ms, ETA 4d 5h
12/03/2025 01:42:55 - INFO - training.fm_trainer - Step 10650/210000 (5.07%): loss=9.0319, lr=1.99e-04, step_time=1829.9ms, ETA 4d 5h
12/03/2025 01:42:55 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:43:21 - INFO - training.fm_trainer - Eval Step 10650: loss=3.2471, ppl=25.72
12/03/2025 01:43:40 - INFO - training.fm_trainer - Step 10660/210000 (5.08%): loss=0.6320, lr=2.00e-04, step_time=1826.2ms, ETA 4d 5h
12/03/2025 01:43:58 - INFO - training.fm_trainer - Step 10670/210000 (5.08%): loss=7.3706, lr=2.00e-04, step_time=1834.7ms, ETA 4d 5h
12/03/2025 01:44:16 - INFO - training.fm_trainer - Step 10680/210000 (5.09%): loss=4.5736, lr=2.00e-04, step_time=1889.9ms, ETA 4d 5h
12/03/2025 01:44:35 - INFO - training.fm_trainer - Step 10690/210000 (5.09%): loss=6.2594, lr=2.00e-04, step_time=1843.4ms, ETA 4d 5h
12/03/2025 01:44:54 - INFO - training.fm_trainer - Step 10700/210000 (5.10%): loss=6.9565, lr=2.00e-04, step_time=1818.2ms, ETA 4d 5h
12/03/2025 01:44:54 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:45:20 - INFO - training.fm_trainer - Eval Step 10700: loss=3.2171, ppl=24.95
12/03/2025 01:45:38 - INFO - training.fm_trainer - Step 10710/210000 (5.10%): loss=5.6707, lr=2.00e-04, step_time=1855.7ms, ETA 4d 5h
12/03/2025 01:45:57 - INFO - training.fm_trainer - Step 10720/210000 (5.10%): loss=4.9243, lr=2.01e-04, grad_norm=0.01, step_time=1974.5ms, ETA 4d 6h
12/03/2025 01:46:15 - INFO - training.fm_trainer - Step 10730/210000 (5.11%): loss=0.9992, lr=2.01e-04, step_time=1803.5ms, ETA 4d 6h
12/03/2025 01:46:33 - INFO - training.fm_trainer - Step 10740/210000 (5.11%): loss=1.9093, lr=2.01e-04, step_time=1820.0ms, ETA 4d 6h
12/03/2025 01:46:52 - INFO - training.fm_trainer - Step 10750/210000 (5.12%): loss=2.6326, lr=2.01e-04, step_time=1819.5ms, ETA 4d 5h
12/03/2025 01:46:52 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:47:19 - INFO - training.fm_trainer - Eval Step 10750: loss=3.2379, ppl=25.48
12/03/2025 01:47:37 - INFO - training.fm_trainer - Step 10760/210000 (5.12%): loss=3.2552, lr=2.02e-04, step_time=1867.3ms, ETA 4d 6h
12/03/2025 01:47:56 - INFO - training.fm_trainer - Step 10770/210000 (5.13%): loss=5.1132, lr=2.02e-04, step_time=1888.0ms, ETA 4d 6h
12/03/2025 01:48:15 - INFO - training.fm_trainer - Step 10780/210000 (5.13%): loss=1.7202, lr=2.02e-04, step_time=1865.5ms, ETA 4d 6h
12/03/2025 01:48:34 - INFO - training.fm_trainer - Step 10790/210000 (5.14%): loss=1.3537, lr=2.02e-04, step_time=1845.5ms, ETA 4d 6h
12/03/2025 01:48:53 - INFO - training.fm_trainer - Step 10800/210000 (5.14%): loss=4.4333, lr=2.02e-04, step_time=1858.6ms, ETA 4d 6h
12/03/2025 01:48:53 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:49:19 - INFO - training.fm_trainer - Eval Step 10800: loss=3.3097, ppl=27.38
12/03/2025 01:49:37 - INFO - training.fm_trainer - Step 10810/210000 (5.15%): loss=9.7746, lr=2.02e-04, step_time=1869.9ms, ETA 4d 6h
12/03/2025 01:49:56 - INFO - training.fm_trainer - Step 10820/210000 (5.15%): loss=3.0733, lr=2.03e-04, step_time=1824.1ms, ETA 4d 6h
12/03/2025 01:50:14 - INFO - training.fm_trainer - Step 10830/210000 (5.16%): loss=6.8656, lr=2.03e-04, step_time=1840.2ms, ETA 4d 6h
12/03/2025 01:50:32 - INFO - training.fm_trainer - Step 10840/210000 (5.16%): loss=0.3220, lr=2.03e-04, step_time=1834.4ms, ETA 4d 6h
12/03/2025 01:50:51 - INFO - training.fm_trainer - Step 10850/210000 (5.17%): loss=0.6692, lr=2.03e-04, step_time=1842.0ms, ETA 4d 6h
12/03/2025 01:50:51 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:51:17 - INFO - training.fm_trainer - Eval Step 10850: loss=3.3789, ppl=29.34
12/03/2025 01:51:35 - INFO - training.fm_trainer - Step 10860/210000 (5.17%): loss=0.9304, lr=2.03e-04, step_time=1835.3ms, ETA 4d 6h
12/03/2025 01:51:53 - INFO - training.fm_trainer - Step 10870/210000 (5.18%): loss=1.5565, lr=2.03e-04, step_time=1827.7ms, ETA 4d 6h
12/03/2025 01:52:12 - INFO - training.fm_trainer - Step 10880/210000 (5.18%): loss=1.1795, lr=2.04e-04, grad_norm=0.06, step_time=1883.0ms, ETA 4d 6h
12/03/2025 01:52:30 - INFO - training.fm_trainer - Step 10890/210000 (5.19%): loss=0.7114, lr=2.04e-04, step_time=1841.0ms, ETA 4d 6h
12/03/2025 01:52:48 - INFO - training.fm_trainer - Step 10900/210000 (5.19%): loss=0.4280, lr=2.04e-04, step_time=1818.9ms, ETA 4d 6h
12/03/2025 01:52:48 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:53:14 - INFO - training.fm_trainer - Eval Step 10900: loss=3.3222, ppl=27.72
12/03/2025 01:53:33 - INFO - training.fm_trainer - Step 10910/210000 (5.20%): loss=6.7613, lr=2.04e-04, step_time=1848.7ms, ETA 4d 6h
12/03/2025 01:53:52 - INFO - training.fm_trainer - Step 10920/210000 (5.20%): loss=2.9825, lr=2.05e-04, step_time=1808.8ms, ETA 4d 5h
12/03/2025 01:54:10 - INFO - training.fm_trainer - Step 10930/210000 (5.20%): loss=2.6433, lr=2.05e-04, step_time=1824.6ms, ETA 4d 5h
12/03/2025 01:54:28 - INFO - training.fm_trainer - Step 10940/210000 (5.21%): loss=8.1351, lr=2.05e-04, step_time=1934.9ms, ETA 4d 6h
12/03/2025 01:54:47 - INFO - training.fm_trainer - Step 10950/210000 (5.21%): loss=5.0538, lr=2.05e-04, step_time=1853.8ms, ETA 4d 6h
12/03/2025 01:54:47 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:55:14 - INFO - training.fm_trainer - Eval Step 10950: loss=3.2062, ppl=24.68
12/03/2025 01:55:33 - INFO - training.fm_trainer - Step 10960/210000 (5.22%): loss=0.7897, lr=2.05e-04, step_time=1863.5ms, ETA 4d 6h
12/03/2025 01:55:51 - INFO - training.fm_trainer - Step 10970/210000 (5.22%): loss=1.0068, lr=2.05e-04, step_time=1826.2ms, ETA 4d 6h
12/03/2025 01:56:09 - INFO - training.fm_trainer - Step 10980/210000 (5.23%): loss=1.0525, lr=2.06e-04, step_time=1825.2ms, ETA 4d 6h
12/03/2025 01:56:28 - INFO - training.fm_trainer - Step 10990/210000 (5.23%): loss=1.1198, lr=2.06e-04, step_time=1831.4ms, ETA 4d 5h
12/03/2025 01:56:46 - INFO - training.fm_trainer - Step 11000/210000 (5.24%): loss=8.6091, lr=2.06e-04, step_time=1845.9ms, ETA 4d 5h
12/03/2025 01:56:46 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:57:12 - INFO - training.fm_trainer - Eval Step 11000: loss=3.2088, ppl=24.75
12/03/2025 01:57:13 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-11000
12/03/2025 01:57:13 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 01:57:14 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-11000/pytorch_model_fsdp_0
12/03/2025 01:57:21 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-11000/pytorch_model_fsdp_0
12/03/2025 01:57:21 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-11000
12/03/2025 01:57:21 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 01:57:23 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-11000/optimizer_0
12/03/2025 01:57:39 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-11000/optimizer_0
12/03/2025 01:57:39 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-11000
12/03/2025 01:57:39 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-11000/scheduler.bin
12/03/2025 01:57:39 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-11000/sampler.bin
12/03/2025 01:57:39 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-11000/sampler_1.bin
12/03/2025 01:57:39 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-11000/random_states_0.pkl
12/03/2025 01:57:40 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-11000
12/03/2025 01:57:40 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-10000
12/03/2025 01:58:01 - INFO - training.fm_trainer - Step 11010/210000 (5.24%): loss=0.7774, lr=2.06e-04, step_time=2083.8ms, ETA 4d 7h
12/03/2025 01:58:19 - INFO - training.fm_trainer - Step 11020/210000 (5.25%): loss=1.8978, lr=2.06e-04, step_time=1852.5ms, ETA 4d 7h
12/03/2025 01:58:38 - INFO - training.fm_trainer - Step 11030/210000 (5.25%): loss=5.1294, lr=2.06e-04, step_time=1866.2ms, ETA 4d 7h
12/03/2025 01:58:56 - INFO - training.fm_trainer - Step 11040/210000 (5.26%): loss=1.9799, lr=2.07e-04, grad_norm=0.06, step_time=1870.7ms, ETA 4d 7h
12/03/2025 01:59:14 - INFO - training.fm_trainer - Step 11050/210000 (5.26%): loss=1.3496, lr=2.07e-04, step_time=1801.6ms, ETA 4d 6h
12/03/2025 01:59:14 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 01:59:40 - INFO - training.fm_trainer - Eval Step 11050: loss=3.2241, ppl=25.13
12/03/2025 01:59:59 - INFO - training.fm_trainer - Step 11060/210000 (5.27%): loss=1.7254, lr=2.07e-04, step_time=1811.9ms, ETA 4d 6h
12/03/2025 02:00:17 - INFO - training.fm_trainer - Step 11070/210000 (5.27%): loss=1.6601, lr=2.07e-04, step_time=1834.8ms, ETA 4d 6h
12/03/2025 02:00:36 - INFO - training.fm_trainer - Step 11080/210000 (5.28%): loss=1.0511, lr=2.08e-04, step_time=1875.9ms, ETA 4d 6h
12/03/2025 02:00:54 - INFO - training.fm_trainer - Step 11090/210000 (5.28%): loss=4.8444, lr=2.08e-04, step_time=1827.4ms, ETA 4d 6h
12/03/2025 02:01:12 - INFO - training.fm_trainer - Step 11100/210000 (5.29%): loss=0.7329, lr=2.08e-04, step_time=1837.1ms, ETA 4d 6h
12/03/2025 02:01:12 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:01:39 - INFO - training.fm_trainer - Eval Step 11100: loss=3.2193, ppl=25.01
12/03/2025 02:01:57 - INFO - training.fm_trainer - Step 11110/210000 (5.29%): loss=0.6552, lr=2.08e-04, step_time=1832.9ms, ETA 4d 6h
12/03/2025 02:02:15 - INFO - training.fm_trainer - Step 11120/210000 (5.30%): loss=6.1018, lr=2.08e-04, step_time=1831.1ms, ETA 4d 6h
12/03/2025 02:02:34 - INFO - training.fm_trainer - Step 11130/210000 (5.30%): loss=0.7494, lr=2.08e-04, step_time=1832.6ms, ETA 4d 5h
12/03/2025 02:02:52 - INFO - training.fm_trainer - Step 11140/210000 (5.30%): loss=3.0006, lr=2.09e-04, step_time=1833.1ms, ETA 4d 5h
12/03/2025 02:03:11 - INFO - training.fm_trainer - Step 11150/210000 (5.31%): loss=8.8060, lr=2.09e-04, step_time=1831.9ms, ETA 4d 5h
12/03/2025 02:03:11 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:03:37 - INFO - training.fm_trainer - Eval Step 11150: loss=3.2460, ppl=25.69
12/03/2025 02:03:55 - INFO - training.fm_trainer - Step 11160/210000 (5.31%): loss=0.8800, lr=2.09e-04, step_time=1849.6ms, ETA 4d 5h
12/03/2025 02:04:14 - INFO - training.fm_trainer - Step 11170/210000 (5.32%): loss=1.2354, lr=2.09e-04, step_time=1828.5ms, ETA 4d 5h
12/03/2025 02:04:32 - INFO - training.fm_trainer - Step 11180/210000 (5.32%): loss=1.6766, lr=2.09e-04, step_time=1839.5ms, ETA 4d 5h
12/03/2025 02:04:50 - INFO - training.fm_trainer - Step 11190/210000 (5.33%): loss=1.2758, lr=2.09e-04, step_time=1884.0ms, ETA 4d 5h
12/03/2025 02:05:09 - INFO - training.fm_trainer - Step 11200/210000 (5.33%): loss=8.5870, lr=2.10e-04, grad_norm=0.05, step_time=1874.0ms, ETA 4d 6h
12/03/2025 02:05:09 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:05:34 - INFO - training.fm_trainer - Eval Step 11200: loss=3.3280, ppl=27.88
12/03/2025 02:05:53 - INFO - training.fm_trainer - Step 11210/210000 (5.34%): loss=0.6843, lr=2.10e-04, step_time=1813.1ms, ETA 4d 5h
12/03/2025 02:06:11 - INFO - training.fm_trainer - Step 11220/210000 (5.34%): loss=5.1571, lr=2.10e-04, step_time=1838.6ms, ETA 4d 5h
12/03/2025 02:06:30 - INFO - training.fm_trainer - Step 11230/210000 (5.35%): loss=0.3408, lr=2.10e-04, step_time=1826.0ms, ETA 4d 5h
12/03/2025 02:06:48 - INFO - training.fm_trainer - Step 11240/210000 (5.35%): loss=1.5196, lr=2.11e-04, step_time=1833.7ms, ETA 4d 5h
12/03/2025 02:07:06 - INFO - training.fm_trainer - Step 11250/210000 (5.36%): loss=0.5439, lr=2.11e-04, step_time=1819.1ms, ETA 4d 5h
12/03/2025 02:07:06 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:07:32 - INFO - training.fm_trainer - Eval Step 11250: loss=3.3274, ppl=27.86
12/03/2025 02:07:51 - INFO - training.fm_trainer - Step 11260/210000 (5.36%): loss=3.0949, lr=2.11e-04, step_time=1841.9ms, ETA 4d 5h
12/03/2025 02:08:09 - INFO - training.fm_trainer - Step 11270/210000 (5.37%): loss=2.5276, lr=2.11e-04, step_time=1859.5ms, ETA 4d 5h
12/03/2025 02:08:28 - INFO - training.fm_trainer - Step 11280/210000 (5.37%): loss=4.5963, lr=2.11e-04, step_time=1860.9ms, ETA 4d 5h
12/03/2025 02:08:46 - INFO - training.fm_trainer - Step 11290/210000 (5.38%): loss=0.7689, lr=2.11e-04, step_time=1828.2ms, ETA 4d 5h
12/03/2025 02:09:04 - INFO - training.fm_trainer - Step 11300/210000 (5.38%): loss=2.8953, lr=2.12e-04, step_time=1841.7ms, ETA 4d 5h
12/03/2025 02:09:04 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:09:30 - INFO - training.fm_trainer - Eval Step 11300: loss=3.2376, ppl=25.47
12/03/2025 02:09:48 - INFO - training.fm_trainer - Step 11310/210000 (5.39%): loss=2.7892, lr=2.12e-04, step_time=1829.4ms, ETA 4d 5h
12/03/2025 02:10:07 - INFO - training.fm_trainer - Step 11320/210000 (5.39%): loss=0.5165, lr=2.12e-04, step_time=1815.4ms, ETA 4d 5h
12/03/2025 02:10:25 - INFO - training.fm_trainer - Step 11330/210000 (5.40%): loss=3.4478, lr=2.12e-04, step_time=1824.5ms, ETA 4d 5h
12/03/2025 02:10:43 - INFO - training.fm_trainer - Step 11340/210000 (5.40%): loss=1.0496, lr=2.12e-04, step_time=1825.6ms, ETA 4d 5h
12/03/2025 02:11:02 - INFO - training.fm_trainer - Step 11350/210000 (5.40%): loss=3.5779, lr=2.12e-04, step_time=1896.9ms, ETA 4d 5h
12/03/2025 02:11:02 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:11:28 - INFO - training.fm_trainer - Eval Step 11350: loss=3.2083, ppl=24.74
12/03/2025 02:11:46 - INFO - training.fm_trainer - Step 11360/210000 (5.41%): loss=1.0706, lr=2.13e-04, grad_norm=0.04, step_time=1834.9ms, ETA 4d 5h
12/03/2025 02:12:04 - INFO - training.fm_trainer - Step 11370/210000 (5.41%): loss=3.7365, lr=2.13e-04, step_time=1842.8ms, ETA 4d 5h
12/03/2025 02:12:23 - INFO - training.fm_trainer - Step 11380/210000 (5.42%): loss=1.9732, lr=2.13e-04, step_time=1825.5ms, ETA 4d 5h
12/03/2025 02:12:41 - INFO - training.fm_trainer - Step 11390/210000 (5.42%): loss=2.7860, lr=2.13e-04, step_time=1806.5ms, ETA 4d 5h
12/03/2025 02:13:00 - INFO - training.fm_trainer - Step 11400/210000 (5.43%): loss=9.8550, lr=2.14e-04, step_time=1832.3ms, ETA 4d 5h
12/03/2025 02:13:00 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:13:26 - INFO - training.fm_trainer - Eval Step 11400: loss=3.2227, ppl=25.10
12/03/2025 02:13:44 - INFO - training.fm_trainer - Step 11410/210000 (5.43%): loss=9.0699, lr=2.14e-04, step_time=1825.2ms, ETA 4d 5h
12/03/2025 02:14:03 - INFO - training.fm_trainer - Step 11420/210000 (5.44%): loss=2.4444, lr=2.14e-04, step_time=1867.4ms, ETA 4d 5h
12/03/2025 02:14:21 - INFO - training.fm_trainer - Step 11430/210000 (5.44%): loss=1.7768, lr=2.14e-04, step_time=1825.1ms, ETA 4d 5h
12/03/2025 02:14:39 - INFO - training.fm_trainer - Step 11440/210000 (5.45%): loss=1.0846, lr=2.14e-04, step_time=1829.8ms, ETA 4d 5h
12/03/2025 02:14:58 - INFO - training.fm_trainer - Step 11450/210000 (5.45%): loss=0.8186, lr=2.14e-04, step_time=1833.5ms, ETA 4d 5h
12/03/2025 02:14:58 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:15:24 - INFO - training.fm_trainer - Eval Step 11450: loss=3.1939, ppl=24.38
12/03/2025 02:15:42 - INFO - training.fm_trainer - Step 11460/210000 (5.46%): loss=0.7103, lr=2.15e-04, step_time=1833.8ms, ETA 4d 5h
12/03/2025 02:16:00 - INFO - training.fm_trainer - Step 11470/210000 (5.46%): loss=1.5562, lr=2.15e-04, step_time=1820.9ms, ETA 4d 5h
12/03/2025 02:16:19 - INFO - training.fm_trainer - Step 11480/210000 (5.47%): loss=1.0831, lr=2.15e-04, step_time=1800.7ms, ETA 4d 4h
12/03/2025 02:16:37 - INFO - training.fm_trainer - Step 11490/210000 (5.47%): loss=4.8597, lr=2.15e-04, step_time=1833.7ms, ETA 4d 4h
12/03/2025 02:16:56 - INFO - training.fm_trainer - Step 11500/210000 (5.48%): loss=0.7430, lr=2.15e-04, step_time=1811.9ms, ETA 4d 4h
12/03/2025 02:16:56 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:17:22 - INFO - training.fm_trainer - Eval Step 11500: loss=3.2505, ppl=25.80
12/03/2025 02:17:22 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-11500
12/03/2025 02:17:22 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 02:17:22 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-11500/pytorch_model_fsdp_0
12/03/2025 02:17:31 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-11500/pytorch_model_fsdp_0
12/03/2025 02:17:31 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-11500
12/03/2025 02:17:31 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 02:17:33 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-11500/optimizer_0
12/03/2025 02:17:50 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-11500/optimizer_0
12/03/2025 02:17:50 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-11500
12/03/2025 02:17:50 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-11500/scheduler.bin
12/03/2025 02:17:50 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-11500/sampler.bin
12/03/2025 02:17:50 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-11500/sampler_1.bin
12/03/2025 02:17:50 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-11500/random_states_0.pkl
12/03/2025 02:17:50 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-11500
12/03/2025 02:17:50 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-10500
12/03/2025 02:18:11 - INFO - training.fm_trainer - Step 11510/210000 (5.48%): loss=4.7352, lr=2.15e-04, step_time=1820.2ms, ETA 4d 4h
12/03/2025 02:18:29 - INFO - training.fm_trainer - Step 11520/210000 (5.49%): loss=2.3006, lr=2.16e-04, grad_norm=0.03, step_time=1864.3ms, ETA 4d 5h
12/03/2025 02:18:48 - INFO - training.fm_trainer - Step 11530/210000 (5.49%): loss=0.9346, lr=2.16e-04, step_time=1823.8ms, ETA 4d 4h
12/03/2025 02:19:06 - INFO - training.fm_trainer - Step 11540/210000 (5.50%): loss=0.6817, lr=2.16e-04, step_time=1815.4ms, ETA 4d 4h
12/03/2025 02:19:24 - INFO - training.fm_trainer - Step 11550/210000 (5.50%): loss=2.5717, lr=2.16e-04, step_time=1820.7ms, ETA 4d 4h
12/03/2025 02:19:24 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:19:50 - INFO - training.fm_trainer - Eval Step 11550: loss=3.2393, ppl=25.52
12/03/2025 02:20:08 - INFO - training.fm_trainer - Step 11560/210000 (5.50%): loss=0.9780, lr=2.17e-04, step_time=1800.0ms, ETA 4d 4h
12/03/2025 02:20:27 - INFO - training.fm_trainer - Step 11570/210000 (5.51%): loss=1.2338, lr=2.17e-04, step_time=1835.8ms, ETA 4d 4h
12/03/2025 02:20:45 - INFO - training.fm_trainer - Step 11580/210000 (5.51%): loss=4.5403, lr=2.17e-04, step_time=1824.7ms, ETA 4d 4h
12/03/2025 02:21:04 - INFO - training.fm_trainer - Step 11590/210000 (5.52%): loss=1.6628, lr=2.17e-04, step_time=1804.6ms, ETA 4d 4h
12/03/2025 02:21:22 - INFO - training.fm_trainer - Step 11600/210000 (5.52%): loss=9.6020, lr=2.17e-04, step_time=1879.6ms, ETA 4d 4h
12/03/2025 02:21:22 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:21:48 - INFO - training.fm_trainer - Eval Step 11600: loss=3.2895, ppl=26.83
12/03/2025 02:22:06 - INFO - training.fm_trainer - Step 11610/210000 (5.53%): loss=3.1144, lr=2.17e-04, step_time=1822.6ms, ETA 4d 4h
12/03/2025 02:22:24 - INFO - training.fm_trainer - Step 11620/210000 (5.53%): loss=0.5184, lr=2.18e-04, step_time=1801.5ms, ETA 4d 4h
12/03/2025 02:22:43 - INFO - training.fm_trainer - Step 11630/210000 (5.54%): loss=9.6986, lr=2.18e-04, step_time=1835.4ms, ETA 4d 4h
12/03/2025 02:23:01 - INFO - training.fm_trainer - Step 11640/210000 (5.54%): loss=3.2031, lr=2.18e-04, step_time=1815.9ms, ETA 4d 4h
12/03/2025 02:23:20 - INFO - training.fm_trainer - Step 11650/210000 (5.55%): loss=5.3396, lr=2.18e-04, step_time=1830.3ms, ETA 4d 4h
12/03/2025 02:23:20 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:23:47 - INFO - training.fm_trainer - Eval Step 11650: loss=3.3169, ppl=27.57
12/03/2025 02:24:06 - INFO - training.fm_trainer - Step 11660/210000 (5.55%): loss=2.6137, lr=2.18e-04, step_time=1832.2ms, ETA 4d 4h
12/03/2025 02:24:24 - INFO - training.fm_trainer - Step 11670/210000 (5.56%): loss=1.3323, lr=2.18e-04, step_time=1849.8ms, ETA 4d 4h
12/03/2025 02:24:43 - INFO - training.fm_trainer - Step 11680/210000 (5.56%): loss=3.9457, lr=2.19e-04, grad_norm=0.02, step_time=1887.1ms, ETA 4d 5h
12/03/2025 02:25:02 - INFO - training.fm_trainer - Step 11690/210000 (5.57%): loss=1.2614, lr=2.19e-04, step_time=1868.2ms, ETA 4d 5h
12/03/2025 02:25:20 - INFO - training.fm_trainer - Step 11700/210000 (5.57%): loss=0.4359, lr=2.19e-04, step_time=1813.6ms, ETA 4d 5h
12/03/2025 02:25:20 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:25:46 - INFO - training.fm_trainer - Eval Step 11700: loss=3.3099, ppl=27.38
12/03/2025 02:26:04 - INFO - training.fm_trainer - Step 11710/210000 (5.58%): loss=0.6963, lr=2.19e-04, step_time=1851.4ms, ETA 4d 5h
12/03/2025 02:26:23 - INFO - training.fm_trainer - Step 11720/210000 (5.58%): loss=1.4047, lr=2.20e-04, step_time=1824.5ms, ETA 4d 5h
12/03/2025 02:26:41 - INFO - training.fm_trainer - Step 11730/210000 (5.59%): loss=0.8747, lr=2.20e-04, step_time=1862.8ms, ETA 4d 5h
12/03/2025 02:26:59 - INFO - training.fm_trainer - Step 11740/210000 (5.59%): loss=0.7748, lr=2.20e-04, step_time=1828.0ms, ETA 4d 5h
12/03/2025 02:27:18 - INFO - training.fm_trainer - Step 11750/210000 (5.60%): loss=0.6275, lr=2.20e-04, step_time=1825.7ms, ETA 4d 5h
12/03/2025 02:27:18 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:27:45 - INFO - training.fm_trainer - Eval Step 11750: loss=3.1981, ppl=24.49
12/03/2025 02:28:03 - INFO - training.fm_trainer - Step 11760/210000 (5.60%): loss=2.6820, lr=2.20e-04, step_time=1836.2ms, ETA 4d 5h
12/03/2025 02:28:22 - INFO - training.fm_trainer - Step 11770/210000 (5.60%): loss=0.7061, lr=2.20e-04, step_time=1868.3ms, ETA 4d 5h
12/03/2025 02:28:40 - INFO - training.fm_trainer - Step 11780/210000 (5.61%): loss=2.1152, lr=2.21e-04, step_time=1804.2ms, ETA 4d 5h
12/03/2025 02:28:59 - INFO - training.fm_trainer - Step 11790/210000 (5.61%): loss=0.8716, lr=2.21e-04, step_time=1809.7ms, ETA 4d 4h
12/03/2025 02:29:17 - INFO - training.fm_trainer - Step 11800/210000 (5.62%): loss=5.9406, lr=2.21e-04, step_time=1833.0ms, ETA 4d 4h
12/03/2025 02:29:17 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:29:43 - INFO - training.fm_trainer - Eval Step 11800: loss=3.2401, ppl=25.54
12/03/2025 02:30:01 - INFO - training.fm_trainer - Step 11810/210000 (5.62%): loss=1.0116, lr=2.21e-04, step_time=1822.4ms, ETA 4d 4h
12/03/2025 02:30:20 - INFO - training.fm_trainer - Step 11820/210000 (5.63%): loss=3.4579, lr=2.21e-04, step_time=1848.6ms, ETA 4d 4h
12/03/2025 02:30:39 - INFO - training.fm_trainer - Step 11830/210000 (5.63%): loss=1.0359, lr=2.21e-04, step_time=1851.2ms, ETA 4d 5h
12/03/2025 02:30:57 - INFO - training.fm_trainer - Step 11840/210000 (5.64%): loss=2.0007, lr=2.22e-04, grad_norm=0.04, step_time=1859.3ms, ETA 4d 5h
12/03/2025 02:31:15 - INFO - training.fm_trainer - Step 11850/210000 (5.64%): loss=2.1649, lr=2.22e-04, step_time=1802.5ms, ETA 4d 4h
12/03/2025 02:31:15 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:31:42 - INFO - training.fm_trainer - Eval Step 11850: loss=3.2433, ppl=25.62
12/03/2025 02:32:01 - INFO - training.fm_trainer - Step 11860/210000 (5.65%): loss=1.8130, lr=2.22e-04, step_time=1804.2ms, ETA 4d 4h
12/03/2025 02:32:19 - INFO - training.fm_trainer - Step 11870/210000 (5.65%): loss=1.2867, lr=2.22e-04, step_time=1808.3ms, ETA 4d 4h
12/03/2025 02:32:37 - INFO - training.fm_trainer - Step 11880/210000 (5.66%): loss=6.7462, lr=2.23e-04, step_time=1832.4ms, ETA 4d 4h
12/03/2025 02:32:56 - INFO - training.fm_trainer - Step 11890/210000 (5.66%): loss=2.1226, lr=2.23e-04, step_time=1840.6ms, ETA 4d 4h
12/03/2025 02:33:14 - INFO - training.fm_trainer - Step 11900/210000 (5.67%): loss=1.6472, lr=2.23e-04, step_time=1819.9ms, ETA 4d 4h
12/03/2025 02:33:14 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:33:40 - INFO - training.fm_trainer - Eval Step 11900: loss=3.2226, ppl=25.09
12/03/2025 02:33:59 - INFO - training.fm_trainer - Step 11910/210000 (5.67%): loss=0.8619, lr=2.23e-04, step_time=1925.4ms, ETA 4d 5h
12/03/2025 02:34:17 - INFO - training.fm_trainer - Step 11920/210000 (5.68%): loss=1.7049, lr=2.23e-04, step_time=1822.4ms, ETA 4d 5h
12/03/2025 02:34:36 - INFO - training.fm_trainer - Step 11930/210000 (5.68%): loss=3.7890, lr=2.23e-04, step_time=1897.7ms, ETA 4d 5h
12/03/2025 02:34:54 - INFO - training.fm_trainer - Step 11940/210000 (5.69%): loss=5.7928, lr=2.24e-04, step_time=1846.3ms, ETA 4d 5h
12/03/2025 02:35:12 - INFO - training.fm_trainer - Step 11950/210000 (5.69%): loss=1.2546, lr=2.24e-04, step_time=1843.4ms, ETA 4d 5h
12/03/2025 02:35:12 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:35:39 - INFO - training.fm_trainer - Eval Step 11950: loss=3.2351, ppl=25.41
12/03/2025 02:35:57 - INFO - training.fm_trainer - Step 11960/210000 (5.70%): loss=1.0237, lr=2.24e-04, step_time=1806.0ms, ETA 4d 5h
12/03/2025 02:36:16 - INFO - training.fm_trainer - Step 11970/210000 (5.70%): loss=2.9549, lr=2.24e-04, step_time=1814.2ms, ETA 4d 5h
12/03/2025 02:36:34 - INFO - training.fm_trainer - Step 11980/210000 (5.70%): loss=2.0371, lr=2.24e-04, step_time=1832.5ms, ETA 4d 5h
12/03/2025 02:36:53 - INFO - training.fm_trainer - Step 11990/210000 (5.71%): loss=0.9890, lr=2.24e-04, step_time=1826.2ms, ETA 4d 4h
12/03/2025 02:37:11 - INFO - training.fm_trainer - Step 12000/210000 (5.71%): loss=4.2416, lr=2.25e-04, grad_norm=0.04, step_time=1865.6ms, ETA 4d 5h
12/03/2025 02:37:11 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:37:37 - INFO - training.fm_trainer - Eval Step 12000: loss=3.2280, ppl=25.23
12/03/2025 02:37:37 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-12000
12/03/2025 02:37:37 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 02:37:38 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-12000/pytorch_model_fsdp_0
12/03/2025 02:37:46 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-12000/pytorch_model_fsdp_0
12/03/2025 02:37:46 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-12000
12/03/2025 02:37:46 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 02:37:49 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-12000/optimizer_0
12/03/2025 02:38:05 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-12000/optimizer_0
12/03/2025 02:38:05 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-12000
12/03/2025 02:38:05 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-12000/scheduler.bin
12/03/2025 02:38:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-12000/sampler.bin
12/03/2025 02:38:05 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-12000/sampler_1.bin
12/03/2025 02:38:05 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-12000/random_states_0.pkl
12/03/2025 02:38:05 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-12000
12/03/2025 02:38:05 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-11000
12/03/2025 02:38:25 - INFO - training.fm_trainer - Step 12010/210000 (5.72%): loss=3.6425, lr=2.25e-04, step_time=1820.5ms, ETA 4d 5h
12/03/2025 02:38:44 - INFO - training.fm_trainer - Step 12020/210000 (5.72%): loss=3.1340, lr=2.25e-04, step_time=1843.6ms, ETA 4d 5h
12/03/2025 02:39:02 - INFO - training.fm_trainer - Step 12030/210000 (5.73%): loss=2.1880, lr=2.25e-04, step_time=1824.5ms, ETA 4d 4h
12/03/2025 02:39:21 - INFO - training.fm_trainer - Step 12040/210000 (5.73%): loss=0.5881, lr=2.26e-04, step_time=1842.6ms, ETA 4d 5h
12/03/2025 02:39:39 - INFO - training.fm_trainer - Step 12050/210000 (5.74%): loss=2.4596, lr=2.26e-04, step_time=1817.3ms, ETA 4d 4h
12/03/2025 02:39:39 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:40:06 - INFO - training.fm_trainer - Eval Step 12050: loss=3.2342, ppl=25.38
12/03/2025 02:40:25 - INFO - training.fm_trainer - Step 12060/210000 (5.74%): loss=1.1644, lr=2.26e-04, step_time=1812.6ms, ETA 4d 4h
12/03/2025 02:40:43 - INFO - training.fm_trainer - Step 12070/210000 (5.75%): loss=3.2494, lr=2.26e-04, step_time=1831.0ms, ETA 4d 4h
12/03/2025 02:41:02 - INFO - training.fm_trainer - Step 12080/210000 (5.75%): loss=0.5297, lr=2.26e-04, step_time=1811.7ms, ETA 4d 4h
12/03/2025 02:41:20 - INFO - training.fm_trainer - Step 12090/210000 (5.76%): loss=1.7878, lr=2.26e-04, step_time=1839.2ms, ETA 4d 4h
12/03/2025 02:41:39 - INFO - training.fm_trainer - Step 12100/210000 (5.76%): loss=3.1737, lr=2.27e-04, step_time=1849.2ms, ETA 4d 4h
12/03/2025 02:41:39 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:42:05 - INFO - training.fm_trainer - Eval Step 12100: loss=3.2306, ppl=25.29
12/03/2025 02:42:23 - INFO - training.fm_trainer - Step 12110/210000 (5.77%): loss=1.0142, lr=2.27e-04, step_time=1965.5ms, ETA 4d 5h
12/03/2025 02:42:41 - INFO - training.fm_trainer - Step 12120/210000 (5.77%): loss=5.0356, lr=2.27e-04, step_time=1841.3ms, ETA 4d 5h
12/03/2025 02:43:00 - INFO - training.fm_trainer - Step 12130/210000 (5.78%): loss=7.7092, lr=2.27e-04, step_time=1851.4ms, ETA 4d 5h
12/03/2025 02:43:18 - INFO - training.fm_trainer - Step 12140/210000 (5.78%): loss=1.0022, lr=2.27e-04, step_time=1815.5ms, ETA 4d 5h
12/03/2025 02:43:36 - INFO - training.fm_trainer - Step 12150/210000 (5.79%): loss=2.3506, lr=2.27e-04, step_time=1816.9ms, ETA 4d 5h
12/03/2025 02:43:36 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:44:02 - INFO - training.fm_trainer - Eval Step 12150: loss=3.2531, ppl=25.87
12/03/2025 02:44:21 - INFO - training.fm_trainer - Step 12160/210000 (5.79%): loss=10.0860, lr=2.28e-04, grad_norm=0.15, step_time=1866.8ms, ETA 4d 5h
12/03/2025 02:44:40 - INFO - training.fm_trainer - Step 12170/210000 (5.80%): loss=9.4110, lr=2.28e-04, step_time=1826.2ms, ETA 4d 5h
12/03/2025 02:44:58 - INFO - training.fm_trainer - Step 12180/210000 (5.80%): loss=0.5779, lr=2.28e-04, step_time=1823.1ms, ETA 4d 5h
12/03/2025 02:45:16 - INFO - training.fm_trainer - Step 12190/210000 (5.80%): loss=0.9143, lr=2.28e-04, step_time=1827.5ms, ETA 4d 5h
12/03/2025 02:45:35 - INFO - training.fm_trainer - Step 12200/210000 (5.81%): loss=0.6916, lr=2.29e-04, step_time=1815.9ms, ETA 4d 4h
12/03/2025 02:45:35 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:46:02 - INFO - training.fm_trainer - Eval Step 12200: loss=3.2937, ppl=26.94
12/03/2025 02:46:20 - INFO - training.fm_trainer - Step 12210/210000 (5.81%): loss=0.4286, lr=2.29e-04, step_time=1835.0ms, ETA 4d 4h
12/03/2025 02:46:39 - INFO - training.fm_trainer - Step 12220/210000 (5.82%): loss=0.5772, lr=2.29e-04, step_time=1915.7ms, ETA 4d 5h
12/03/2025 02:46:57 - INFO - training.fm_trainer - Step 12230/210000 (5.82%): loss=1.5324, lr=2.29e-04, step_time=1834.4ms, ETA 4d 5h
12/03/2025 02:47:16 - INFO - training.fm_trainer - Step 12240/210000 (5.83%): loss=0.9303, lr=2.29e-04, step_time=1860.5ms, ETA 4d 5h
12/03/2025 02:47:34 - INFO - training.fm_trainer - Step 12250/210000 (5.83%): loss=3.5145, lr=2.29e-04, step_time=1970.8ms, ETA 4d 6h
12/03/2025 02:47:34 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:48:00 - INFO - training.fm_trainer - Eval Step 12250: loss=3.2641, ppl=26.16
12/03/2025 02:48:18 - INFO - training.fm_trainer - Step 12260/210000 (5.84%): loss=1.5248, lr=2.30e-04, step_time=1836.1ms, ETA 4d 5h
12/03/2025 02:48:37 - INFO - training.fm_trainer - Step 12270/210000 (5.84%): loss=0.7854, lr=2.30e-04, step_time=1822.3ms, ETA 4d 5h
12/03/2025 02:48:55 - INFO - training.fm_trainer - Step 12280/210000 (5.85%): loss=2.7593, lr=2.30e-04, step_time=1816.0ms, ETA 4d 5h
12/03/2025 02:49:13 - INFO - training.fm_trainer - Step 12290/210000 (5.85%): loss=7.1369, lr=2.30e-04, step_time=1816.3ms, ETA 4d 5h
12/03/2025 02:49:32 - INFO - training.fm_trainer - Step 12300/210000 (5.86%): loss=1.0577, lr=2.30e-04, step_time=1850.9ms, ETA 4d 5h
12/03/2025 02:49:32 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:49:58 - INFO - training.fm_trainer - Eval Step 12300: loss=3.2297, ppl=25.27
12/03/2025 02:50:17 - INFO - training.fm_trainer - Step 12310/210000 (5.86%): loss=7.2498, lr=2.30e-04, step_time=1806.3ms, ETA 4d 5h
12/03/2025 02:50:35 - INFO - training.fm_trainer - Step 12320/210000 (5.87%): loss=1.5271, lr=2.31e-04, grad_norm=0.04, step_time=1860.6ms, ETA 4d 5h
12/03/2025 02:50:53 - INFO - training.fm_trainer - Step 12330/210000 (5.87%): loss=1.1969, lr=2.31e-04, step_time=1858.3ms, ETA 4d 5h
12/03/2025 02:51:12 - INFO - training.fm_trainer - Step 12340/210000 (5.88%): loss=0.7758, lr=2.31e-04, step_time=1874.5ms, ETA 4d 5h
12/03/2025 02:51:30 - INFO - training.fm_trainer - Step 12350/210000 (5.88%): loss=3.5808, lr=2.31e-04, step_time=1818.3ms, ETA 4d 5h
12/03/2025 02:51:30 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:51:56 - INFO - training.fm_trainer - Eval Step 12350: loss=3.2358, ppl=25.43
12/03/2025 02:52:14 - INFO - training.fm_trainer - Step 12360/210000 (5.89%): loss=2.7788, lr=2.32e-04, step_time=1826.8ms, ETA 4d 5h
12/03/2025 02:52:33 - INFO - training.fm_trainer - Step 12370/210000 (5.89%): loss=6.7981, lr=2.32e-04, step_time=1855.7ms, ETA 4d 5h
12/03/2025 02:52:51 - INFO - training.fm_trainer - Step 12380/210000 (5.90%): loss=7.1731, lr=2.32e-04, step_time=1827.1ms, ETA 4d 5h
12/03/2025 02:53:09 - INFO - training.fm_trainer - Step 12390/210000 (5.90%): loss=1.3841, lr=2.32e-04, step_time=1826.9ms, ETA 4d 5h
12/03/2025 02:53:28 - INFO - training.fm_trainer - Step 12400/210000 (5.90%): loss=4.5513, lr=2.32e-04, step_time=1829.1ms, ETA 4d 4h
12/03/2025 02:53:28 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:53:54 - INFO - training.fm_trainer - Eval Step 12400: loss=3.2662, ppl=26.21
12/03/2025 02:54:13 - INFO - training.fm_trainer - Step 12410/210000 (5.91%): loss=0.7505, lr=2.32e-04, step_time=1848.8ms, ETA 4d 5h
12/03/2025 02:54:31 - INFO - training.fm_trainer - Step 12420/210000 (5.91%): loss=1.3149, lr=2.33e-04, step_time=1822.1ms, ETA 4d 4h
12/03/2025 02:54:50 - INFO - training.fm_trainer - Step 12430/210000 (5.92%): loss=1.8635, lr=2.33e-04, step_time=1842.5ms, ETA 4d 4h
12/03/2025 02:55:08 - INFO - training.fm_trainer - Step 12440/210000 (5.92%): loss=5.1328, lr=2.33e-04, step_time=1857.8ms, ETA 4d 5h
12/03/2025 02:55:27 - INFO - training.fm_trainer - Step 12450/210000 (5.93%): loss=1.3685, lr=2.33e-04, step_time=1809.0ms, ETA 4d 4h
12/03/2025 02:55:27 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:55:53 - INFO - training.fm_trainer - Eval Step 12450: loss=3.2939, ppl=26.95
12/03/2025 02:56:11 - INFO - training.fm_trainer - Step 12460/210000 (5.93%): loss=0.5784, lr=2.33e-04, step_time=1858.7ms, ETA 4d 4h
12/03/2025 02:56:30 - INFO - training.fm_trainer - Step 12470/210000 (5.94%): loss=0.7092, lr=2.33e-04, step_time=1820.0ms, ETA 4d 4h
12/03/2025 02:56:48 - INFO - training.fm_trainer - Step 12480/210000 (5.94%): loss=2.4955, lr=2.34e-04, grad_norm=0.02, step_time=1901.4ms, ETA 4d 5h
12/03/2025 02:57:06 - INFO - training.fm_trainer - Step 12490/210000 (5.95%): loss=0.7125, lr=2.34e-04, step_time=1813.1ms, ETA 4d 5h
12/03/2025 02:57:25 - INFO - training.fm_trainer - Step 12500/210000 (5.95%): loss=0.5965, lr=2.34e-04, step_time=1821.2ms, ETA 4d 4h
12/03/2025 02:57:25 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 02:57:51 - INFO - training.fm_trainer - Eval Step 12500: loss=3.3061, ppl=27.28
12/03/2025 02:57:51 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-12500
12/03/2025 02:57:51 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 02:57:52 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-12500/pytorch_model_fsdp_0
12/03/2025 02:58:00 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-12500/pytorch_model_fsdp_0
12/03/2025 02:58:00 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-12500
12/03/2025 02:58:00 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 02:58:02 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-12500/optimizer_0
12/03/2025 02:58:17 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-12500/optimizer_0
12/03/2025 02:58:17 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-12500
12/03/2025 02:58:17 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-12500/scheduler.bin
12/03/2025 02:58:17 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-12500/sampler.bin
12/03/2025 02:58:17 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-12500/sampler_1.bin
12/03/2025 02:58:18 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-12500/random_states_0.pkl
12/03/2025 02:58:18 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-12500
12/03/2025 02:58:18 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-11500
12/03/2025 02:58:38 - INFO - training.fm_trainer - Step 12510/210000 (5.96%): loss=6.0966, lr=2.34e-04, step_time=1830.9ms, ETA 4d 4h
12/03/2025 02:58:57 - INFO - training.fm_trainer - Step 12520/210000 (5.96%): loss=1.0117, lr=2.35e-04, step_time=1808.2ms, ETA 4d 4h
12/03/2025 02:59:16 - INFO - training.fm_trainer - Step 12530/210000 (5.97%): loss=0.5654, lr=2.35e-04, step_time=1821.8ms, ETA 4d 4h
12/03/2025 02:59:34 - INFO - training.fm_trainer - Step 12540/210000 (5.97%): loss=1.1731, lr=2.35e-04, step_time=1852.9ms, ETA 4d 4h
12/03/2025 02:59:53 - INFO - training.fm_trainer - Step 12550/210000 (5.98%): loss=6.3838, lr=2.35e-04, step_time=1827.1ms, ETA 4d 4h
12/03/2025 02:59:53 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:00:19 - INFO - training.fm_trainer - Eval Step 12550: loss=3.2621, ppl=26.10
12/03/2025 03:00:37 - INFO - training.fm_trainer - Step 12560/210000 (5.98%): loss=2.2274, lr=2.35e-04, step_time=1816.3ms, ETA 4d 4h
12/03/2025 03:00:56 - INFO - training.fm_trainer - Step 12570/210000 (5.99%): loss=5.4431, lr=2.35e-04, step_time=1828.4ms, ETA 4d 4h
12/03/2025 03:01:14 - INFO - training.fm_trainer - Step 12580/210000 (5.99%): loss=1.8406, lr=2.36e-04, step_time=1803.6ms, ETA 4d 4h
12/03/2025 03:01:33 - INFO - training.fm_trainer - Step 12590/210000 (6.00%): loss=0.9736, lr=2.36e-04, step_time=1825.4ms, ETA 4d 4h
12/03/2025 03:01:51 - INFO - training.fm_trainer - Step 12600/210000 (6.00%): loss=1.4496, lr=2.36e-04, step_time=1834.8ms, ETA 4d 4h
12/03/2025 03:01:51 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:02:18 - INFO - training.fm_trainer - Eval Step 12600: loss=3.2817, ppl=26.62
12/03/2025 03:02:37 - INFO - training.fm_trainer - Step 12610/210000 (6.00%): loss=1.7345, lr=2.36e-04, step_time=1875.0ms, ETA 4d 4h
12/03/2025 03:02:55 - INFO - training.fm_trainer - Step 12620/210000 (6.01%): loss=1.1600, lr=2.36e-04, step_time=1807.5ms, ETA 4d 4h
12/03/2025 03:03:14 - INFO - training.fm_trainer - Step 12630/210000 (6.01%): loss=0.8593, lr=2.36e-04, step_time=1815.6ms, ETA 4d 4h
12/03/2025 03:03:32 - INFO - training.fm_trainer - Step 12640/210000 (6.02%): loss=4.5766, lr=2.37e-04, grad_norm=0.03, step_time=1852.7ms, ETA 4d 4h
12/03/2025 03:03:50 - INFO - training.fm_trainer - Step 12650/210000 (6.02%): loss=0.9535, lr=2.37e-04, step_time=1822.0ms, ETA 4d 4h
12/03/2025 03:03:50 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:04:17 - INFO - training.fm_trainer - Eval Step 12650: loss=3.2376, ppl=25.47
12/03/2025 03:04:35 - INFO - training.fm_trainer - Step 12660/210000 (6.03%): loss=0.8062, lr=2.37e-04, step_time=1807.2ms, ETA 4d 4h
12/03/2025 03:04:53 - INFO - training.fm_trainer - Step 12670/210000 (6.03%): loss=3.9184, lr=2.37e-04, step_time=1907.4ms, ETA 4d 4h
12/03/2025 03:05:12 - INFO - training.fm_trainer - Step 12680/210000 (6.04%): loss=0.7817, lr=2.38e-04, step_time=1815.1ms, ETA 4d 4h
12/03/2025 03:05:30 - INFO - training.fm_trainer - Step 12690/210000 (6.04%): loss=1.4203, lr=2.38e-04, step_time=1850.5ms, ETA 4d 4h
12/03/2025 03:05:48 - INFO - training.fm_trainer - Step 12700/210000 (6.05%): loss=1.6198, lr=2.38e-04, step_time=1830.1ms, ETA 4d 4h
12/03/2025 03:05:48 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:06:14 - INFO - training.fm_trainer - Eval Step 12700: loss=3.2562, ppl=25.95
12/03/2025 03:06:33 - INFO - training.fm_trainer - Step 12710/210000 (6.05%): loss=2.3270, lr=2.38e-04, step_time=1877.5ms, ETA 4d 4h
12/03/2025 03:06:51 - INFO - training.fm_trainer - Step 12720/210000 (6.06%): loss=1.0959, lr=2.38e-04, step_time=1845.5ms, ETA 4d 4h
12/03/2025 03:07:10 - INFO - training.fm_trainer - Step 12730/210000 (6.06%): loss=1.9218, lr=2.38e-04, step_time=1928.4ms, ETA 4d 5h
12/03/2025 03:07:28 - INFO - training.fm_trainer - Step 12740/210000 (6.07%): loss=0.5734, lr=2.39e-04, step_time=1817.5ms, ETA 4d 5h
12/03/2025 03:07:47 - INFO - training.fm_trainer - Step 12750/210000 (6.07%): loss=7.2113, lr=2.39e-04, step_time=1838.5ms, ETA 4d 5h
12/03/2025 03:07:47 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:08:13 - INFO - training.fm_trainer - Eval Step 12750: loss=3.2470, ppl=25.71
12/03/2025 03:08:31 - INFO - training.fm_trainer - Step 12760/210000 (6.08%): loss=0.8943, lr=2.39e-04, step_time=1821.2ms, ETA 4d 4h
12/03/2025 03:08:50 - INFO - training.fm_trainer - Step 12770/210000 (6.08%): loss=1.0738, lr=2.39e-04, step_time=1845.9ms, ETA 4d 4h
12/03/2025 03:09:08 - INFO - training.fm_trainer - Step 12780/210000 (6.09%): loss=2.3896, lr=2.39e-04, step_time=1830.1ms, ETA 4d 4h
12/03/2025 03:09:26 - INFO - training.fm_trainer - Step 12790/210000 (6.09%): loss=1.1631, lr=2.39e-04, step_time=1850.0ms, ETA 4d 4h
12/03/2025 03:09:45 - INFO - training.fm_trainer - Step 12800/210000 (6.10%): loss=4.1258, lr=2.40e-04, grad_norm=0.04, step_time=1840.7ms, ETA 4d 4h
12/03/2025 03:09:45 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:10:11 - INFO - training.fm_trainer - Eval Step 12800: loss=3.2273, ppl=25.21
12/03/2025 03:10:30 - INFO - training.fm_trainer - Step 12810/210000 (6.10%): loss=1.3089, lr=2.40e-04, step_time=1815.1ms, ETA 4d 4h
12/03/2025 03:10:48 - INFO - training.fm_trainer - Step 12820/210000 (6.10%): loss=2.1428, lr=2.40e-04, step_time=1807.9ms, ETA 4d 4h
12/03/2025 03:11:06 - INFO - training.fm_trainer - Step 12830/210000 (6.11%): loss=5.6426, lr=2.40e-04, step_time=1849.7ms, ETA 4d 4h
12/03/2025 03:11:24 - INFO - training.fm_trainer - Step 12840/210000 (6.11%): loss=1.2377, lr=2.41e-04, step_time=1812.3ms, ETA 4d 4h
12/03/2025 03:11:43 - INFO - training.fm_trainer - Step 12850/210000 (6.12%): loss=1.2831, lr=2.41e-04, step_time=1842.3ms, ETA 4d 4h
12/03/2025 03:11:43 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:12:09 - INFO - training.fm_trainer - Eval Step 12850: loss=3.2207, ppl=25.05
12/03/2025 03:12:27 - INFO - training.fm_trainer - Step 12860/210000 (6.12%): loss=0.8173, lr=2.41e-04, step_time=1825.7ms, ETA 4d 4h
12/03/2025 03:12:46 - INFO - training.fm_trainer - Step 12870/210000 (6.13%): loss=0.9709, lr=2.41e-04, step_time=1838.6ms, ETA 4d 4h
12/03/2025 03:13:05 - INFO - training.fm_trainer - Step 12880/210000 (6.13%): loss=0.6217, lr=2.41e-04, step_time=1983.0ms, ETA 4d 5h
12/03/2025 03:13:23 - INFO - training.fm_trainer - Step 12890/210000 (6.14%): loss=3.4149, lr=2.41e-04, step_time=1854.8ms, ETA 4d 5h
12/03/2025 03:13:41 - INFO - training.fm_trainer - Step 12900/210000 (6.14%): loss=4.7666, lr=2.42e-04, step_time=1846.6ms, ETA 4d 5h
12/03/2025 03:13:41 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:14:07 - INFO - training.fm_trainer - Eval Step 12900: loss=3.2295, ppl=25.27
12/03/2025 03:14:26 - INFO - training.fm_trainer - Step 12910/210000 (6.15%): loss=0.6714, lr=2.42e-04, step_time=1811.3ms, ETA 4d 5h
12/03/2025 03:14:44 - INFO - training.fm_trainer - Step 12920/210000 (6.15%): loss=1.4067, lr=2.42e-04, step_time=1834.3ms, ETA 4d 5h
12/03/2025 03:15:02 - INFO - training.fm_trainer - Step 12930/210000 (6.16%): loss=0.5886, lr=2.42e-04, step_time=1844.2ms, ETA 4d 4h
12/03/2025 03:15:21 - INFO - training.fm_trainer - Step 12940/210000 (6.16%): loss=2.2245, lr=2.42e-04, step_time=1842.8ms, ETA 4d 4h
12/03/2025 03:15:40 - INFO - training.fm_trainer - Step 12950/210000 (6.17%): loss=1.3271, lr=2.42e-04, step_time=2094.8ms, ETA 4d 6h
12/03/2025 03:15:40 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:16:06 - INFO - training.fm_trainer - Eval Step 12950: loss=3.2235, ppl=25.12
12/03/2025 03:16:25 - INFO - training.fm_trainer - Step 12960/210000 (6.17%): loss=1.6351, lr=2.43e-04, grad_norm=0.02, step_time=1843.2ms, ETA 4d 6h
12/03/2025 03:16:43 - INFO - training.fm_trainer - Step 12970/210000 (6.18%): loss=5.0132, lr=2.43e-04, step_time=1822.4ms, ETA 4d 5h
12/03/2025 03:17:01 - INFO - training.fm_trainer - Step 12980/210000 (6.18%): loss=4.9323, lr=2.43e-04, step_time=1838.6ms, ETA 4d 5h
12/03/2025 03:17:20 - INFO - training.fm_trainer - Step 12990/210000 (6.19%): loss=0.6956, lr=2.43e-04, step_time=1919.1ms, ETA 4d 6h
12/03/2025 03:17:38 - INFO - training.fm_trainer - Step 13000/210000 (6.19%): loss=5.4374, lr=2.44e-04, step_time=1826.9ms, ETA 4d 5h
12/03/2025 03:17:38 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:18:05 - INFO - training.fm_trainer - Eval Step 13000: loss=3.2638, ppl=26.15
12/03/2025 03:18:05 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-13000
12/03/2025 03:18:05 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 03:18:06 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-13000/pytorch_model_fsdp_0
12/03/2025 03:18:15 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-13000/pytorch_model_fsdp_0
12/03/2025 03:18:15 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-13000
12/03/2025 03:18:15 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 03:18:18 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-13000/optimizer_0
12/03/2025 03:18:36 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-13000/optimizer_0
12/03/2025 03:18:36 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-13000
12/03/2025 03:18:36 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-13000/scheduler.bin
12/03/2025 03:18:36 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-13000/sampler.bin
12/03/2025 03:18:36 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-13000/sampler_1.bin
12/03/2025 03:18:36 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-13000/random_states_0.pkl
12/03/2025 03:18:36 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-13000
12/03/2025 03:18:36 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-12000
12/03/2025 03:18:56 - INFO - training.fm_trainer - Step 13010/210000 (6.20%): loss=2.2831, lr=2.44e-04, step_time=1874.5ms, ETA 4d 5h
12/03/2025 03:19:14 - INFO - training.fm_trainer - Step 13020/210000 (6.20%): loss=0.8436, lr=2.44e-04, step_time=1818.6ms, ETA 4d 5h
12/03/2025 03:19:33 - INFO - training.fm_trainer - Step 13030/210000 (6.20%): loss=1.1992, lr=2.44e-04, step_time=1823.2ms, ETA 4d 5h
12/03/2025 03:19:51 - INFO - training.fm_trainer - Step 13040/210000 (6.21%): loss=0.5437, lr=2.44e-04, step_time=1957.9ms, ETA 4d 6h
12/03/2025 03:20:10 - INFO - training.fm_trainer - Step 13050/210000 (6.21%): loss=4.3320, lr=2.44e-04, step_time=1864.4ms, ETA 4d 6h
12/03/2025 03:20:10 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:20:36 - INFO - training.fm_trainer - Eval Step 13050: loss=3.3263, ppl=27.83
12/03/2025 03:20:54 - INFO - training.fm_trainer - Step 13060/210000 (6.22%): loss=1.3070, lr=2.45e-04, step_time=1872.2ms, ETA 4d 6h
12/03/2025 03:21:13 - INFO - training.fm_trainer - Step 13070/210000 (6.22%): loss=1.4602, lr=2.45e-04, step_time=1823.5ms, ETA 4d 5h
12/03/2025 03:21:31 - INFO - training.fm_trainer - Step 13080/210000 (6.23%): loss=2.9181, lr=2.45e-04, step_time=1851.3ms, ETA 4d 5h
12/03/2025 03:21:49 - INFO - training.fm_trainer - Step 13090/210000 (6.23%): loss=0.3928, lr=2.45e-04, step_time=1820.8ms, ETA 4d 5h
12/03/2025 03:22:08 - INFO - training.fm_trainer - Step 13100/210000 (6.24%): loss=0.7688, lr=2.45e-04, step_time=1832.7ms, ETA 4d 5h
12/03/2025 03:22:08 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:22:35 - INFO - training.fm_trainer - Eval Step 13100: loss=3.3647, ppl=28.93
12/03/2025 03:22:53 - INFO - training.fm_trainer - Step 13110/210000 (6.24%): loss=0.7862, lr=2.45e-04, step_time=1830.8ms, ETA 4d 5h
12/03/2025 03:23:12 - INFO - training.fm_trainer - Step 13120/210000 (6.25%): loss=6.6070, lr=2.46e-04, grad_norm=0.04, step_time=1869.6ms, ETA 4d 5h
12/03/2025 03:23:30 - INFO - training.fm_trainer - Step 13130/210000 (6.25%): loss=5.1632, lr=2.46e-04, step_time=1817.3ms, ETA 4d 5h
12/03/2025 03:23:48 - INFO - training.fm_trainer - Step 13140/210000 (6.26%): loss=1.5501, lr=2.46e-04, step_time=1832.5ms, ETA 4d 5h
12/03/2025 03:24:07 - INFO - training.fm_trainer - Step 13150/210000 (6.26%): loss=2.6630, lr=2.46e-04, step_time=1821.1ms, ETA 4d 4h
12/03/2025 03:24:07 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:24:33 - INFO - training.fm_trainer - Eval Step 13150: loss=3.3397, ppl=28.21
12/03/2025 03:24:52 - INFO - training.fm_trainer - Step 13160/210000 (6.27%): loss=1.4137, lr=2.47e-04, step_time=1825.6ms, ETA 4d 4h
12/03/2025 03:25:10 - INFO - training.fm_trainer - Step 13170/210000 (6.27%): loss=1.0653, lr=2.47e-04, step_time=1844.8ms, ETA 4d 4h
12/03/2025 03:25:29 - INFO - training.fm_trainer - Step 13180/210000 (6.28%): loss=1.2880, lr=2.47e-04, step_time=1813.7ms, ETA 4d 4h
12/03/2025 03:25:47 - INFO - training.fm_trainer - Step 13190/210000 (6.28%): loss=2.5669, lr=2.47e-04, step_time=1830.1ms, ETA 4d 4h
12/03/2025 03:26:06 - INFO - training.fm_trainer - Step 13200/210000 (6.29%): loss=6.3797, lr=2.47e-04, step_time=1837.3ms, ETA 4d 4h
12/03/2025 03:26:06 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:26:33 - INFO - training.fm_trainer - Eval Step 13200: loss=3.2420, ppl=25.58
12/03/2025 03:26:51 - INFO - training.fm_trainer - Step 13210/210000 (6.29%): loss=1.9750, lr=2.47e-04, step_time=1804.4ms, ETA 4d 4h
12/03/2025 03:27:09 - INFO - training.fm_trainer - Step 13220/210000 (6.30%): loss=0.8759, lr=2.48e-04, step_time=1903.2ms, ETA 4d 4h
12/03/2025 03:27:28 - INFO - training.fm_trainer - Step 13230/210000 (6.30%): loss=7.2577, lr=2.48e-04, step_time=1811.9ms, ETA 4d 4h
12/03/2025 03:27:46 - INFO - training.fm_trainer - Step 13240/210000 (6.30%): loss=1.1303, lr=2.48e-04, step_time=1862.6ms, ETA 4d 4h
12/03/2025 03:28:05 - INFO - training.fm_trainer - Step 13250/210000 (6.31%): loss=4.5185, lr=2.48e-04, step_time=1837.2ms, ETA 4d 4h
12/03/2025 03:28:05 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:28:31 - INFO - training.fm_trainer - Eval Step 13250: loss=3.2622, ppl=26.11
12/03/2025 03:28:49 - INFO - training.fm_trainer - Step 13260/210000 (6.31%): loss=1.4283, lr=2.48e-04, step_time=1817.6ms, ETA 4d 4h
12/03/2025 03:29:07 - INFO - training.fm_trainer - Step 13270/210000 (6.32%): loss=0.7511, lr=2.48e-04, step_time=1824.3ms, ETA 4d 4h
12/03/2025 03:29:26 - INFO - training.fm_trainer - Step 13280/210000 (6.32%): loss=1.8189, lr=2.49e-04, grad_norm=0.06, step_time=1854.8ms, ETA 4d 4h
12/03/2025 03:29:44 - INFO - training.fm_trainer - Step 13290/210000 (6.33%): loss=1.8573, lr=2.49e-04, step_time=1868.8ms, ETA 4d 4h
12/03/2025 03:30:03 - INFO - training.fm_trainer - Step 13300/210000 (6.33%): loss=3.3124, lr=2.49e-04, step_time=1854.8ms, ETA 4d 4h
12/03/2025 03:30:03 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:30:30 - INFO - training.fm_trainer - Eval Step 13300: loss=3.2683, ppl=26.27
12/03/2025 03:30:49 - INFO - training.fm_trainer - Step 13310/210000 (6.34%): loss=0.9698, lr=2.49e-04, step_time=1816.6ms, ETA 4d 4h
12/03/2025 03:31:07 - INFO - training.fm_trainer - Step 13320/210000 (6.34%): loss=1.4169, lr=2.50e-04, step_time=1962.0ms, ETA 4d 5h
12/03/2025 03:31:26 - INFO - training.fm_trainer - Step 13330/210000 (6.35%): loss=1.0447, lr=2.50e-04, step_time=1863.0ms, ETA 4d 5h
12/03/2025 03:31:45 - INFO - training.fm_trainer - Step 13340/210000 (6.35%): loss=1.6778, lr=2.50e-04, step_time=1848.6ms, ETA 4d 5h
12/03/2025 03:32:03 - INFO - training.fm_trainer - Step 13350/210000 (6.36%): loss=1.6211, lr=2.50e-04, step_time=1947.0ms, ETA 4d 5h
12/03/2025 03:32:03 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:32:30 - INFO - training.fm_trainer - Eval Step 13350: loss=3.2649, ppl=26.18
12/03/2025 03:32:49 - INFO - training.fm_trainer - Step 13360/210000 (6.36%): loss=0.7735, lr=2.50e-04, step_time=1818.2ms, ETA 4d 5h
12/03/2025 03:33:07 - INFO - training.fm_trainer - Step 13370/210000 (6.37%): loss=3.5870, lr=2.50e-04, step_time=1895.1ms, ETA 4d 5h
12/03/2025 03:33:26 - INFO - training.fm_trainer - Step 13380/210000 (6.37%): loss=3.1698, lr=2.51e-04, step_time=1912.4ms, ETA 4d 5h
12/03/2025 03:33:44 - INFO - training.fm_trainer - Step 13390/210000 (6.38%): loss=3.0830, lr=2.51e-04, step_time=1825.7ms, ETA 4d 5h
12/03/2025 03:34:02 - INFO - training.fm_trainer - Step 13400/210000 (6.38%): loss=0.8688, lr=2.51e-04, step_time=1806.7ms, ETA 4d 5h
12/03/2025 03:34:02 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:34:29 - INFO - training.fm_trainer - Eval Step 13400: loss=3.2308, ppl=25.30
12/03/2025 03:34:47 - INFO - training.fm_trainer - Step 13410/210000 (6.39%): loss=1.5425, lr=2.51e-04, step_time=1834.2ms, ETA 4d 5h
12/03/2025 03:35:06 - INFO - training.fm_trainer - Step 13420/210000 (6.39%): loss=0.5002, lr=2.51e-04, step_time=1836.0ms, ETA 4d 5h
12/03/2025 03:35:24 - INFO - training.fm_trainer - Step 13430/210000 (6.40%): loss=9.1073, lr=2.51e-04, step_time=1834.4ms, ETA 4d 5h
12/03/2025 03:35:43 - INFO - training.fm_trainer - Step 13440/210000 (6.40%): loss=1.9535, lr=2.52e-04, grad_norm=0.11, step_time=1924.9ms, ETA 4d 5h
12/03/2025 03:36:01 - INFO - training.fm_trainer - Step 13450/210000 (6.40%): loss=8.0225, lr=2.52e-04, step_time=1860.2ms, ETA 4d 5h
12/03/2025 03:36:01 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:36:27 - INFO - training.fm_trainer - Eval Step 13450: loss=3.2754, ppl=26.45
12/03/2025 03:36:46 - INFO - training.fm_trainer - Step 13460/210000 (6.41%): loss=0.4598, lr=2.52e-04, step_time=1822.6ms, ETA 4d 5h
12/03/2025 03:37:04 - INFO - training.fm_trainer - Step 13470/210000 (6.41%): loss=2.3841, lr=2.52e-04, step_time=1818.3ms, ETA 4d 5h
12/03/2025 03:37:22 - INFO - training.fm_trainer - Step 13480/210000 (6.42%): loss=4.4487, lr=2.53e-04, step_time=1819.1ms, ETA 4d 4h
12/03/2025 03:37:41 - INFO - training.fm_trainer - Step 13490/210000 (6.42%): loss=1.0006, lr=2.53e-04, step_time=1823.8ms, ETA 4d 4h
12/03/2025 03:37:59 - INFO - training.fm_trainer - Step 13500/210000 (6.43%): loss=2.1255, lr=2.53e-04, step_time=1819.1ms, ETA 4d 4h
12/03/2025 03:37:59 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:38:25 - INFO - training.fm_trainer - Eval Step 13500: loss=3.2648, ppl=26.18
12/03/2025 03:38:26 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-13500
12/03/2025 03:38:26 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 03:38:27 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-13500/pytorch_model_fsdp_0
12/03/2025 03:38:37 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-13500/pytorch_model_fsdp_0
12/03/2025 03:38:37 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-13500
12/03/2025 03:38:37 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 03:38:40 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-13500/optimizer_0
12/03/2025 03:39:00 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-13500/optimizer_0
12/03/2025 03:39:00 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-13500
12/03/2025 03:39:00 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-13500/scheduler.bin
12/03/2025 03:39:00 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-13500/sampler.bin
12/03/2025 03:39:00 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-13500/sampler_1.bin
12/03/2025 03:39:00 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-13500/random_states_0.pkl
12/03/2025 03:39:00 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-13500
12/03/2025 03:39:00 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-12500
12/03/2025 03:39:22 - INFO - training.fm_trainer - Step 13510/210000 (6.43%): loss=1.2558, lr=2.53e-04, step_time=1891.1ms, ETA 4d 4h
12/03/2025 03:39:40 - INFO - training.fm_trainer - Step 13520/210000 (6.44%): loss=3.1005, lr=2.53e-04, step_time=1822.7ms, ETA 4d 4h
12/03/2025 03:39:59 - INFO - training.fm_trainer - Step 13530/210000 (6.44%): loss=0.5780, lr=2.53e-04, step_time=1850.7ms, ETA 4d 4h
12/03/2025 03:40:17 - INFO - training.fm_trainer - Step 13540/210000 (6.45%): loss=0.8854, lr=2.54e-04, step_time=1807.6ms, ETA 4d 4h
12/03/2025 03:40:36 - INFO - training.fm_trainer - Step 13550/210000 (6.45%): loss=4.9556, lr=2.54e-04, step_time=1831.0ms, ETA 4d 4h
12/03/2025 03:40:36 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:41:01 - INFO - training.fm_trainer - Eval Step 13550: loss=3.2846, ppl=26.70
12/03/2025 03:41:21 - INFO - training.fm_trainer - Step 13560/210000 (6.46%): loss=2.6948, lr=2.54e-04, step_time=1839.4ms, ETA 4d 4h
12/03/2025 03:41:39 - INFO - training.fm_trainer - Step 13570/210000 (6.46%): loss=3.1161, lr=2.54e-04, step_time=1828.5ms, ETA 4d 4h
12/03/2025 03:41:58 - INFO - training.fm_trainer - Step 13580/210000 (6.47%): loss=1.1312, lr=2.54e-04, step_time=1824.5ms, ETA 4d 4h
12/03/2025 03:42:16 - INFO - training.fm_trainer - Step 13590/210000 (6.47%): loss=5.3705, lr=2.54e-04, step_time=1845.1ms, ETA 4d 4h
12/03/2025 03:42:35 - INFO - training.fm_trainer - Step 13600/210000 (6.48%): loss=1.0625, lr=2.55e-04, grad_norm=0.01, step_time=1866.3ms, ETA 4d 4h
12/03/2025 03:42:35 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:43:01 - INFO - training.fm_trainer - Eval Step 13600: loss=3.2866, ppl=26.75
12/03/2025 03:43:19 - INFO - training.fm_trainer - Step 13610/210000 (6.48%): loss=4.9684, lr=2.55e-04, step_time=1838.4ms, ETA 4d 4h
12/03/2025 03:43:37 - INFO - training.fm_trainer - Step 13620/210000 (6.49%): loss=4.0249, lr=2.55e-04, step_time=1809.6ms, ETA 4d 4h
12/03/2025 03:43:56 - INFO - training.fm_trainer - Step 13630/210000 (6.49%): loss=0.5400, lr=2.55e-04, step_time=1807.7ms, ETA 4d 4h
12/03/2025 03:44:14 - INFO - training.fm_trainer - Step 13640/210000 (6.50%): loss=0.9793, lr=2.56e-04, step_time=1809.6ms, ETA 4d 3h
12/03/2025 03:44:33 - INFO - training.fm_trainer - Step 13650/210000 (6.50%): loss=6.4788, lr=2.56e-04, step_time=1830.3ms, ETA 4d 3h
12/03/2025 03:44:33 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:44:58 - INFO - training.fm_trainer - Eval Step 13650: loss=3.3065, ppl=27.29
12/03/2025 03:45:17 - INFO - training.fm_trainer - Step 13660/210000 (6.50%): loss=0.7988, lr=2.56e-04, step_time=1829.1ms, ETA 4d 3h
12/03/2025 03:45:35 - INFO - training.fm_trainer - Step 13670/210000 (6.51%): loss=5.5221, lr=2.56e-04, step_time=1929.8ms, ETA 4d 4h
12/03/2025 03:45:54 - INFO - training.fm_trainer - Step 13680/210000 (6.51%): loss=0.7387, lr=2.56e-04, step_time=1884.2ms, ETA 4d 4h
12/03/2025 03:46:12 - INFO - training.fm_trainer - Step 13690/210000 (6.52%): loss=0.6391, lr=2.56e-04, step_time=1858.1ms, ETA 4d 4h
12/03/2025 03:46:31 - INFO - training.fm_trainer - Step 13700/210000 (6.52%): loss=1.0199, lr=2.57e-04, step_time=1843.8ms, ETA 4d 4h
12/03/2025 03:46:31 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:46:57 - INFO - training.fm_trainer - Eval Step 13700: loss=3.3089, ppl=27.35
12/03/2025 03:47:15 - INFO - training.fm_trainer - Step 13710/210000 (6.53%): loss=3.0505, lr=2.57e-04, step_time=1826.3ms, ETA 4d 4h
12/03/2025 03:47:33 - INFO - training.fm_trainer - Step 13720/210000 (6.53%): loss=5.3456, lr=2.57e-04, step_time=1839.9ms, ETA 4d 4h
12/03/2025 03:47:52 - INFO - training.fm_trainer - Step 13730/210000 (6.54%): loss=4.9110, lr=2.57e-04, step_time=1826.2ms, ETA 4d 4h
12/03/2025 03:48:10 - INFO - training.fm_trainer - Step 13740/210000 (6.54%): loss=6.4600, lr=2.57e-04, step_time=1825.6ms, ETA 4d 4h
12/03/2025 03:48:28 - INFO - training.fm_trainer - Step 13750/210000 (6.55%): loss=0.9961, lr=2.57e-04, step_time=1829.8ms, ETA 4d 4h
12/03/2025 03:48:28 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:48:54 - INFO - training.fm_trainer - Eval Step 13750: loss=3.3267, ppl=27.85
12/03/2025 03:49:13 - INFO - training.fm_trainer - Step 13760/210000 (6.55%): loss=5.4166, lr=2.58e-04, grad_norm=0.01, step_time=1859.8ms, ETA 4d 4h
12/03/2025 03:49:31 - INFO - training.fm_trainer - Step 13770/210000 (6.56%): loss=2.2248, lr=2.58e-04, step_time=1812.6ms, ETA 4d 4h
12/03/2025 03:49:50 - INFO - training.fm_trainer - Step 13780/210000 (6.56%): loss=0.6724, lr=2.58e-04, step_time=1834.9ms, ETA 4d 4h
12/03/2025 03:50:08 - INFO - training.fm_trainer - Step 13790/210000 (6.57%): loss=0.9347, lr=2.58e-04, step_time=1832.0ms, ETA 4d 4h
12/03/2025 03:50:26 - INFO - training.fm_trainer - Step 13800/210000 (6.57%): loss=3.9021, lr=2.59e-04, step_time=1827.6ms, ETA 4d 4h
12/03/2025 03:50:26 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:50:52 - INFO - training.fm_trainer - Eval Step 13800: loss=3.2535, ppl=25.88
12/03/2025 03:51:11 - INFO - training.fm_trainer - Step 13810/210000 (6.58%): loss=5.2258, lr=2.59e-04, step_time=1822.3ms, ETA 4d 4h
12/03/2025 03:51:29 - INFO - training.fm_trainer - Step 13820/210000 (6.58%): loss=9.4143, lr=2.59e-04, step_time=1833.8ms, ETA 4d 4h
12/03/2025 03:51:47 - INFO - training.fm_trainer - Step 13830/210000 (6.59%): loss=4.3321, lr=2.59e-04, step_time=1920.0ms, ETA 4d 4h
12/03/2025 03:52:06 - INFO - training.fm_trainer - Step 13840/210000 (6.59%): loss=3.9956, lr=2.59e-04, step_time=1813.4ms, ETA 4d 4h
12/03/2025 03:52:24 - INFO - training.fm_trainer - Step 13850/210000 (6.60%): loss=0.8417, lr=2.59e-04, step_time=1820.9ms, ETA 4d 4h
12/03/2025 03:52:24 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:52:50 - INFO - training.fm_trainer - Eval Step 13850: loss=3.2576, ppl=25.99
12/03/2025 03:53:09 - INFO - training.fm_trainer - Step 13860/210000 (6.60%): loss=1.0236, lr=2.60e-04, step_time=1837.4ms, ETA 4d 4h
12/03/2025 03:53:27 - INFO - training.fm_trainer - Step 13870/210000 (6.60%): loss=1.9555, lr=2.60e-04, step_time=1815.0ms, ETA 4d 4h
12/03/2025 03:53:46 - INFO - training.fm_trainer - Step 13880/210000 (6.61%): loss=0.8648, lr=2.60e-04, step_time=1822.2ms, ETA 4d 3h
12/03/2025 03:54:04 - INFO - training.fm_trainer - Step 13890/210000 (6.61%): loss=3.8332, lr=2.60e-04, step_time=1815.2ms, ETA 4d 3h
12/03/2025 03:54:23 - INFO - training.fm_trainer - Step 13900/210000 (6.62%): loss=6.6398, lr=2.60e-04, step_time=1842.8ms, ETA 4d 3h
12/03/2025 03:54:23 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:54:50 - INFO - training.fm_trainer - Eval Step 13900: loss=3.2177, ppl=24.97
12/03/2025 03:55:08 - INFO - training.fm_trainer - Step 13910/210000 (6.62%): loss=1.0821, lr=2.60e-04, step_time=1835.2ms, ETA 4d 3h
12/03/2025 03:55:26 - INFO - training.fm_trainer - Step 13920/210000 (6.63%): loss=6.1206, lr=2.61e-04, grad_norm=0.02, step_time=1865.3ms, ETA 4d 4h
12/03/2025 03:55:45 - INFO - training.fm_trainer - Step 13930/210000 (6.63%): loss=7.8568, lr=2.61e-04, step_time=1847.3ms, ETA 4d 4h
12/03/2025 03:56:03 - INFO - training.fm_trainer - Step 13940/210000 (6.64%): loss=0.7184, lr=2.61e-04, step_time=1826.4ms, ETA 4d 4h
12/03/2025 03:56:21 - INFO - training.fm_trainer - Step 13950/210000 (6.64%): loss=1.3393, lr=2.61e-04, step_time=1820.0ms, ETA 4d 3h
12/03/2025 03:56:21 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:56:47 - INFO - training.fm_trainer - Eval Step 13950: loss=3.2159, ppl=24.93
12/03/2025 03:57:06 - INFO - training.fm_trainer - Step 13960/210000 (6.65%): loss=1.8458, lr=2.62e-04, step_time=1819.5ms, ETA 4d 3h
12/03/2025 03:57:24 - INFO - training.fm_trainer - Step 13970/210000 (6.65%): loss=1.1535, lr=2.62e-04, step_time=1824.8ms, ETA 4d 3h
12/03/2025 03:57:43 - INFO - training.fm_trainer - Step 13980/210000 (6.66%): loss=1.0753, lr=2.62e-04, step_time=1824.8ms, ETA 4d 3h
12/03/2025 03:58:01 - INFO - training.fm_trainer - Step 13990/210000 (6.66%): loss=4.9072, lr=2.62e-04, step_time=1920.5ms, ETA 4d 4h
12/03/2025 03:58:19 - INFO - training.fm_trainer - Step 14000/210000 (6.67%): loss=1.5887, lr=2.62e-04, step_time=1829.3ms, ETA 4d 4h
12/03/2025 03:58:19 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 03:58:46 - INFO - training.fm_trainer - Eval Step 14000: loss=3.2153, ppl=24.91
12/03/2025 03:58:46 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-14000
12/03/2025 03:58:46 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 03:58:47 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-14000/pytorch_model_fsdp_0
12/03/2025 03:58:56 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-14000/pytorch_model_fsdp_0
12/03/2025 03:58:56 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-14000
12/03/2025 03:58:56 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 03:58:59 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-14000/optimizer_0
12/03/2025 03:59:19 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-14000/optimizer_0
12/03/2025 03:59:19 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-14000
12/03/2025 03:59:19 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-14000/scheduler.bin
12/03/2025 03:59:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-14000/sampler.bin
12/03/2025 03:59:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-14000/sampler_1.bin
12/03/2025 03:59:19 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-14000/random_states_0.pkl
12/03/2025 03:59:19 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-14000
12/03/2025 03:59:19 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-13000
12/03/2025 03:59:39 - INFO - training.fm_trainer - Step 14010/210000 (6.67%): loss=4.2111, lr=2.62e-04, step_time=1810.6ms, ETA 4d 3h
12/03/2025 03:59:57 - INFO - training.fm_trainer - Step 14020/210000 (6.68%): loss=0.8239, lr=2.63e-04, step_time=1815.5ms, ETA 4d 3h
12/03/2025 04:00:16 - INFO - training.fm_trainer - Step 14030/210000 (6.68%): loss=1.7242, lr=2.63e-04, step_time=1842.1ms, ETA 4d 3h
12/03/2025 04:00:35 - INFO - training.fm_trainer - Step 14040/210000 (6.69%): loss=1.1744, lr=2.63e-04, step_time=1846.8ms, ETA 4d 3h
12/03/2025 04:00:53 - INFO - training.fm_trainer - Step 14050/210000 (6.69%): loss=3.4376, lr=2.63e-04, step_time=1858.6ms, ETA 4d 4h
12/03/2025 04:00:53 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:01:19 - INFO - training.fm_trainer - Eval Step 14050: loss=3.2551, ppl=25.92
12/03/2025 04:01:38 - INFO - training.fm_trainer - Step 14060/210000 (6.70%): loss=1.3765, lr=2.63e-04, step_time=1823.9ms, ETA 4d 3h
12/03/2025 04:01:56 - INFO - training.fm_trainer - Step 14070/210000 (6.70%): loss=9.1600, lr=2.63e-04, step_time=1817.1ms, ETA 4d 3h
12/03/2025 04:02:15 - INFO - training.fm_trainer - Step 14080/210000 (6.70%): loss=5.7172, lr=2.64e-04, grad_norm=0.03, step_time=1863.2ms, ETA 4d 4h
12/03/2025 04:02:33 - INFO - training.fm_trainer - Step 14090/210000 (6.71%): loss=1.7763, lr=2.64e-04, step_time=1843.6ms, ETA 4d 4h
12/03/2025 04:02:51 - INFO - training.fm_trainer - Step 14100/210000 (6.71%): loss=0.7038, lr=2.64e-04, step_time=1824.5ms, ETA 4d 3h
12/03/2025 04:02:51 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:03:18 - INFO - training.fm_trainer - Eval Step 14100: loss=3.2718, ppl=26.36
12/03/2025 04:03:36 - INFO - training.fm_trainer - Step 14110/210000 (6.72%): loss=0.9233, lr=2.64e-04, step_time=1879.4ms, ETA 4d 4h
12/03/2025 04:03:55 - INFO - training.fm_trainer - Step 14120/210000 (6.72%): loss=1.0015, lr=2.65e-04, step_time=1862.3ms, ETA 4d 4h
12/03/2025 04:04:13 - INFO - training.fm_trainer - Step 14130/210000 (6.73%): loss=0.9246, lr=2.65e-04, step_time=1818.9ms, ETA 4d 4h
12/03/2025 04:04:31 - INFO - training.fm_trainer - Step 14140/210000 (6.73%): loss=2.2367, lr=2.65e-04, step_time=1821.7ms, ETA 4d 4h
12/03/2025 04:04:50 - INFO - training.fm_trainer - Step 14150/210000 (6.74%): loss=0.9034, lr=2.65e-04, step_time=1869.3ms, ETA 4d 4h
12/03/2025 04:04:50 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:05:16 - INFO - training.fm_trainer - Eval Step 14150: loss=3.2436, ppl=25.63
12/03/2025 04:05:34 - INFO - training.fm_trainer - Step 14160/210000 (6.74%): loss=1.0565, lr=2.65e-04, step_time=1825.8ms, ETA 4d 4h
12/03/2025 04:05:53 - INFO - training.fm_trainer - Step 14170/210000 (6.75%): loss=8.5434, lr=2.65e-04, step_time=1873.9ms, ETA 4d 4h
12/03/2025 04:06:11 - INFO - training.fm_trainer - Step 14180/210000 (6.75%): loss=1.6703, lr=2.66e-04, step_time=1817.6ms, ETA 4d 4h
12/03/2025 04:06:30 - INFO - training.fm_trainer - Step 14190/210000 (6.76%): loss=1.3346, lr=2.66e-04, step_time=1819.9ms, ETA 4d 4h
12/03/2025 04:06:48 - INFO - training.fm_trainer - Step 14200/210000 (6.76%): loss=5.3473, lr=2.66e-04, step_time=1907.6ms, ETA 4d 4h
12/03/2025 04:06:48 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:07:14 - INFO - training.fm_trainer - Eval Step 14200: loss=3.2203, ppl=25.04
12/03/2025 04:07:33 - INFO - training.fm_trainer - Step 14210/210000 (6.77%): loss=0.9443, lr=2.66e-04, step_time=1822.8ms, ETA 4d 4h
12/03/2025 04:07:51 - INFO - training.fm_trainer - Step 14220/210000 (6.77%): loss=6.1362, lr=2.66e-04, step_time=1829.0ms, ETA 4d 4h
12/03/2025 04:08:09 - INFO - training.fm_trainer - Step 14230/210000 (6.78%): loss=2.5248, lr=2.66e-04, step_time=1823.9ms, ETA 4d 4h
12/03/2025 04:08:28 - INFO - training.fm_trainer - Step 14240/210000 (6.78%): loss=1.1500, lr=2.67e-04, grad_norm=0.07, step_time=1853.5ms, ETA 4d 4h
12/03/2025 04:08:46 - INFO - training.fm_trainer - Step 14250/210000 (6.79%): loss=5.0692, lr=2.67e-04, step_time=1829.0ms, ETA 4d 4h
12/03/2025 04:08:46 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:09:12 - INFO - training.fm_trainer - Eval Step 14250: loss=3.2570, ppl=25.97
12/03/2025 04:09:30 - INFO - training.fm_trainer - Step 14260/210000 (6.79%): loss=1.0223, lr=2.67e-04, step_time=1834.6ms, ETA 4d 4h
12/03/2025 04:09:49 - INFO - training.fm_trainer - Step 14270/210000 (6.80%): loss=7.9664, lr=2.67e-04, step_time=1849.1ms, ETA 4d 4h
12/03/2025 04:10:07 - INFO - training.fm_trainer - Step 14280/210000 (6.80%): loss=2.0352, lr=2.68e-04, step_time=1826.3ms, ETA 4d 3h
12/03/2025 04:10:25 - INFO - training.fm_trainer - Step 14290/210000 (6.80%): loss=1.2365, lr=2.68e-04, step_time=1814.3ms, ETA 4d 3h
12/03/2025 04:10:44 - INFO - training.fm_trainer - Step 14300/210000 (6.81%): loss=5.6810, lr=2.68e-04, step_time=1864.0ms, ETA 4d 3h
12/03/2025 04:10:44 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:11:10 - INFO - training.fm_trainer - Eval Step 14300: loss=3.2270, ppl=25.21
12/03/2025 04:11:29 - INFO - training.fm_trainer - Step 14310/210000 (6.81%): loss=9.5438, lr=2.68e-04, step_time=1819.9ms, ETA 4d 3h
12/03/2025 04:11:48 - INFO - training.fm_trainer - Step 14320/210000 (6.82%): loss=5.5842, lr=2.68e-04, step_time=1812.3ms, ETA 4d 3h
12/03/2025 04:12:06 - INFO - training.fm_trainer - Step 14330/210000 (6.82%): loss=1.4826, lr=2.68e-04, step_time=1832.8ms, ETA 4d 3h
12/03/2025 04:12:24 - INFO - training.fm_trainer - Step 14340/210000 (6.83%): loss=0.7533, lr=2.69e-04, step_time=1849.2ms, ETA 4d 3h
12/03/2025 04:12:43 - INFO - training.fm_trainer - Step 14350/210000 (6.83%): loss=5.7604, lr=2.69e-04, step_time=1981.2ms, ETA 4d 4h
12/03/2025 04:12:43 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:13:10 - INFO - training.fm_trainer - Eval Step 14350: loss=3.2426, ppl=25.60
12/03/2025 04:13:29 - INFO - training.fm_trainer - Step 14360/210000 (6.84%): loss=1.1093, lr=2.69e-04, step_time=1820.9ms, ETA 4d 4h
12/03/2025 04:13:47 - INFO - training.fm_trainer - Step 14370/210000 (6.84%): loss=1.1566, lr=2.69e-04, step_time=1833.6ms, ETA 4d 4h
12/03/2025 04:14:05 - INFO - training.fm_trainer - Step 14380/210000 (6.85%): loss=3.8992, lr=2.69e-04, step_time=1845.2ms, ETA 4d 4h
12/03/2025 04:14:24 - INFO - training.fm_trainer - Step 14390/210000 (6.85%): loss=1.7811, lr=2.69e-04, step_time=1808.6ms, ETA 4d 4h
12/03/2025 04:14:42 - INFO - training.fm_trainer - Step 14400/210000 (6.86%): loss=6.1274, lr=2.70e-04, grad_norm=0.16, step_time=1827.9ms, ETA 4d 4h
12/03/2025 04:14:42 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:15:10 - INFO - training.fm_trainer - Eval Step 14400: loss=3.3882, ppl=29.61
12/03/2025 04:15:28 - INFO - training.fm_trainer - Step 14410/210000 (6.86%): loss=1.4348, lr=2.70e-04, step_time=1815.7ms, ETA 4d 3h
12/03/2025 04:15:46 - INFO - training.fm_trainer - Step 14420/210000 (6.87%): loss=3.9852, lr=2.70e-04, step_time=1820.8ms, ETA 4d 3h
12/03/2025 04:16:05 - INFO - training.fm_trainer - Step 14430/210000 (6.87%): loss=1.0323, lr=2.70e-04, step_time=1829.1ms, ETA 4d 3h
12/03/2025 04:16:23 - INFO - training.fm_trainer - Step 14440/210000 (6.88%): loss=3.1742, lr=2.71e-04, step_time=1943.1ms, ETA 4d 4h
12/03/2025 04:16:41 - INFO - training.fm_trainer - Step 14450/210000 (6.88%): loss=7.4220, lr=2.71e-04, step_time=1833.6ms, ETA 4d 4h
12/03/2025 04:16:41 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:17:07 - INFO - training.fm_trainer - Eval Step 14450: loss=3.3997, ppl=29.95
12/03/2025 04:17:26 - INFO - training.fm_trainer - Step 14460/210000 (6.89%): loss=3.5363, lr=2.71e-04, step_time=1842.5ms, ETA 4d 4h
12/03/2025 04:17:44 - INFO - training.fm_trainer - Step 14470/210000 (6.89%): loss=0.4396, lr=2.71e-04, step_time=1820.4ms, ETA 4d 4h
12/03/2025 04:18:03 - INFO - training.fm_trainer - Step 14480/210000 (6.90%): loss=0.6154, lr=2.71e-04, step_time=1816.1ms, ETA 4d 3h
12/03/2025 04:18:21 - INFO - training.fm_trainer - Step 14490/210000 (6.90%): loss=2.4059, lr=2.71e-04, step_time=1821.4ms, ETA 4d 3h
12/03/2025 04:18:40 - INFO - training.fm_trainer - Step 14500/210000 (6.90%): loss=3.1025, lr=2.72e-04, step_time=1814.6ms, ETA 4d 3h
12/03/2025 04:18:40 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:19:05 - INFO - training.fm_trainer - Eval Step 14500: loss=3.2954, ppl=26.99
12/03/2025 04:19:05 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-14500
12/03/2025 04:19:05 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 04:19:06 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-14500/pytorch_model_fsdp_0
12/03/2025 04:19:13 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-14500/pytorch_model_fsdp_0
12/03/2025 04:19:13 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-14500
12/03/2025 04:19:13 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 04:19:17 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-14500/optimizer_0
12/03/2025 04:19:32 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-14500/optimizer_0
12/03/2025 04:19:33 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-14500
12/03/2025 04:19:33 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-14500/scheduler.bin
12/03/2025 04:19:33 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-14500/sampler.bin
12/03/2025 04:19:33 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-14500/sampler_1.bin
12/03/2025 04:19:33 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-14500/random_states_0.pkl
12/03/2025 04:19:33 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-14500
12/03/2025 04:19:33 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-13500
12/03/2025 04:19:53 - INFO - training.fm_trainer - Step 14510/210000 (6.91%): loss=1.5008, lr=2.72e-04, step_time=1844.2ms, ETA 4d 3h
12/03/2025 04:20:11 - INFO - training.fm_trainer - Step 14520/210000 (6.91%): loss=1.0476, lr=2.72e-04, step_time=1821.4ms, ETA 4d 3h
12/03/2025 04:20:29 - INFO - training.fm_trainer - Step 14530/210000 (6.92%): loss=3.0024, lr=2.72e-04, step_time=1816.2ms, ETA 4d 3h
12/03/2025 04:20:47 - INFO - training.fm_trainer - Step 14540/210000 (6.92%): loss=7.3857, lr=2.72e-04, step_time=1823.6ms, ETA 4d 3h
12/03/2025 04:21:06 - INFO - training.fm_trainer - Step 14550/210000 (6.93%): loss=2.4213, lr=2.72e-04, step_time=1835.0ms, ETA 4d 3h
12/03/2025 04:21:06 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:21:32 - INFO - training.fm_trainer - Eval Step 14550: loss=3.2618, ppl=26.10
12/03/2025 04:21:50 - INFO - training.fm_trainer - Step 14560/210000 (6.93%): loss=1.1741, lr=2.73e-04, grad_norm=0.02, step_time=1850.1ms, ETA 4d 3h
12/03/2025 04:22:08 - INFO - training.fm_trainer - Step 14570/210000 (6.94%): loss=1.5975, lr=2.73e-04, step_time=1832.8ms, ETA 4d 3h
12/03/2025 04:22:27 - INFO - training.fm_trainer - Step 14580/210000 (6.94%): loss=5.3879, lr=2.73e-04, step_time=1848.1ms, ETA 4d 3h
12/03/2025 04:22:45 - INFO - training.fm_trainer - Step 14590/210000 (6.95%): loss=8.9987, lr=2.73e-04, step_time=1844.5ms, ETA 4d 3h
12/03/2025 04:23:04 - INFO - training.fm_trainer - Step 14600/210000 (6.95%): loss=5.8422, lr=2.74e-04, step_time=1834.7ms, ETA 4d 3h
12/03/2025 04:23:04 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:23:30 - INFO - training.fm_trainer - Eval Step 14600: loss=3.2254, ppl=25.16
12/03/2025 04:23:48 - INFO - training.fm_trainer - Step 14610/210000 (6.96%): loss=0.7748, lr=2.74e-04, step_time=1853.3ms, ETA 4d 3h
12/03/2025 04:24:06 - INFO - training.fm_trainer - Step 14620/210000 (6.96%): loss=0.7848, lr=2.74e-04, step_time=1805.9ms, ETA 4d 3h
12/03/2025 04:24:25 - INFO - training.fm_trainer - Step 14630/210000 (6.97%): loss=2.6513, lr=2.74e-04, step_time=1845.4ms, ETA 4d 3h
12/03/2025 04:24:43 - INFO - training.fm_trainer - Step 14640/210000 (6.97%): loss=5.9307, lr=2.74e-04, step_time=1829.2ms, ETA 4d 3h
12/03/2025 04:25:02 - INFO - training.fm_trainer - Step 14650/210000 (6.98%): loss=1.6435, lr=2.74e-04, step_time=1830.3ms, ETA 4d 3h
12/03/2025 04:25:02 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:25:28 - INFO - training.fm_trainer - Eval Step 14650: loss=3.2059, ppl=24.68
12/03/2025 04:25:46 - INFO - training.fm_trainer - Step 14660/210000 (6.98%): loss=1.3499, lr=2.75e-04, step_time=1836.7ms, ETA 4d 3h
12/03/2025 04:26:05 - INFO - training.fm_trainer - Step 14670/210000 (6.99%): loss=2.1047, lr=2.75e-04, step_time=1815.6ms, ETA 4d 3h
12/03/2025 04:26:23 - INFO - training.fm_trainer - Step 14680/210000 (6.99%): loss=7.8556, lr=2.75e-04, step_time=1834.0ms, ETA 4d 3h
12/03/2025 04:26:42 - INFO - training.fm_trainer - Step 14690/210000 (7.00%): loss=1.2011, lr=2.75e-04, step_time=1827.0ms, ETA 4d 3h
12/03/2025 04:27:00 - INFO - training.fm_trainer - Step 14700/210000 (7.00%): loss=1.0595, lr=2.75e-04, step_time=1857.3ms, ETA 4d 3h
12/03/2025 04:27:00 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:27:26 - INFO - training.fm_trainer - Eval Step 14700: loss=3.1981, ppl=24.49
12/03/2025 04:27:44 - INFO - training.fm_trainer - Step 14710/210000 (7.00%): loss=0.9242, lr=2.75e-04, step_time=1813.5ms, ETA 4d 3h
12/03/2025 04:28:03 - INFO - training.fm_trainer - Step 14720/210000 (7.01%): loss=1.0099, lr=2.76e-04, grad_norm=0.03, step_time=1985.5ms, ETA 4d 4h
12/03/2025 04:28:21 - INFO - training.fm_trainer - Step 14730/210000 (7.01%): loss=4.9882, lr=2.76e-04, step_time=1823.7ms, ETA 4d 4h
12/03/2025 04:28:39 - INFO - training.fm_trainer - Step 14740/210000 (7.02%): loss=1.2832, lr=2.76e-04, step_time=1829.6ms, ETA 4d 4h
12/03/2025 04:28:58 - INFO - training.fm_trainer - Step 14750/210000 (7.02%): loss=9.0216, lr=2.76e-04, step_time=1848.6ms, ETA 4d 4h
12/03/2025 04:28:58 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:29:24 - INFO - training.fm_trainer - Eval Step 14750: loss=3.2323, ppl=25.34
12/03/2025 04:29:42 - INFO - training.fm_trainer - Step 14760/210000 (7.03%): loss=0.8917, lr=2.77e-04, step_time=1851.8ms, ETA 4d 4h
12/03/2025 04:30:00 - INFO - training.fm_trainer - Step 14770/210000 (7.03%): loss=8.5043, lr=2.77e-04, step_time=1818.8ms, ETA 4d 3h
12/03/2025 04:30:19 - INFO - training.fm_trainer - Step 14780/210000 (7.04%): loss=6.5434, lr=2.77e-04, step_time=1867.4ms, ETA 4d 4h
12/03/2025 04:30:37 - INFO - training.fm_trainer - Step 14790/210000 (7.04%): loss=1.6033, lr=2.77e-04, step_time=1828.1ms, ETA 4d 3h
12/03/2025 04:30:55 - INFO - training.fm_trainer - Step 14800/210000 (7.05%): loss=0.5601, lr=2.77e-04, step_time=1849.7ms, ETA 4d 3h
12/03/2025 04:30:55 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:31:22 - INFO - training.fm_trainer - Eval Step 14800: loss=3.2469, ppl=25.71
12/03/2025 04:31:40 - INFO - training.fm_trainer - Step 14810/210000 (7.05%): loss=0.7519, lr=2.77e-04, step_time=1865.9ms, ETA 4d 4h
12/03/2025 04:31:58 - INFO - training.fm_trainer - Step 14820/210000 (7.06%): loss=6.4163, lr=2.78e-04, step_time=1834.9ms, ETA 4d 4h
12/03/2025 04:32:17 - INFO - training.fm_trainer - Step 14830/210000 (7.06%): loss=1.0363, lr=2.78e-04, step_time=1827.8ms, ETA 4d 3h
12/03/2025 04:32:35 - INFO - training.fm_trainer - Step 14840/210000 (7.07%): loss=0.8640, lr=2.78e-04, step_time=1832.0ms, ETA 4d 3h
12/03/2025 04:32:54 - INFO - training.fm_trainer - Step 14850/210000 (7.07%): loss=1.0709, lr=2.78e-04, step_time=1831.3ms, ETA 4d 3h
12/03/2025 04:32:54 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:33:20 - INFO - training.fm_trainer - Eval Step 14850: loss=3.3128, ppl=27.46
12/03/2025 04:33:38 - INFO - training.fm_trainer - Step 14860/210000 (7.08%): loss=2.0528, lr=2.78e-04, step_time=1853.4ms, ETA 4d 3h
12/03/2025 04:33:56 - INFO - training.fm_trainer - Step 14870/210000 (7.08%): loss=1.3724, lr=2.78e-04, step_time=1815.7ms, ETA 4d 3h
12/03/2025 04:34:15 - INFO - training.fm_trainer - Step 14880/210000 (7.09%): loss=0.9080, lr=2.79e-04, grad_norm=0.01, step_time=1859.1ms, ETA 4d 3h
12/03/2025 04:34:33 - INFO - training.fm_trainer - Step 14890/210000 (7.09%): loss=0.8801, lr=2.79e-04, step_time=1828.2ms, ETA 4d 3h
12/03/2025 04:34:51 - INFO - training.fm_trainer - Step 14900/210000 (7.10%): loss=0.9884, lr=2.79e-04, step_time=1805.7ms, ETA 4d 3h
12/03/2025 04:34:51 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:35:18 - INFO - training.fm_trainer - Eval Step 14900: loss=3.3046, ppl=27.24
12/03/2025 04:35:36 - INFO - training.fm_trainer - Step 14910/210000 (7.10%): loss=2.9219, lr=2.79e-04, step_time=1827.7ms, ETA 4d 3h
12/03/2025 04:35:55 - INFO - training.fm_trainer - Step 14920/210000 (7.10%): loss=9.1561, lr=2.80e-04, step_time=1835.2ms, ETA 4d 3h
12/03/2025 04:36:13 - INFO - training.fm_trainer - Step 14930/210000 (7.11%): loss=1.7418, lr=2.80e-04, step_time=1857.8ms, ETA 4d 3h
12/03/2025 04:36:32 - INFO - training.fm_trainer - Step 14940/210000 (7.11%): loss=0.7219, lr=2.80e-04, step_time=1940.0ms, ETA 4d 4h
12/03/2025 04:36:50 - INFO - training.fm_trainer - Step 14950/210000 (7.12%): loss=4.6437, lr=2.80e-04, step_time=1819.3ms, ETA 4d 3h
12/03/2025 04:36:50 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:37:17 - INFO - training.fm_trainer - Eval Step 14950: loss=3.3164, ppl=27.56
12/03/2025 04:37:36 - INFO - training.fm_trainer - Step 14960/210000 (7.12%): loss=6.4772, lr=2.80e-04, step_time=1810.5ms, ETA 4d 3h
12/03/2025 04:37:54 - INFO - training.fm_trainer - Step 14970/210000 (7.13%): loss=1.2248, lr=2.80e-04, step_time=1821.2ms, ETA 4d 3h
12/03/2025 04:38:12 - INFO - training.fm_trainer - Step 14980/210000 (7.13%): loss=1.6664, lr=2.81e-04, step_time=1821.9ms, ETA 4d 3h
12/03/2025 04:38:30 - INFO - training.fm_trainer - Step 14990/210000 (7.14%): loss=1.4260, lr=2.81e-04, step_time=1824.9ms, ETA 4d 3h
12/03/2025 04:38:49 - INFO - training.fm_trainer - Step 15000/210000 (7.14%): loss=0.8259, lr=2.81e-04, step_time=1831.3ms, ETA 4d 3h
12/03/2025 04:38:49 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:39:15 - INFO - training.fm_trainer - Eval Step 15000: loss=3.3021, ppl=27.17
12/03/2025 04:39:15 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-15000
12/03/2025 04:39:15 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 04:39:16 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-15000/pytorch_model_fsdp_0
12/03/2025 04:39:24 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-15000/pytorch_model_fsdp_0
12/03/2025 04:39:24 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-15000
12/03/2025 04:39:24 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 04:39:26 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-15000/optimizer_0
12/03/2025 04:39:42 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-15000/optimizer_0
12/03/2025 04:39:42 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-15000
12/03/2025 04:39:42 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-15000/scheduler.bin
12/03/2025 04:39:42 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-15000/sampler.bin
12/03/2025 04:39:42 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-15000/sampler_1.bin
12/03/2025 04:39:42 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-15000/random_states_0.pkl
12/03/2025 04:39:42 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-15000
12/03/2025 04:39:42 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-14000
12/03/2025 04:40:02 - INFO - training.fm_trainer - Step 15010/210000 (7.15%): loss=0.5278, lr=2.81e-04, step_time=1884.7ms, ETA 4d 3h
12/03/2025 04:40:21 - INFO - training.fm_trainer - Step 15020/210000 (7.15%): loss=4.0773, lr=2.81e-04, step_time=1822.9ms, ETA 4d 3h
12/03/2025 04:40:39 - INFO - training.fm_trainer - Step 15030/210000 (7.16%): loss=0.4466, lr=2.81e-04, step_time=1847.4ms, ETA 4d 3h
12/03/2025 04:40:58 - INFO - training.fm_trainer - Step 15040/210000 (7.16%): loss=9.3224, lr=2.82e-04, grad_norm=0.09, step_time=1872.3ms, ETA 4d 3h
12/03/2025 04:41:16 - INFO - training.fm_trainer - Step 15050/210000 (7.17%): loss=1.5854, lr=2.82e-04, step_time=1818.5ms, ETA 4d 3h
12/03/2025 04:41:16 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:41:42 - INFO - training.fm_trainer - Eval Step 15050: loss=3.2433, ppl=25.62
12/03/2025 04:42:01 - INFO - training.fm_trainer - Step 15060/210000 (7.17%): loss=1.1714, lr=2.82e-04, step_time=1812.0ms, ETA 4d 3h
12/03/2025 04:42:19 - INFO - training.fm_trainer - Step 15070/210000 (7.18%): loss=0.5600, lr=2.82e-04, step_time=1807.9ms, ETA 4d 3h
12/03/2025 04:42:37 - INFO - training.fm_trainer - Step 15080/210000 (7.18%): loss=0.5150, lr=2.83e-04, step_time=1824.8ms, ETA 4d 3h
12/03/2025 04:42:56 - INFO - training.fm_trainer - Step 15090/210000 (7.19%): loss=1.6339, lr=2.83e-04, step_time=1868.3ms, ETA 4d 3h
12/03/2025 04:43:15 - INFO - training.fm_trainer - Step 15100/210000 (7.19%): loss=9.2101, lr=2.83e-04, step_time=1838.7ms, ETA 4d 3h
12/03/2025 04:43:15 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:43:41 - INFO - training.fm_trainer - Eval Step 15100: loss=3.2111, ppl=24.81
12/03/2025 04:44:00 - INFO - training.fm_trainer - Step 15110/210000 (7.20%): loss=2.8067, lr=2.83e-04, step_time=1841.0ms, ETA 4d 3h
12/03/2025 04:44:19 - INFO - training.fm_trainer - Step 15120/210000 (7.20%): loss=0.8794, lr=2.83e-04, step_time=1806.8ms, ETA 4d 3h
12/03/2025 04:44:37 - INFO - training.fm_trainer - Step 15130/210000 (7.20%): loss=1.8625, lr=2.83e-04, step_time=1859.3ms, ETA 4d 3h
12/03/2025 04:44:56 - INFO - training.fm_trainer - Step 15140/210000 (7.21%): loss=1.5254, lr=2.84e-04, step_time=1836.6ms, ETA 4d 3h
12/03/2025 04:45:14 - INFO - training.fm_trainer - Step 15150/210000 (7.21%): loss=1.0446, lr=2.84e-04, step_time=1879.7ms, ETA 4d 3h
12/03/2025 04:45:14 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:45:40 - INFO - training.fm_trainer - Eval Step 15150: loss=3.2893, ppl=26.82
12/03/2025 04:45:59 - INFO - training.fm_trainer - Step 15160/210000 (7.22%): loss=3.1877, lr=2.84e-04, step_time=1834.5ms, ETA 4d 3h
12/03/2025 04:46:17 - INFO - training.fm_trainer - Step 15170/210000 (7.22%): loss=1.2466, lr=2.84e-04, step_time=1820.5ms, ETA 4d 3h
12/03/2025 04:46:35 - INFO - training.fm_trainer - Step 15180/210000 (7.23%): loss=2.9464, lr=2.84e-04, step_time=1832.3ms, ETA 4d 3h
12/03/2025 04:46:54 - INFO - training.fm_trainer - Step 15190/210000 (7.23%): loss=1.2325, lr=2.84e-04, step_time=1867.6ms, ETA 4d 3h
12/03/2025 04:47:12 - INFO - training.fm_trainer - Step 15200/210000 (7.24%): loss=1.2913, lr=2.85e-04, grad_norm=0.07, step_time=1865.1ms, ETA 4d 3h
12/03/2025 04:47:12 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:47:38 - INFO - training.fm_trainer - Eval Step 15200: loss=3.3517, ppl=28.55
12/03/2025 04:47:57 - INFO - training.fm_trainer - Step 15210/210000 (7.24%): loss=5.2165, lr=2.85e-04, step_time=1847.2ms, ETA 4d 3h
12/03/2025 04:48:15 - INFO - training.fm_trainer - Step 15220/210000 (7.25%): loss=1.2592, lr=2.85e-04, step_time=1839.4ms, ETA 4d 3h
12/03/2025 04:48:34 - INFO - training.fm_trainer - Step 15230/210000 (7.25%): loss=1.5646, lr=2.85e-04, step_time=1835.2ms, ETA 4d 3h
12/03/2025 04:48:52 - INFO - training.fm_trainer - Step 15240/210000 (7.26%): loss=3.7654, lr=2.86e-04, step_time=1843.0ms, ETA 4d 3h
12/03/2025 04:49:11 - INFO - training.fm_trainer - Step 15250/210000 (7.26%): loss=2.4208, lr=2.86e-04, step_time=1808.1ms, ETA 4d 3h
12/03/2025 04:49:11 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:49:37 - INFO - training.fm_trainer - Eval Step 15250: loss=3.2947, ppl=26.97
12/03/2025 04:49:55 - INFO - training.fm_trainer - Step 15260/210000 (7.27%): loss=4.5072, lr=2.86e-04, step_time=1874.6ms, ETA 4d 3h
12/03/2025 04:50:14 - INFO - training.fm_trainer - Step 15270/210000 (7.27%): loss=2.0951, lr=2.86e-04, step_time=1838.3ms, ETA 4d 3h
12/03/2025 04:50:32 - INFO - training.fm_trainer - Step 15280/210000 (7.28%): loss=0.9051, lr=2.86e-04, step_time=1824.9ms, ETA 4d 3h
12/03/2025 04:50:51 - INFO - training.fm_trainer - Step 15290/210000 (7.28%): loss=9.6400, lr=2.86e-04, step_time=2086.6ms, ETA 4d 4h
12/03/2025 04:51:10 - INFO - training.fm_trainer - Step 15300/210000 (7.29%): loss=2.2105, lr=2.87e-04, step_time=1873.2ms, ETA 4d 4h
12/03/2025 04:51:10 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:51:36 - INFO - training.fm_trainer - Eval Step 15300: loss=3.2468, ppl=25.71
12/03/2025 04:51:55 - INFO - training.fm_trainer - Step 15310/210000 (7.29%): loss=1.1266, lr=2.87e-04, step_time=1840.7ms, ETA 4d 4h
12/03/2025 04:52:13 - INFO - training.fm_trainer - Step 15320/210000 (7.30%): loss=6.8398, lr=2.87e-04, step_time=1851.6ms, ETA 4d 4h
12/03/2025 04:52:32 - INFO - training.fm_trainer - Step 15330/210000 (7.30%): loss=3.3974, lr=2.87e-04, step_time=1810.7ms, ETA 4d 4h
12/03/2025 04:52:50 - INFO - training.fm_trainer - Step 15340/210000 (7.30%): loss=3.4701, lr=2.87e-04, step_time=1828.4ms, ETA 4d 4h
12/03/2025 04:53:08 - INFO - training.fm_trainer - Step 15350/210000 (7.31%): loss=6.8924, lr=2.87e-04, step_time=1825.0ms, ETA 4d 4h
12/03/2025 04:53:08 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:53:34 - INFO - training.fm_trainer - Eval Step 15350: loss=3.2869, ppl=26.76
12/03/2025 04:53:53 - INFO - training.fm_trainer - Step 15360/210000 (7.31%): loss=4.4154, lr=2.88e-04, grad_norm=0.05, step_time=1867.7ms, ETA 4d 4h
12/03/2025 04:54:11 - INFO - training.fm_trainer - Step 15370/210000 (7.32%): loss=3.0150, lr=2.88e-04, step_time=1809.1ms, ETA 4d 3h
12/03/2025 04:54:29 - INFO - training.fm_trainer - Step 15380/210000 (7.32%): loss=1.9891, lr=2.88e-04, step_time=1817.1ms, ETA 4d 3h
12/03/2025 04:54:47 - INFO - training.fm_trainer - Step 15390/210000 (7.33%): loss=4.5345, lr=2.88e-04, step_time=1824.6ms, ETA 4d 3h
12/03/2025 04:55:06 - INFO - training.fm_trainer - Step 15400/210000 (7.33%): loss=1.1438, lr=2.89e-04, step_time=1815.1ms, ETA 4d 3h
12/03/2025 04:55:06 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:55:32 - INFO - training.fm_trainer - Eval Step 15400: loss=3.2978, ppl=27.05
12/03/2025 04:55:51 - INFO - training.fm_trainer - Step 15410/210000 (7.34%): loss=3.5597, lr=2.89e-04, step_time=1931.0ms, ETA 4d 3h
12/03/2025 04:56:10 - INFO - training.fm_trainer - Step 15420/210000 (7.34%): loss=0.4132, lr=2.89e-04, step_time=1951.7ms, ETA 4d 4h
12/03/2025 04:56:29 - INFO - training.fm_trainer - Step 15430/210000 (7.35%): loss=0.7465, lr=2.89e-04, step_time=1809.6ms, ETA 4d 4h
12/03/2025 04:56:47 - INFO - training.fm_trainer - Step 15440/210000 (7.35%): loss=4.2837, lr=2.89e-04, step_time=1846.8ms, ETA 4d 4h
12/03/2025 04:57:05 - INFO - training.fm_trainer - Step 15450/210000 (7.36%): loss=4.7756, lr=2.89e-04, step_time=1826.7ms, ETA 4d 4h
12/03/2025 04:57:05 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:57:31 - INFO - training.fm_trainer - Eval Step 15450: loss=3.2777, ppl=26.51
12/03/2025 04:57:50 - INFO - training.fm_trainer - Step 15460/210000 (7.36%): loss=4.3491, lr=2.90e-04, step_time=1833.6ms, ETA 4d 3h
12/03/2025 04:58:08 - INFO - training.fm_trainer - Step 15470/210000 (7.37%): loss=8.7507, lr=2.90e-04, step_time=1820.6ms, ETA 4d 3h
12/03/2025 04:58:27 - INFO - training.fm_trainer - Step 15480/210000 (7.37%): loss=0.6059, lr=2.90e-04, step_time=1841.6ms, ETA 4d 3h
12/03/2025 04:58:45 - INFO - training.fm_trainer - Step 15490/210000 (7.38%): loss=2.1374, lr=2.90e-04, step_time=1826.4ms, ETA 4d 3h
12/03/2025 04:59:04 - INFO - training.fm_trainer - Step 15500/210000 (7.38%): loss=1.0759, lr=2.90e-04, step_time=1819.9ms, ETA 4d 3h
12/03/2025 04:59:04 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 04:59:30 - INFO - training.fm_trainer - Eval Step 15500: loss=3.2379, ppl=25.48
12/03/2025 04:59:30 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-15500
12/03/2025 04:59:30 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 04:59:30 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-15500/pytorch_model_fsdp_0
12/03/2025 04:59:39 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-15500/pytorch_model_fsdp_0
12/03/2025 04:59:39 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-15500
12/03/2025 04:59:39 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 04:59:42 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-15500/optimizer_0
12/03/2025 04:59:58 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-15500/optimizer_0
12/03/2025 04:59:58 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-15500
12/03/2025 04:59:58 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-15500/scheduler.bin
12/03/2025 04:59:58 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-15500/sampler.bin
12/03/2025 04:59:58 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-15500/sampler_1.bin
12/03/2025 04:59:58 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-15500/random_states_0.pkl
12/03/2025 04:59:58 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-15500
12/03/2025 04:59:58 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-14500
12/03/2025 05:00:19 - INFO - training.fm_trainer - Step 15510/210000 (7.39%): loss=0.7470, lr=2.90e-04, step_time=1941.4ms, ETA 4d 4h
12/03/2025 05:00:37 - INFO - training.fm_trainer - Step 15520/210000 (7.39%): loss=4.8675, lr=2.91e-04, grad_norm=0.02, step_time=1875.5ms, ETA 4d 4h
12/03/2025 05:00:56 - INFO - training.fm_trainer - Step 15530/210000 (7.40%): loss=5.9116, lr=2.91e-04, step_time=1839.1ms, ETA 4d 4h
12/03/2025 05:01:14 - INFO - training.fm_trainer - Step 15540/210000 (7.40%): loss=1.6390, lr=2.91e-04, step_time=1817.6ms, ETA 4d 3h
12/03/2025 05:01:33 - INFO - training.fm_trainer - Step 15550/210000 (7.40%): loss=11.1219, lr=2.91e-04, step_time=1815.7ms, ETA 4d 3h
12/03/2025 05:01:33 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:02:00 - INFO - training.fm_trainer - Eval Step 15550: loss=3.2395, ppl=25.52
12/03/2025 05:02:19 - INFO - training.fm_trainer - Step 15560/210000 (7.41%): loss=1.8674, lr=2.92e-04, step_time=1840.3ms, ETA 4d 3h
12/03/2025 05:02:37 - INFO - training.fm_trainer - Step 15570/210000 (7.41%): loss=0.7304, lr=2.92e-04, step_time=1841.8ms, ETA 4d 3h
12/03/2025 05:02:55 - INFO - training.fm_trainer - Step 15580/210000 (7.42%): loss=0.6324, lr=2.92e-04, step_time=1814.1ms, ETA 4d 3h
12/03/2025 05:03:14 - INFO - training.fm_trainer - Step 15590/210000 (7.42%): loss=10.9332, lr=2.92e-04, step_time=1838.5ms, ETA 4d 3h
12/03/2025 05:03:32 - INFO - training.fm_trainer - Step 15600/210000 (7.43%): loss=2.3879, lr=2.92e-04, step_time=1865.9ms, ETA 4d 3h
12/03/2025 05:03:32 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:03:58 - INFO - training.fm_trainer - Eval Step 15600: loss=3.2989, ppl=27.08
12/03/2025 05:04:17 - INFO - training.fm_trainer - Step 15610/210000 (7.43%): loss=10.2642, lr=2.92e-04, step_time=1830.7ms, ETA 4d 3h
12/03/2025 05:04:35 - INFO - training.fm_trainer - Step 15620/210000 (7.44%): loss=0.6172, lr=2.93e-04, step_time=1816.3ms, ETA 4d 3h
12/03/2025 05:04:54 - INFO - training.fm_trainer - Step 15630/210000 (7.44%): loss=7.4857, lr=2.93e-04, step_time=1826.3ms, ETA 4d 3h
12/03/2025 05:05:12 - INFO - training.fm_trainer - Step 15640/210000 (7.45%): loss=9.4184, lr=2.93e-04, step_time=1861.1ms, ETA 4d 3h
12/03/2025 05:05:31 - INFO - training.fm_trainer - Step 15650/210000 (7.45%): loss=5.1882, lr=2.93e-04, step_time=1814.7ms, ETA 4d 3h
12/03/2025 05:05:31 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:05:57 - INFO - training.fm_trainer - Eval Step 15650: loss=3.2708, ppl=26.33
12/03/2025 05:06:15 - INFO - training.fm_trainer - Step 15660/210000 (7.46%): loss=0.4742, lr=2.93e-04, step_time=1805.5ms, ETA 4d 3h
12/03/2025 05:06:34 - INFO - training.fm_trainer - Step 15670/210000 (7.46%): loss=0.7860, lr=2.93e-04, step_time=1871.4ms, ETA 4d 3h
12/03/2025 05:06:52 - INFO - training.fm_trainer - Step 15680/210000 (7.47%): loss=0.7823, lr=2.94e-04, grad_norm=0.01, step_time=1880.0ms, ETA 4d 3h
12/03/2025 05:07:11 - INFO - training.fm_trainer - Step 15690/210000 (7.47%): loss=6.5027, lr=2.94e-04, step_time=1815.6ms, ETA 4d 3h
12/03/2025 05:07:29 - INFO - training.fm_trainer - Step 15700/210000 (7.48%): loss=3.3881, lr=2.94e-04, step_time=1832.3ms, ETA 4d 3h
12/03/2025 05:07:29 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:07:55 - INFO - training.fm_trainer - Eval Step 15700: loss=3.2418, ppl=25.58
12/03/2025 05:08:13 - INFO - training.fm_trainer - Step 15710/210000 (7.48%): loss=1.0605, lr=2.94e-04, step_time=1825.5ms, ETA 4d 3h
12/03/2025 05:08:31 - INFO - training.fm_trainer - Step 15720/210000 (7.49%): loss=10.0944, lr=2.95e-04, step_time=1823.8ms, ETA 4d 3h
12/03/2025 05:08:50 - INFO - training.fm_trainer - Step 15730/210000 (7.49%): loss=1.0076, lr=2.95e-04, step_time=1836.9ms, ETA 4d 3h
12/03/2025 05:09:08 - INFO - training.fm_trainer - Step 15740/210000 (7.50%): loss=0.7442, lr=2.95e-04, step_time=1816.6ms, ETA 4d 2h
12/03/2025 05:09:27 - INFO - training.fm_trainer - Step 15750/210000 (7.50%): loss=8.5596, lr=2.95e-04, step_time=1837.3ms, ETA 4d 3h
12/03/2025 05:09:27 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:09:54 - INFO - training.fm_trainer - Eval Step 15750: loss=3.2142, ppl=24.88
12/03/2025 05:10:12 - INFO - training.fm_trainer - Step 15760/210000 (7.50%): loss=1.1654, lr=2.95e-04, step_time=1824.8ms, ETA 4d 2h
12/03/2025 05:10:30 - INFO - training.fm_trainer - Step 15770/210000 (7.51%): loss=0.7895, lr=2.95e-04, step_time=1888.7ms, ETA 4d 3h
12/03/2025 05:10:49 - INFO - training.fm_trainer - Step 15780/210000 (7.51%): loss=1.2784, lr=2.96e-04, step_time=1850.4ms, ETA 4d 3h
12/03/2025 05:11:07 - INFO - training.fm_trainer - Step 15790/210000 (7.52%): loss=2.7008, lr=2.96e-04, step_time=1820.0ms, ETA 4d 3h
12/03/2025 05:11:25 - INFO - training.fm_trainer - Step 15800/210000 (7.52%): loss=7.3681, lr=2.96e-04, step_time=1835.5ms, ETA 4d 3h
12/03/2025 05:11:25 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:11:52 - INFO - training.fm_trainer - Eval Step 15800: loss=3.2420, ppl=25.59
12/03/2025 05:12:12 - INFO - training.fm_trainer - Step 15810/210000 (7.53%): loss=0.6965, lr=2.96e-04, step_time=1810.6ms, ETA 4d 2h
12/03/2025 05:12:30 - INFO - training.fm_trainer - Step 15820/210000 (7.53%): loss=1.6623, lr=2.96e-04, step_time=1825.4ms, ETA 4d 2h
12/03/2025 05:12:48 - INFO - training.fm_trainer - Step 15830/210000 (7.54%): loss=3.2583, lr=2.96e-04, step_time=1852.1ms, ETA 4d 3h
12/03/2025 05:13:07 - INFO - training.fm_trainer - Step 15840/210000 (7.54%): loss=1.0800, lr=2.97e-04, grad_norm=0.04, step_time=1885.0ms, ETA 4d 3h
12/03/2025 05:13:25 - INFO - training.fm_trainer - Step 15850/210000 (7.55%): loss=5.0093, lr=2.97e-04, step_time=1820.6ms, ETA 4d 3h
12/03/2025 05:13:25 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:13:52 - INFO - training.fm_trainer - Eval Step 15850: loss=3.2123, ppl=24.84
12/03/2025 05:14:10 - INFO - training.fm_trainer - Step 15860/210000 (7.55%): loss=1.2893, lr=2.97e-04, step_time=1821.8ms, ETA 4d 3h
12/03/2025 05:14:28 - INFO - training.fm_trainer - Step 15870/210000 (7.56%): loss=5.7675, lr=2.97e-04, step_time=1835.1ms, ETA 4d 3h
12/03/2025 05:14:47 - INFO - training.fm_trainer - Step 15880/210000 (7.56%): loss=6.8052, lr=2.98e-04, step_time=1812.0ms, ETA 4d 2h
12/03/2025 05:15:05 - INFO - training.fm_trainer - Step 15890/210000 (7.57%): loss=4.3731, lr=2.98e-04, step_time=1815.0ms, ETA 4d 2h
12/03/2025 05:15:24 - INFO - training.fm_trainer - Step 15900/210000 (7.57%): loss=5.0929, lr=2.98e-04, step_time=1904.4ms, ETA 4d 3h
12/03/2025 05:15:24 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:15:51 - INFO - training.fm_trainer - Eval Step 15900: loss=3.2925, ppl=26.91
12/03/2025 05:16:09 - INFO - training.fm_trainer - Step 15910/210000 (7.58%): loss=1.6400, lr=2.98e-04, step_time=1814.8ms, ETA 4d 3h
12/03/2025 05:16:28 - INFO - training.fm_trainer - Step 15920/210000 (7.58%): loss=1.4001, lr=2.98e-04, step_time=1838.3ms, ETA 4d 3h
12/03/2025 05:16:47 - INFO - training.fm_trainer - Step 15930/210000 (7.59%): loss=5.0523, lr=2.98e-04, step_time=1923.5ms, ETA 4d 3h
12/03/2025 05:17:06 - INFO - training.fm_trainer - Step 15940/210000 (7.59%): loss=3.6468, lr=2.99e-04, step_time=1834.2ms, ETA 4d 3h
12/03/2025 05:17:24 - INFO - training.fm_trainer - Step 15950/210000 (7.60%): loss=0.3112, lr=2.99e-04, step_time=1843.1ms, ETA 4d 3h
12/03/2025 05:17:24 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:17:52 - INFO - training.fm_trainer - Eval Step 15950: loss=3.3336, ppl=28.04
12/03/2025 05:18:10 - INFO - training.fm_trainer - Step 15960/210000 (7.60%): loss=0.6688, lr=2.99e-04, step_time=1826.5ms, ETA 4d 3h
12/03/2025 05:18:29 - INFO - training.fm_trainer - Step 15970/210000 (7.60%): loss=1.5722, lr=2.99e-04, step_time=1839.4ms, ETA 4d 3h
12/03/2025 05:18:48 - INFO - training.fm_trainer - Step 15980/210000 (7.61%): loss=1.2930, lr=2.99e-04, step_time=1835.7ms, ETA 4d 3h
12/03/2025 05:19:06 - INFO - training.fm_trainer - Step 15990/210000 (7.61%): loss=4.3606, lr=2.99e-04, step_time=1816.9ms, ETA 4d 3h
12/03/2025 05:19:25 - INFO - training.fm_trainer - Step 16000/210000 (7.62%): loss=5.5628, lr=3.00e-04, grad_norm=0.06, step_time=1851.4ms, ETA 4d 3h
12/03/2025 05:19:25 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:19:51 - INFO - training.fm_trainer - Eval Step 16000: loss=3.2797, ppl=26.57
12/03/2025 05:19:51 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-16000
12/03/2025 05:19:51 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 05:19:51 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-16000/pytorch_model_fsdp_0
12/03/2025 05:19:59 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-16000/pytorch_model_fsdp_0
12/03/2025 05:19:59 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-16000
12/03/2025 05:19:59 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 05:20:02 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-16000/optimizer_0
12/03/2025 05:20:17 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-16000/optimizer_0
12/03/2025 05:20:17 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-16000
12/03/2025 05:20:18 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-16000/scheduler.bin
12/03/2025 05:20:18 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-16000/sampler.bin
12/03/2025 05:20:18 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-16000/sampler_1.bin
12/03/2025 05:20:18 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-16000/random_states_0.pkl
12/03/2025 05:20:18 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-16000
12/03/2025 05:20:18 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-15000
12/03/2025 05:20:39 - INFO - training.fm_trainer - Step 16010/210000 (7.62%): loss=3.2577, lr=3.00e-04, step_time=1926.4ms, ETA 4d 3h
12/03/2025 05:20:57 - INFO - training.fm_trainer - Step 16020/210000 (7.63%): loss=2.0465, lr=3.00e-04, step_time=1869.6ms, ETA 4d 3h
12/03/2025 05:21:16 - INFO - training.fm_trainer - Step 16030/210000 (7.63%): loss=0.8027, lr=3.00e-04, step_time=1855.8ms, ETA 4d 3h
12/03/2025 05:21:34 - INFO - training.fm_trainer - Step 16040/210000 (7.64%): loss=1.2836, lr=3.00e-04, step_time=1841.0ms, ETA 4d 3h
12/03/2025 05:21:53 - INFO - training.fm_trainer - Step 16050/210000 (7.64%): loss=1.0989, lr=3.00e-04, step_time=1824.4ms, ETA 4d 3h
12/03/2025 05:21:53 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:22:18 - INFO - training.fm_trainer - Eval Step 16050: loss=3.2377, ppl=25.47
12/03/2025 05:22:37 - INFO - training.fm_trainer - Step 16060/210000 (7.65%): loss=2.0585, lr=3.00e-04, step_time=1847.3ms, ETA 4d 3h
12/03/2025 05:22:55 - INFO - training.fm_trainer - Step 16070/210000 (7.65%): loss=1.9610, lr=3.00e-04, step_time=1832.4ms, ETA 4d 3h
12/03/2025 05:23:14 - INFO - training.fm_trainer - Step 16080/210000 (7.66%): loss=5.6275, lr=3.00e-04, step_time=1871.0ms, ETA 4d 3h
12/03/2025 05:23:32 - INFO - training.fm_trainer - Step 16090/210000 (7.66%): loss=5.2127, lr=3.00e-04, step_time=1834.2ms, ETA 4d 3h
12/03/2025 05:23:51 - INFO - training.fm_trainer - Step 16100/210000 (7.67%): loss=1.7647, lr=3.00e-04, step_time=1804.7ms, ETA 4d 3h
12/03/2025 05:23:51 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:24:16 - INFO - training.fm_trainer - Eval Step 16100: loss=3.2410, ppl=25.56
12/03/2025 05:24:35 - INFO - training.fm_trainer - Step 16110/210000 (7.67%): loss=1.4059, lr=3.00e-04, step_time=1856.8ms, ETA 4d 3h
12/03/2025 05:24:53 - INFO - training.fm_trainer - Step 16120/210000 (7.68%): loss=0.6428, lr=3.00e-04, step_time=1812.7ms, ETA 4d 3h
12/03/2025 05:25:12 - INFO - training.fm_trainer - Step 16130/210000 (7.68%): loss=1.4071, lr=3.00e-04, step_time=1832.7ms, ETA 4d 3h
12/03/2025 05:25:30 - INFO - training.fm_trainer - Step 16140/210000 (7.69%): loss=0.9718, lr=3.00e-04, step_time=1862.2ms, ETA 4d 3h
12/03/2025 05:25:48 - INFO - training.fm_trainer - Step 16150/210000 (7.69%): loss=1.2194, lr=3.00e-04, step_time=1829.2ms, ETA 4d 3h
12/03/2025 05:25:48 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:26:15 - INFO - training.fm_trainer - Eval Step 16150: loss=3.2611, ppl=26.08
12/03/2025 05:26:34 - INFO - training.fm_trainer - Step 16160/210000 (7.70%): loss=2.2995, lr=3.00e-04, grad_norm=0.04, step_time=1899.5ms, ETA 4d 3h
12/03/2025 05:26:52 - INFO - training.fm_trainer - Step 16170/210000 (7.70%): loss=1.1927, lr=3.00e-04, step_time=1806.5ms, ETA 4d 3h
12/03/2025 05:27:10 - INFO - training.fm_trainer - Step 16180/210000 (7.70%): loss=1.5464, lr=3.00e-04, step_time=1831.3ms, ETA 4d 3h
12/03/2025 05:27:29 - INFO - training.fm_trainer - Step 16190/210000 (7.71%): loss=1.3912, lr=3.00e-04, step_time=1813.9ms, ETA 4d 3h
12/03/2025 05:27:47 - INFO - training.fm_trainer - Step 16200/210000 (7.71%): loss=4.9612, lr=3.00e-04, step_time=1824.7ms, ETA 4d 2h
12/03/2025 05:27:47 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:28:13 - INFO - training.fm_trainer - Eval Step 16200: loss=3.2469, ppl=25.71
12/03/2025 05:28:32 - INFO - training.fm_trainer - Step 16210/210000 (7.72%): loss=3.3188, lr=3.00e-04, step_time=1853.1ms, ETA 4d 3h
12/03/2025 05:28:50 - INFO - training.fm_trainer - Step 16220/210000 (7.72%): loss=3.1911, lr=3.00e-04, step_time=1909.2ms, ETA 4d 3h
12/03/2025 05:29:09 - INFO - training.fm_trainer - Step 16230/210000 (7.73%): loss=8.7335, lr=3.00e-04, step_time=1855.3ms, ETA 4d 3h
12/03/2025 05:29:27 - INFO - training.fm_trainer - Step 16240/210000 (7.73%): loss=2.4243, lr=3.00e-04, step_time=1837.8ms, ETA 4d 3h
12/03/2025 05:29:46 - INFO - training.fm_trainer - Step 16250/210000 (7.74%): loss=0.9175, lr=3.00e-04, step_time=1818.6ms, ETA 4d 3h
12/03/2025 05:29:46 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:30:13 - INFO - training.fm_trainer - Eval Step 16250: loss=3.2194, ppl=25.01
12/03/2025 05:30:31 - INFO - training.fm_trainer - Step 16260/210000 (7.74%): loss=0.4438, lr=3.00e-04, step_time=1853.9ms, ETA 4d 3h
12/03/2025 05:30:50 - INFO - training.fm_trainer - Step 16270/210000 (7.75%): loss=0.5691, lr=3.00e-04, step_time=1813.5ms, ETA 4d 3h
12/03/2025 05:31:08 - INFO - training.fm_trainer - Step 16280/210000 (7.75%): loss=0.7023, lr=3.00e-04, step_time=1828.6ms, ETA 4d 3h
12/03/2025 05:31:27 - INFO - training.fm_trainer - Step 16290/210000 (7.76%): loss=0.6672, lr=3.00e-04, step_time=1813.4ms, ETA 4d 2h
12/03/2025 05:31:45 - INFO - training.fm_trainer - Step 16300/210000 (7.76%): loss=1.0955, lr=3.00e-04, step_time=1836.8ms, ETA 4d 2h
12/03/2025 05:31:45 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:32:11 - INFO - training.fm_trainer - Eval Step 16300: loss=3.3154, ppl=27.53
12/03/2025 05:32:29 - INFO - training.fm_trainer - Step 16310/210000 (7.77%): loss=0.6469, lr=3.00e-04, step_time=1888.4ms, ETA 4d 3h
12/03/2025 05:32:47 - INFO - training.fm_trainer - Step 16320/210000 (7.77%): loss=1.1210, lr=3.00e-04, grad_norm=0.04, step_time=1857.5ms, ETA 4d 3h
12/03/2025 05:33:05 - INFO - training.fm_trainer - Step 16330/210000 (7.78%): loss=6.8100, lr=3.00e-04, step_time=1821.0ms, ETA 4d 3h
12/03/2025 05:33:24 - INFO - training.fm_trainer - Step 16340/210000 (7.78%): loss=2.3482, lr=3.00e-04, step_time=1842.2ms, ETA 4d 3h
12/03/2025 05:33:42 - INFO - training.fm_trainer - Step 16350/210000 (7.79%): loss=0.4613, lr=3.00e-04, step_time=1812.2ms, ETA 4d 2h
12/03/2025 05:33:42 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:34:08 - INFO - training.fm_trainer - Eval Step 16350: loss=3.3158, ppl=27.55
12/03/2025 05:34:27 - INFO - training.fm_trainer - Step 16360/210000 (7.79%): loss=4.3765, lr=3.00e-04, step_time=1870.4ms, ETA 4d 3h
12/03/2025 05:34:45 - INFO - training.fm_trainer - Step 16370/210000 (7.80%): loss=1.1312, lr=3.00e-04, step_time=1811.3ms, ETA 4d 2h
12/03/2025 05:35:03 - INFO - training.fm_trainer - Step 16380/210000 (7.80%): loss=8.8577, lr=3.00e-04, step_time=1856.9ms, ETA 4d 2h
12/03/2025 05:35:22 - INFO - training.fm_trainer - Step 16390/210000 (7.80%): loss=0.9753, lr=3.00e-04, step_time=1822.9ms, ETA 4d 2h
12/03/2025 05:35:40 - INFO - training.fm_trainer - Step 16400/210000 (7.81%): loss=2.2991, lr=3.00e-04, step_time=1832.8ms, ETA 4d 2h
12/03/2025 05:35:40 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:36:06 - INFO - training.fm_trainer - Eval Step 16400: loss=3.3378, ppl=28.16
12/03/2025 05:36:24 - INFO - training.fm_trainer - Step 16410/210000 (7.81%): loss=0.8707, lr=3.00e-04, step_time=1849.6ms, ETA 4d 2h
12/03/2025 05:36:43 - INFO - training.fm_trainer - Step 16420/210000 (7.82%): loss=0.4849, lr=3.00e-04, step_time=1816.5ms, ETA 4d 2h
12/03/2025 05:37:01 - INFO - training.fm_trainer - Step 16430/210000 (7.82%): loss=1.2405, lr=3.00e-04, step_time=1832.4ms, ETA 4d 2h
12/03/2025 05:37:19 - INFO - training.fm_trainer - Step 16440/210000 (7.83%): loss=2.8161, lr=3.00e-04, step_time=1826.8ms, ETA 4d 2h
12/03/2025 05:37:37 - INFO - training.fm_trainer - Step 16450/210000 (7.83%): loss=1.0150, lr=3.00e-04, step_time=1832.8ms, ETA 4d 2h
12/03/2025 05:37:37 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:38:03 - INFO - training.fm_trainer - Eval Step 16450: loss=3.2714, ppl=26.35
12/03/2025 05:38:22 - INFO - training.fm_trainer - Step 16460/210000 (7.84%): loss=1.6430, lr=3.00e-04, step_time=1835.6ms, ETA 4d 2h
12/03/2025 05:38:40 - INFO - training.fm_trainer - Step 16470/210000 (7.84%): loss=4.1624, lr=3.00e-04, step_time=1815.6ms, ETA 4d 2h
12/03/2025 05:38:58 - INFO - training.fm_trainer - Step 16480/210000 (7.85%): loss=1.1611, lr=3.00e-04, grad_norm=0.03, step_time=1849.8ms, ETA 4d 2h
12/03/2025 05:39:17 - INFO - training.fm_trainer - Step 16490/210000 (7.85%): loss=8.0118, lr=3.00e-04, step_time=1837.3ms, ETA 4d 2h
12/03/2025 05:39:35 - INFO - training.fm_trainer - Step 16500/210000 (7.86%): loss=6.2822, lr=3.00e-04, step_time=1819.5ms, ETA 4d 2h
12/03/2025 05:39:35 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:40:01 - INFO - training.fm_trainer - Eval Step 16500: loss=3.2643, ppl=26.16
12/03/2025 05:40:01 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-16500
12/03/2025 05:40:01 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 05:40:02 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-16500/pytorch_model_fsdp_0
12/03/2025 05:40:12 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-16500/pytorch_model_fsdp_0
12/03/2025 05:40:12 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-16500
12/03/2025 05:40:12 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 05:40:15 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-16500/optimizer_0
12/03/2025 05:40:34 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-16500/optimizer_0
12/03/2025 05:40:34 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-16500
12/03/2025 05:40:34 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-16500/scheduler.bin
12/03/2025 05:40:34 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-16500/sampler.bin
12/03/2025 05:40:34 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-16500/sampler_1.bin
12/03/2025 05:40:34 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-16500/random_states_0.pkl
12/03/2025 05:40:34 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-16500
12/03/2025 05:40:34 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-15500
12/03/2025 05:40:55 - INFO - training.fm_trainer - Step 16510/210000 (7.86%): loss=3.6923, lr=3.00e-04, step_time=1872.3ms, ETA 4d 2h
12/03/2025 05:41:14 - INFO - training.fm_trainer - Step 16520/210000 (7.87%): loss=1.1644, lr=3.00e-04, step_time=1814.0ms, ETA 4d 2h
12/03/2025 05:41:32 - INFO - training.fm_trainer - Step 16530/210000 (7.87%): loss=3.6291, lr=3.00e-04, step_time=1861.8ms, ETA 4d 2h
12/03/2025 05:41:50 - INFO - training.fm_trainer - Step 16540/210000 (7.88%): loss=2.5026, lr=3.00e-04, step_time=1831.4ms, ETA 4d 2h
12/03/2025 05:42:09 - INFO - training.fm_trainer - Step 16550/210000 (7.88%): loss=0.6571, lr=3.00e-04, step_time=1809.2ms, ETA 4d 2h
12/03/2025 05:42:09 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:42:35 - INFO - training.fm_trainer - Eval Step 16550: loss=3.2117, ppl=24.82
12/03/2025 05:42:54 - INFO - training.fm_trainer - Step 16560/210000 (7.89%): loss=8.2308, lr=3.00e-04, step_time=1839.6ms, ETA 4d 2h
12/03/2025 05:43:12 - INFO - training.fm_trainer - Step 16570/210000 (7.89%): loss=0.8304, lr=3.00e-04, step_time=1845.1ms, ETA 4d 2h
12/03/2025 05:43:30 - INFO - training.fm_trainer - Step 16580/210000 (7.90%): loss=1.2538, lr=3.00e-04, step_time=1812.8ms, ETA 4d 2h
12/03/2025 05:43:49 - INFO - training.fm_trainer - Step 16590/210000 (7.90%): loss=1.9297, lr=3.00e-04, step_time=1810.9ms, ETA 4d 2h
12/03/2025 05:44:07 - INFO - training.fm_trainer - Step 16600/210000 (7.90%): loss=4.6095, lr=3.00e-04, step_time=1819.1ms, ETA 4d 2h
12/03/2025 05:44:07 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:44:33 - INFO - training.fm_trainer - Eval Step 16600: loss=3.2500, ppl=25.79
12/03/2025 05:44:52 - INFO - training.fm_trainer - Step 16610/210000 (7.91%): loss=9.6829, lr=3.00e-04, step_time=1835.5ms, ETA 4d 2h
12/03/2025 05:45:10 - INFO - training.fm_trainer - Step 16620/210000 (7.91%): loss=3.0511, lr=3.00e-04, step_time=1816.8ms, ETA 4d 2h
12/03/2025 05:45:29 - INFO - training.fm_trainer - Step 16630/210000 (7.92%): loss=2.8818, lr=3.00e-04, step_time=1811.3ms, ETA 4d 2h
12/03/2025 05:45:47 - INFO - training.fm_trainer - Step 16640/210000 (7.92%): loss=7.5101, lr=3.00e-04, grad_norm=0.01, step_time=1927.9ms, ETA 4d 2h
12/03/2025 05:46:05 - INFO - training.fm_trainer - Step 16650/210000 (7.93%): loss=2.0324, lr=3.00e-04, step_time=1850.6ms, ETA 4d 2h
12/03/2025 05:46:05 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:46:31 - INFO - training.fm_trainer - Eval Step 16650: loss=3.2198, ppl=25.02
12/03/2025 05:46:50 - INFO - training.fm_trainer - Step 16660/210000 (7.93%): loss=1.6991, lr=3.00e-04, step_time=1860.0ms, ETA 4d 2h
12/03/2025 05:47:08 - INFO - training.fm_trainer - Step 16670/210000 (7.94%): loss=4.3558, lr=3.00e-04, step_time=1934.6ms, ETA 4d 3h
12/03/2025 05:47:26 - INFO - training.fm_trainer - Step 16680/210000 (7.94%): loss=2.1959, lr=3.00e-04, step_time=1843.6ms, ETA 4d 3h
12/03/2025 05:47:45 - INFO - training.fm_trainer - Step 16690/210000 (7.95%): loss=0.6261, lr=3.00e-04, step_time=2026.8ms, ETA 4d 4h
12/03/2025 05:48:04 - INFO - training.fm_trainer - Step 16700/210000 (7.95%): loss=1.0712, lr=3.00e-04, step_time=1840.7ms, ETA 4d 4h
12/03/2025 05:48:04 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:48:30 - INFO - training.fm_trainer - Eval Step 16700: loss=3.2184, ppl=24.99
12/03/2025 05:48:48 - INFO - training.fm_trainer - Step 16710/210000 (7.96%): loss=5.1105, lr=3.00e-04, step_time=1820.3ms, ETA 4d 3h
12/03/2025 05:49:07 - INFO - training.fm_trainer - Step 16720/210000 (7.96%): loss=1.2289, lr=3.00e-04, step_time=1828.0ms, ETA 4d 3h
12/03/2025 05:49:25 - INFO - training.fm_trainer - Step 16730/210000 (7.97%): loss=2.0923, lr=3.00e-04, step_time=1872.4ms, ETA 4d 3h
12/03/2025 05:49:43 - INFO - training.fm_trainer - Step 16740/210000 (7.97%): loss=0.4436, lr=3.00e-04, step_time=1834.0ms, ETA 4d 3h
12/03/2025 05:50:02 - INFO - training.fm_trainer - Step 16750/210000 (7.98%): loss=10.6092, lr=3.00e-04, step_time=1830.9ms, ETA 4d 3h
12/03/2025 05:50:02 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:50:28 - INFO - training.fm_trainer - Eval Step 16750: loss=3.3002, ppl=27.12
12/03/2025 05:50:47 - INFO - training.fm_trainer - Step 16760/210000 (7.98%): loss=4.7869, lr=3.00e-04, step_time=1812.5ms, ETA 4d 3h
12/03/2025 05:51:05 - INFO - training.fm_trainer - Step 16770/210000 (7.99%): loss=2.0848, lr=3.00e-04, step_time=1812.5ms, ETA 4d 3h
12/03/2025 05:51:24 - INFO - training.fm_trainer - Step 16780/210000 (7.99%): loss=5.9542, lr=3.00e-04, step_time=1802.9ms, ETA 4d 2h
12/03/2025 05:51:42 - INFO - training.fm_trainer - Step 16790/210000 (8.00%): loss=4.8431, lr=3.00e-04, step_time=1825.5ms, ETA 4d 2h
12/03/2025 05:52:00 - INFO - training.fm_trainer - Step 16800/210000 (8.00%): loss=8.7720, lr=3.00e-04, grad_norm=0.04, step_time=1839.6ms, ETA 4d 2h
12/03/2025 05:52:01 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:52:27 - INFO - training.fm_trainer - Eval Step 16800: loss=3.2504, ppl=25.80
12/03/2025 05:52:45 - INFO - training.fm_trainer - Step 16810/210000 (8.00%): loss=1.0900, lr=3.00e-04, step_time=1813.0ms, ETA 4d 2h
12/03/2025 05:53:04 - INFO - training.fm_trainer - Step 16820/210000 (8.01%): loss=6.0010, lr=3.00e-04, step_time=1869.4ms, ETA 4d 2h
12/03/2025 05:53:22 - INFO - training.fm_trainer - Step 16830/210000 (8.01%): loss=1.6811, lr=3.00e-04, step_time=1819.8ms, ETA 4d 2h
12/03/2025 05:53:41 - INFO - training.fm_trainer - Step 16840/210000 (8.02%): loss=7.2382, lr=3.00e-04, step_time=1827.9ms, ETA 4d 2h
12/03/2025 05:53:59 - INFO - training.fm_trainer - Step 16850/210000 (8.02%): loss=0.7001, lr=3.00e-04, step_time=1825.7ms, ETA 4d 2h
12/03/2025 05:53:59 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:54:25 - INFO - training.fm_trainer - Eval Step 16850: loss=3.2613, ppl=26.08
12/03/2025 05:54:43 - INFO - training.fm_trainer - Step 16860/210000 (8.03%): loss=1.0587, lr=3.00e-04, step_time=1837.6ms, ETA 4d 2h
12/03/2025 05:55:01 - INFO - training.fm_trainer - Step 16870/210000 (8.03%): loss=0.8515, lr=3.00e-04, step_time=1820.0ms, ETA 4d 2h
12/03/2025 05:55:20 - INFO - training.fm_trainer - Step 16880/210000 (8.04%): loss=1.6634, lr=3.00e-04, step_time=1829.7ms, ETA 4d 2h
12/03/2025 05:55:38 - INFO - training.fm_trainer - Step 16890/210000 (8.04%): loss=4.5887, lr=3.00e-04, step_time=1818.1ms, ETA 4d 2h
12/03/2025 05:55:57 - INFO - training.fm_trainer - Step 16900/210000 (8.05%): loss=4.1614, lr=3.00e-04, step_time=1845.8ms, ETA 4d 2h
12/03/2025 05:55:57 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:56:23 - INFO - training.fm_trainer - Eval Step 16900: loss=3.2454, ppl=25.67
12/03/2025 05:56:41 - INFO - training.fm_trainer - Step 16910/210000 (8.05%): loss=5.3531, lr=3.00e-04, step_time=1836.1ms, ETA 4d 2h
12/03/2025 05:56:59 - INFO - training.fm_trainer - Step 16920/210000 (8.06%): loss=2.3801, lr=3.00e-04, step_time=1838.3ms, ETA 4d 2h
12/03/2025 05:57:18 - INFO - training.fm_trainer - Step 16930/210000 (8.06%): loss=0.9366, lr=3.00e-04, step_time=1805.5ms, ETA 4d 2h
12/03/2025 05:57:37 - INFO - training.fm_trainer - Step 16940/210000 (8.07%): loss=8.0499, lr=3.00e-04, step_time=1845.3ms, ETA 4d 2h
12/03/2025 05:57:55 - INFO - training.fm_trainer - Step 16950/210000 (8.07%): loss=1.4619, lr=3.00e-04, step_time=1864.0ms, ETA 4d 2h
12/03/2025 05:57:55 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 05:58:22 - INFO - training.fm_trainer - Eval Step 16950: loss=3.2390, ppl=25.51
12/03/2025 05:58:40 - INFO - training.fm_trainer - Step 16960/210000 (8.08%): loss=2.2695, lr=3.00e-04, grad_norm=0.01, step_time=1869.2ms, ETA 4d 2h
12/03/2025 05:58:58 - INFO - training.fm_trainer - Step 16970/210000 (8.08%): loss=1.5734, lr=3.00e-04, step_time=1803.5ms, ETA 4d 2h
12/03/2025 05:59:17 - INFO - training.fm_trainer - Step 16980/210000 (8.09%): loss=0.9023, lr=3.00e-04, step_time=1811.0ms, ETA 4d 2h
12/03/2025 05:59:35 - INFO - training.fm_trainer - Step 16990/210000 (8.09%): loss=3.7987, lr=3.00e-04, step_time=1909.7ms, ETA 4d 2h
12/03/2025 05:59:54 - INFO - training.fm_trainer - Step 17000/210000 (8.10%): loss=1.0915, lr=3.00e-04, step_time=1835.5ms, ETA 4d 2h
12/03/2025 05:59:54 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:00:20 - INFO - training.fm_trainer - Eval Step 17000: loss=3.2389, ppl=25.51
12/03/2025 06:00:20 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-17000
12/03/2025 06:00:20 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 06:00:20 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-17000/pytorch_model_fsdp_0
12/03/2025 06:00:31 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-17000/pytorch_model_fsdp_0
12/03/2025 06:00:31 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-17000
12/03/2025 06:00:31 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 06:00:34 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-17000/optimizer_0
12/03/2025 06:00:54 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-17000/optimizer_0
12/03/2025 06:00:54 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-17000
12/03/2025 06:00:54 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-17000/scheduler.bin
12/03/2025 06:00:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-17000/sampler.bin
12/03/2025 06:00:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-17000/sampler_1.bin
12/03/2025 06:00:54 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-17000/random_states_0.pkl
12/03/2025 06:00:54 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-17000
12/03/2025 06:00:54 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-16000
12/03/2025 06:01:15 - INFO - training.fm_trainer - Step 17010/210000 (8.10%): loss=3.0144, lr=3.00e-04, step_time=1872.4ms, ETA 4d 2h
12/03/2025 06:01:34 - INFO - training.fm_trainer - Step 17020/210000 (8.10%): loss=4.5443, lr=3.00e-04, step_time=1853.5ms, ETA 4d 2h
12/03/2025 06:01:52 - INFO - training.fm_trainer - Step 17030/210000 (8.11%): loss=0.9675, lr=3.00e-04, step_time=1843.6ms, ETA 4d 2h
12/03/2025 06:02:11 - INFO - training.fm_trainer - Step 17040/210000 (8.11%): loss=1.8683, lr=3.00e-04, step_time=1850.6ms, ETA 4d 2h
12/03/2025 06:02:29 - INFO - training.fm_trainer - Step 17050/210000 (8.12%): loss=3.4919, lr=3.00e-04, step_time=1823.8ms, ETA 4d 2h
12/03/2025 06:02:29 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:02:55 - INFO - training.fm_trainer - Eval Step 17050: loss=3.2221, ppl=25.08
12/03/2025 06:03:13 - INFO - training.fm_trainer - Step 17060/210000 (8.12%): loss=6.6913, lr=3.00e-04, step_time=1823.0ms, ETA 4d 2h
12/03/2025 06:03:32 - INFO - training.fm_trainer - Step 17070/210000 (8.13%): loss=2.0064, lr=3.00e-04, step_time=1808.7ms, ETA 4d 2h
12/03/2025 06:03:50 - INFO - training.fm_trainer - Step 17080/210000 (8.13%): loss=1.7112, lr=3.00e-04, step_time=1823.8ms, ETA 4d 2h
12/03/2025 06:04:08 - INFO - training.fm_trainer - Step 17090/210000 (8.14%): loss=1.0507, lr=3.00e-04, step_time=1851.7ms, ETA 4d 2h
12/03/2025 06:04:30 - INFO - training.fm_trainer - Step 17100/210000 (8.14%): loss=4.3269, lr=3.00e-04, step_time=1816.3ms, ETA 4d 2h
12/03/2025 06:04:30 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:04:56 - INFO - training.fm_trainer - Eval Step 17100: loss=3.2265, ppl=25.19
12/03/2025 06:05:14 - INFO - training.fm_trainer - Step 17110/210000 (8.15%): loss=2.7317, lr=3.00e-04, step_time=1875.8ms, ETA 4d 2h
12/03/2025 06:05:33 - INFO - training.fm_trainer - Step 17120/210000 (8.15%): loss=1.2869, lr=3.00e-04, grad_norm=0.04, step_time=1903.2ms, ETA 4d 2h
12/03/2025 06:05:51 - INFO - training.fm_trainer - Step 17130/210000 (8.16%): loss=1.2727, lr=3.00e-04, step_time=1858.4ms, ETA 4d 2h
12/03/2025 06:06:09 - INFO - training.fm_trainer - Step 17140/210000 (8.16%): loss=1.0983, lr=3.00e-04, step_time=1858.1ms, ETA 4d 3h
12/03/2025 06:06:28 - INFO - training.fm_trainer - Step 17150/210000 (8.17%): loss=0.4064, lr=3.00e-04, step_time=1845.7ms, ETA 4d 3h
12/03/2025 06:06:28 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:06:54 - INFO - training.fm_trainer - Eval Step 17150: loss=3.2487, ppl=25.76
12/03/2025 06:07:13 - INFO - training.fm_trainer - Step 17160/210000 (8.17%): loss=6.4441, lr=3.00e-04, step_time=1822.8ms, ETA 4d 2h
12/03/2025 06:07:32 - INFO - training.fm_trainer - Step 17170/210000 (8.18%): loss=2.3152, lr=3.00e-04, step_time=1812.4ms, ETA 4d 2h
12/03/2025 06:07:50 - INFO - training.fm_trainer - Step 17180/210000 (8.18%): loss=0.7622, lr=3.00e-04, step_time=1823.2ms, ETA 4d 2h
12/03/2025 06:08:08 - INFO - training.fm_trainer - Step 17190/210000 (8.19%): loss=9.0805, lr=3.00e-04, step_time=1819.9ms, ETA 4d 2h
12/03/2025 06:08:27 - INFO - training.fm_trainer - Step 17200/210000 (8.19%): loss=0.4416, lr=3.00e-04, step_time=1948.8ms, ETA 4d 3h
12/03/2025 06:08:27 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:08:53 - INFO - training.fm_trainer - Eval Step 17200: loss=3.3321, ppl=28.00
12/03/2025 06:09:12 - INFO - training.fm_trainer - Step 17210/210000 (8.20%): loss=1.0104, lr=3.00e-04, step_time=1845.5ms, ETA 4d 3h
12/03/2025 06:09:30 - INFO - training.fm_trainer - Step 17220/210000 (8.20%): loss=0.9194, lr=3.00e-04, step_time=1811.2ms, ETA 4d 2h
12/03/2025 06:09:48 - INFO - training.fm_trainer - Step 17230/210000 (8.20%): loss=2.2450, lr=3.00e-04, step_time=1839.5ms, ETA 4d 2h
12/03/2025 06:10:06 - INFO - training.fm_trainer - Step 17240/210000 (8.21%): loss=2.7544, lr=3.00e-04, step_time=1810.7ms, ETA 4d 2h
12/03/2025 06:10:25 - INFO - training.fm_trainer - Step 17250/210000 (8.21%): loss=5.0141, lr=3.00e-04, step_time=1826.0ms, ETA 4d 2h
12/03/2025 06:10:25 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:10:51 - INFO - training.fm_trainer - Eval Step 17250: loss=3.3750, ppl=29.22
12/03/2025 06:11:12 - INFO - training.fm_trainer - Step 17260/210000 (8.22%): loss=1.5014, lr=3.00e-04, step_time=1835.5ms, ETA 4d 2h
12/03/2025 06:11:30 - INFO - training.fm_trainer - Step 17270/210000 (8.22%): loss=0.9911, lr=3.00e-04, step_time=1902.2ms, ETA 4d 2h
12/03/2025 06:11:49 - INFO - training.fm_trainer - Step 17280/210000 (8.23%): loss=0.7993, lr=3.00e-04, grad_norm=0.02, step_time=1949.0ms, ETA 4d 3h
12/03/2025 06:12:07 - INFO - training.fm_trainer - Step 17290/210000 (8.23%): loss=1.0077, lr=3.00e-04, step_time=1822.6ms, ETA 4d 3h
12/03/2025 06:12:26 - INFO - training.fm_trainer - Step 17300/210000 (8.24%): loss=1.4996, lr=3.00e-04, step_time=1833.2ms, ETA 4d 3h
12/03/2025 06:12:26 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:12:51 - INFO - training.fm_trainer - Eval Step 17300: loss=3.3756, ppl=29.24
12/03/2025 06:13:10 - INFO - training.fm_trainer - Step 17310/210000 (8.24%): loss=0.5458, lr=3.00e-04, step_time=1829.1ms, ETA 4d 2h
12/03/2025 06:13:28 - INFO - training.fm_trainer - Step 17320/210000 (8.25%): loss=3.6691, lr=3.00e-04, step_time=1828.6ms, ETA 4d 2h
12/03/2025 06:13:47 - INFO - training.fm_trainer - Step 17330/210000 (8.25%): loss=1.3665, lr=3.00e-04, step_time=1816.5ms, ETA 4d 2h
12/03/2025 06:14:05 - INFO - training.fm_trainer - Step 17340/210000 (8.26%): loss=1.0656, lr=3.00e-04, step_time=1854.9ms, ETA 4d 2h
12/03/2025 06:14:23 - INFO - training.fm_trainer - Step 17350/210000 (8.26%): loss=1.7755, lr=3.00e-04, step_time=1931.1ms, ETA 4d 3h
12/03/2025 06:14:23 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:14:50 - INFO - training.fm_trainer - Eval Step 17350: loss=3.2135, ppl=24.87
12/03/2025 06:15:08 - INFO - training.fm_trainer - Step 17360/210000 (8.27%): loss=0.6528, lr=3.00e-04, step_time=1815.9ms, ETA 4d 2h
12/03/2025 06:15:27 - INFO - training.fm_trainer - Step 17370/210000 (8.27%): loss=1.5331, lr=3.00e-04, step_time=1835.4ms, ETA 4d 2h
12/03/2025 06:15:45 - INFO - training.fm_trainer - Step 17380/210000 (8.28%): loss=4.0941, lr=3.00e-04, step_time=1812.5ms, ETA 4d 2h
12/03/2025 06:16:04 - INFO - training.fm_trainer - Step 17390/210000 (8.28%): loss=1.4048, lr=3.00e-04, step_time=1825.8ms, ETA 4d 2h
12/03/2025 06:16:22 - INFO - training.fm_trainer - Step 17400/210000 (8.29%): loss=1.6142, lr=3.00e-04, step_time=1821.8ms, ETA 4d 2h
12/03/2025 06:16:22 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:16:48 - INFO - training.fm_trainer - Eval Step 17400: loss=3.2053, ppl=24.66
12/03/2025 06:17:06 - INFO - training.fm_trainer - Step 17410/210000 (8.29%): loss=2.5200, lr=3.00e-04, step_time=1848.9ms, ETA 4d 2h
12/03/2025 06:17:24 - INFO - training.fm_trainer - Step 17420/210000 (8.30%): loss=1.4974, lr=3.00e-04, step_time=1817.9ms, ETA 4d 2h
12/03/2025 06:17:43 - INFO - training.fm_trainer - Step 17430/210000 (8.30%): loss=0.9857, lr=3.00e-04, step_time=1842.7ms, ETA 4d 2h
12/03/2025 06:18:02 - INFO - training.fm_trainer - Step 17440/210000 (8.30%): loss=0.8113, lr=3.00e-04, grad_norm=0.02, step_time=1961.0ms, ETA 4d 3h
12/03/2025 06:18:20 - INFO - training.fm_trainer - Step 17450/210000 (8.31%): loss=3.8203, lr=3.00e-04, step_time=1904.4ms, ETA 4d 3h
12/03/2025 06:18:20 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:18:46 - INFO - training.fm_trainer - Eval Step 17450: loss=3.2210, ppl=25.05
12/03/2025 06:19:05 - INFO - training.fm_trainer - Step 17460/210000 (8.31%): loss=0.6765, lr=3.00e-04, step_time=1822.8ms, ETA 4d 3h
12/03/2025 06:19:23 - INFO - training.fm_trainer - Step 17470/210000 (8.32%): loss=1.6658, lr=3.00e-04, step_time=1816.0ms, ETA 4d 2h
12/03/2025 06:19:41 - INFO - training.fm_trainer - Step 17480/210000 (8.32%): loss=4.7493, lr=3.00e-04, step_time=1809.5ms, ETA 4d 2h
12/03/2025 06:20:00 - INFO - training.fm_trainer - Step 17490/210000 (8.33%): loss=2.0711, lr=3.00e-04, step_time=1850.0ms, ETA 4d 2h
12/03/2025 06:20:18 - INFO - training.fm_trainer - Step 17500/210000 (8.33%): loss=1.4727, lr=3.00e-04, step_time=1812.2ms, ETA 4d 2h
12/03/2025 06:20:18 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:20:44 - INFO - training.fm_trainer - Eval Step 17500: loss=3.1759, ppl=23.95
12/03/2025 06:20:44 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-17500
12/03/2025 06:20:44 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 06:20:45 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-17500/pytorch_model_fsdp_0
12/03/2025 06:20:53 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-17500/pytorch_model_fsdp_0
12/03/2025 06:20:53 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-17500
12/03/2025 06:20:53 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 06:20:55 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-17500/optimizer_0
12/03/2025 06:21:11 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-17500/optimizer_0
12/03/2025 06:21:11 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-17500
12/03/2025 06:21:11 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-17500/scheduler.bin
12/03/2025 06:21:11 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-17500/sampler.bin
12/03/2025 06:21:11 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-17500/sampler_1.bin
12/03/2025 06:21:11 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-17500/random_states_0.pkl
12/03/2025 06:21:11 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-17500
12/03/2025 06:21:11 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-16500
12/03/2025 06:21:32 - INFO - training.fm_trainer - Step 17510/210000 (8.34%): loss=1.3686, lr=3.00e-04, step_time=1846.4ms, ETA 4d 2h
12/03/2025 06:21:51 - INFO - training.fm_trainer - Step 17520/210000 (8.34%): loss=1.0548, lr=3.00e-04, step_time=1836.5ms, ETA 4d 2h
12/03/2025 06:22:09 - INFO - training.fm_trainer - Step 17530/210000 (8.35%): loss=0.8220, lr=3.00e-04, step_time=1811.2ms, ETA 4d 2h
12/03/2025 06:22:27 - INFO - training.fm_trainer - Step 17540/210000 (8.35%): loss=0.9110, lr=3.00e-04, step_time=1844.3ms, ETA 4d 2h
12/03/2025 06:22:46 - INFO - training.fm_trainer - Step 17550/210000 (8.36%): loss=3.4144, lr=3.00e-04, step_time=1870.2ms, ETA 4d 2h
12/03/2025 06:22:46 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:23:12 - INFO - training.fm_trainer - Eval Step 17550: loss=3.2355, ppl=25.42
12/03/2025 06:23:31 - INFO - training.fm_trainer - Step 17560/210000 (8.36%): loss=8.2443, lr=3.00e-04, step_time=1833.3ms, ETA 4d 2h
12/03/2025 06:23:49 - INFO - training.fm_trainer - Step 17570/210000 (8.37%): loss=2.1466, lr=3.00e-04, step_time=1841.5ms, ETA 4d 2h
12/03/2025 06:24:07 - INFO - training.fm_trainer - Step 17580/210000 (8.37%): loss=0.7026, lr=3.00e-04, step_time=1816.9ms, ETA 4d 2h
12/03/2025 06:24:26 - INFO - training.fm_trainer - Step 17590/210000 (8.38%): loss=3.3088, lr=3.00e-04, step_time=1923.6ms, ETA 4d 2h
12/03/2025 06:24:44 - INFO - training.fm_trainer - Step 17600/210000 (8.38%): loss=0.7381, lr=3.00e-04, grad_norm=0.01, step_time=1855.2ms, ETA 4d 2h
12/03/2025 06:24:44 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:25:11 - INFO - training.fm_trainer - Eval Step 17600: loss=3.2973, ppl=27.04
12/03/2025 06:25:30 - INFO - training.fm_trainer - Step 17610/210000 (8.39%): loss=2.2959, lr=3.00e-04, step_time=1821.7ms, ETA 4d 2h
12/03/2025 06:25:48 - INFO - training.fm_trainer - Step 17620/210000 (8.39%): loss=1.0064, lr=3.00e-04, step_time=1837.8ms, ETA 4d 2h
12/03/2025 06:26:06 - INFO - training.fm_trainer - Step 17630/210000 (8.40%): loss=0.8870, lr=3.00e-04, step_time=1805.3ms, ETA 4d 2h
12/03/2025 06:26:25 - INFO - training.fm_trainer - Step 17640/210000 (8.40%): loss=3.5666, lr=3.00e-04, step_time=1831.2ms, ETA 4d 2h
12/03/2025 06:26:43 - INFO - training.fm_trainer - Step 17650/210000 (8.40%): loss=0.7387, lr=3.00e-04, step_time=1821.2ms, ETA 4d 2h
12/03/2025 06:26:43 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:27:10 - INFO - training.fm_trainer - Eval Step 17650: loss=3.2953, ppl=26.99
12/03/2025 06:27:28 - INFO - training.fm_trainer - Step 17660/210000 (8.41%): loss=5.5298, lr=3.00e-04, step_time=1829.2ms, ETA 4d 2h
12/03/2025 06:27:47 - INFO - training.fm_trainer - Step 17670/210000 (8.41%): loss=0.5522, lr=3.00e-04, step_time=1901.1ms, ETA 4d 2h
12/03/2025 06:28:05 - INFO - training.fm_trainer - Step 17680/210000 (8.42%): loss=4.7397, lr=3.00e-04, step_time=1833.3ms, ETA 4d 2h
12/03/2025 06:28:23 - INFO - training.fm_trainer - Step 17690/210000 (8.42%): loss=0.9095, lr=3.00e-04, step_time=1810.3ms, ETA 4d 2h
12/03/2025 06:28:42 - INFO - training.fm_trainer - Step 17700/210000 (8.43%): loss=1.9722, lr=3.00e-04, step_time=1813.8ms, ETA 4d 2h
12/03/2025 06:28:42 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:29:08 - INFO - training.fm_trainer - Eval Step 17700: loss=3.2360, ppl=25.43
12/03/2025 06:29:26 - INFO - training.fm_trainer - Step 17710/210000 (8.43%): loss=8.3419, lr=3.00e-04, step_time=1831.9ms, ETA 4d 2h
12/03/2025 06:29:44 - INFO - training.fm_trainer - Step 17720/210000 (8.44%): loss=1.2661, lr=3.00e-04, step_time=1817.2ms, ETA 4d 1h
12/03/2025 06:30:03 - INFO - training.fm_trainer - Step 17730/210000 (8.44%): loss=0.8968, lr=3.00e-04, step_time=1822.2ms, ETA 4d 1h
12/03/2025 06:30:21 - INFO - training.fm_trainer - Step 17740/210000 (8.45%): loss=1.7389, lr=3.00e-04, step_time=1820.7ms, ETA 4d 1h
12/03/2025 06:30:40 - INFO - training.fm_trainer - Step 17750/210000 (8.45%): loss=6.2666, lr=3.00e-04, step_time=1851.3ms, ETA 4d 1h
12/03/2025 06:30:40 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:31:06 - INFO - training.fm_trainer - Eval Step 17750: loss=3.1968, ppl=24.45
12/03/2025 06:31:24 - INFO - training.fm_trainer - Step 17760/210000 (8.46%): loss=1.5198, lr=3.00e-04, grad_norm=0.04, step_time=1822.4ms, ETA 4d 1h
12/03/2025 06:31:42 - INFO - training.fm_trainer - Step 17770/210000 (8.46%): loss=1.4428, lr=3.00e-04, step_time=1816.3ms, ETA 4d 1h
12/03/2025 06:32:01 - INFO - training.fm_trainer - Step 17780/210000 (8.47%): loss=1.0605, lr=3.00e-04, step_time=1949.2ms, ETA 4d 2h
12/03/2025 06:32:19 - INFO - training.fm_trainer - Step 17790/210000 (8.47%): loss=0.7969, lr=3.00e-04, step_time=1862.1ms, ETA 4d 2h
12/03/2025 06:32:38 - INFO - training.fm_trainer - Step 17800/210000 (8.48%): loss=0.4856, lr=3.00e-04, step_time=1809.5ms, ETA 4d 2h
12/03/2025 06:32:38 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:33:04 - INFO - training.fm_trainer - Eval Step 17800: loss=3.1983, ppl=24.49
12/03/2025 06:33:23 - INFO - training.fm_trainer - Step 17810/210000 (8.48%): loss=1.9702, lr=3.00e-04, step_time=1855.8ms, ETA 4d 2h
12/03/2025 06:33:41 - INFO - training.fm_trainer - Step 17820/210000 (8.49%): loss=1.0251, lr=3.00e-04, step_time=1978.8ms, ETA 4d 3h
12/03/2025 06:33:59 - INFO - training.fm_trainer - Step 17830/210000 (8.49%): loss=0.7143, lr=3.00e-04, step_time=1839.5ms, ETA 4d 3h
12/03/2025 06:34:18 - INFO - training.fm_trainer - Step 17840/210000 (8.50%): loss=0.9702, lr=3.00e-04, step_time=1907.4ms, ETA 4d 3h
12/03/2025 06:34:36 - INFO - training.fm_trainer - Step 17850/210000 (8.50%): loss=1.1530, lr=3.00e-04, step_time=1815.9ms, ETA 4d 3h
12/03/2025 06:34:36 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:35:02 - INFO - training.fm_trainer - Eval Step 17850: loss=3.1894, ppl=24.27
12/03/2025 06:35:20 - INFO - training.fm_trainer - Step 17860/210000 (8.50%): loss=1.0464, lr=3.00e-04, step_time=1811.7ms, ETA 4d 2h
12/03/2025 06:35:39 - INFO - training.fm_trainer - Step 17870/210000 (8.51%): loss=1.2402, lr=3.00e-04, step_time=1821.8ms, ETA 4d 2h
12/03/2025 06:35:57 - INFO - training.fm_trainer - Step 17880/210000 (8.51%): loss=4.0106, lr=3.00e-04, step_time=1809.1ms, ETA 4d 2h
12/03/2025 06:36:16 - INFO - training.fm_trainer - Step 17890/210000 (8.52%): loss=2.6916, lr=3.00e-04, step_time=1845.8ms, ETA 4d 2h
12/03/2025 06:36:34 - INFO - training.fm_trainer - Step 17900/210000 (8.52%): loss=7.4690, lr=3.00e-04, step_time=1928.5ms, ETA 4d 2h
12/03/2025 06:36:34 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:37:00 - INFO - training.fm_trainer - Eval Step 17900: loss=3.2132, ppl=24.86
12/03/2025 06:37:19 - INFO - training.fm_trainer - Step 17910/210000 (8.53%): loss=5.4146, lr=3.00e-04, step_time=1851.3ms, ETA 4d 2h
12/03/2025 06:37:37 - INFO - training.fm_trainer - Step 17920/210000 (8.53%): loss=1.2914, lr=3.00e-04, grad_norm=0.06, step_time=1870.2ms, ETA 4d 2h
12/03/2025 06:37:55 - INFO - training.fm_trainer - Step 17930/210000 (8.54%): loss=6.1192, lr=3.00e-04, step_time=1833.1ms, ETA 4d 2h
12/03/2025 06:38:14 - INFO - training.fm_trainer - Step 17940/210000 (8.54%): loss=7.2009, lr=3.00e-04, step_time=1856.5ms, ETA 4d 2h
12/03/2025 06:38:32 - INFO - training.fm_trainer - Step 17950/210000 (8.55%): loss=1.5317, lr=3.00e-04, step_time=1862.5ms, ETA 4d 2h
12/03/2025 06:38:32 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:38:58 - INFO - training.fm_trainer - Eval Step 17950: loss=3.2446, ppl=25.65
12/03/2025 06:39:17 - INFO - training.fm_trainer - Step 17960/210000 (8.55%): loss=3.4978, lr=3.00e-04, step_time=1836.8ms, ETA 4d 2h
12/03/2025 06:39:35 - INFO - training.fm_trainer - Step 17970/210000 (8.56%): loss=4.1397, lr=3.00e-04, step_time=1850.3ms, ETA 4d 2h
12/03/2025 06:39:53 - INFO - training.fm_trainer - Step 17980/210000 (8.56%): loss=0.9194, lr=3.00e-04, step_time=1821.4ms, ETA 4d 2h
12/03/2025 06:40:12 - INFO - training.fm_trainer - Step 17990/210000 (8.57%): loss=7.2980, lr=3.00e-04, step_time=1907.3ms, ETA 4d 2h
12/03/2025 06:40:30 - INFO - training.fm_trainer - Step 18000/210000 (8.57%): loss=2.9811, lr=3.00e-04, step_time=1812.8ms, ETA 4d 2h
12/03/2025 06:40:30 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:40:56 - INFO - training.fm_trainer - Eval Step 18000: loss=3.2134, ppl=24.86
12/03/2025 06:40:56 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-18000
12/03/2025 06:40:56 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 06:40:57 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-18000/pytorch_model_fsdp_0
12/03/2025 06:41:06 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-18000/pytorch_model_fsdp_0
12/03/2025 06:41:06 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-18000
12/03/2025 06:41:06 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 06:41:09 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-18000/optimizer_0
12/03/2025 06:41:25 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-18000/optimizer_0
12/03/2025 06:41:25 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-18000
12/03/2025 06:41:25 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-18000/scheduler.bin
12/03/2025 06:41:25 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-18000/sampler.bin
12/03/2025 06:41:25 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-18000/sampler_1.bin
12/03/2025 06:41:25 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-18000/random_states_0.pkl
12/03/2025 06:41:25 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-18000
12/03/2025 06:41:25 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-17000
12/03/2025 06:41:46 - INFO - training.fm_trainer - Step 18010/210000 (8.58%): loss=0.5773, lr=3.00e-04, step_time=1837.3ms, ETA 4d 2h
12/03/2025 06:42:04 - INFO - training.fm_trainer - Step 18020/210000 (8.58%): loss=2.2363, lr=3.00e-04, step_time=1801.0ms, ETA 4d 2h
12/03/2025 06:42:23 - INFO - training.fm_trainer - Step 18030/210000 (8.59%): loss=4.6472, lr=3.00e-04, step_time=1825.5ms, ETA 4d 2h
12/03/2025 06:42:41 - INFO - training.fm_trainer - Step 18040/210000 (8.59%): loss=3.6034, lr=3.00e-04, step_time=1827.7ms, ETA 4d 2h
12/03/2025 06:42:59 - INFO - training.fm_trainer - Step 18050/210000 (8.60%): loss=1.9850, lr=3.00e-04, step_time=1813.9ms, ETA 4d 2h
12/03/2025 06:42:59 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:43:26 - INFO - training.fm_trainer - Eval Step 18050: loss=3.3148, ppl=27.52
12/03/2025 06:43:44 - INFO - training.fm_trainer - Step 18060/210000 (8.60%): loss=7.4746, lr=3.00e-04, step_time=1854.7ms, ETA 4d 2h
12/03/2025 06:44:03 - INFO - training.fm_trainer - Step 18070/210000 (8.60%): loss=3.0802, lr=3.00e-04, step_time=1821.1ms, ETA 4d 1h
12/03/2025 06:44:21 - INFO - training.fm_trainer - Step 18080/210000 (8.61%): loss=1.5870, lr=3.00e-04, grad_norm=0.01, step_time=1846.6ms, ETA 4d 2h
12/03/2025 06:44:40 - INFO - training.fm_trainer - Step 18090/210000 (8.61%): loss=3.7318, lr=3.00e-04, step_time=1841.7ms, ETA 4d 2h
12/03/2025 06:44:58 - INFO - training.fm_trainer - Step 18100/210000 (8.62%): loss=2.2613, lr=3.00e-04, step_time=1846.6ms, ETA 4d 2h
12/03/2025 06:44:58 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:45:24 - INFO - training.fm_trainer - Eval Step 18100: loss=3.3464, ppl=28.40
12/03/2025 06:45:42 - INFO - training.fm_trainer - Step 18110/210000 (8.62%): loss=1.2306, lr=3.00e-04, step_time=1836.1ms, ETA 4d 2h
12/03/2025 06:46:01 - INFO - training.fm_trainer - Step 18120/210000 (8.63%): loss=9.4796, lr=3.00e-04, step_time=1844.0ms, ETA 4d 2h
12/03/2025 06:46:19 - INFO - training.fm_trainer - Step 18130/210000 (8.63%): loss=4.8469, lr=3.00e-04, step_time=1824.6ms, ETA 4d 1h
12/03/2025 06:46:37 - INFO - training.fm_trainer - Step 18140/210000 (8.64%): loss=4.9860, lr=3.00e-04, step_time=1829.0ms, ETA 4d 1h
12/03/2025 06:46:56 - INFO - training.fm_trainer - Step 18150/210000 (8.64%): loss=0.5589, lr=3.00e-04, step_time=1808.6ms, ETA 4d 1h
12/03/2025 06:46:56 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:47:22 - INFO - training.fm_trainer - Eval Step 18150: loss=3.3389, ppl=28.19
12/03/2025 06:47:40 - INFO - training.fm_trainer - Step 18160/210000 (8.65%): loss=1.2581, lr=3.00e-04, step_time=1834.6ms, ETA 4d 1h
12/03/2025 06:47:59 - INFO - training.fm_trainer - Step 18170/210000 (8.65%): loss=0.9563, lr=3.00e-04, step_time=1802.0ms, ETA 4d 1h
12/03/2025 06:48:17 - INFO - training.fm_trainer - Step 18180/210000 (8.66%): loss=8.8856, lr=3.00e-04, step_time=1826.6ms, ETA 4d 1h
12/03/2025 06:48:35 - INFO - training.fm_trainer - Step 18190/210000 (8.66%): loss=1.0283, lr=3.00e-04, step_time=1839.0ms, ETA 4d 1h
12/03/2025 06:48:54 - INFO - training.fm_trainer - Step 18200/210000 (8.67%): loss=0.7198, lr=3.00e-04, step_time=1819.0ms, ETA 4d 1h
12/03/2025 06:48:54 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:49:20 - INFO - training.fm_trainer - Eval Step 18200: loss=3.2782, ppl=26.53
12/03/2025 06:49:38 - INFO - training.fm_trainer - Step 18210/210000 (8.67%): loss=0.6315, lr=3.00e-04, step_time=1847.7ms, ETA 4d 1h
12/03/2025 06:49:57 - INFO - training.fm_trainer - Step 18220/210000 (8.68%): loss=1.3004, lr=3.00e-04, step_time=1832.9ms, ETA 4d 1h
12/03/2025 06:50:15 - INFO - training.fm_trainer - Step 18230/210000 (8.68%): loss=1.7103, lr=3.00e-04, step_time=1815.1ms, ETA 4d 1h
12/03/2025 06:50:34 - INFO - training.fm_trainer - Step 18240/210000 (8.69%): loss=3.9945, lr=3.00e-04, grad_norm=0.90, step_time=1856.4ms, ETA 4d 1h
12/03/2025 06:50:52 - INFO - training.fm_trainer - Step 18250/210000 (8.69%): loss=0.9886, lr=3.00e-04, step_time=1820.0ms, ETA 4d 1h
12/03/2025 06:50:52 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:51:18 - INFO - training.fm_trainer - Eval Step 18250: loss=3.2184, ppl=24.99
12/03/2025 06:51:36 - INFO - training.fm_trainer - Step 18260/210000 (8.70%): loss=0.8036, lr=3.00e-04, step_time=1809.7ms, ETA 4d 1h
12/03/2025 06:51:55 - INFO - training.fm_trainer - Step 18270/210000 (8.70%): loss=0.9598, lr=3.00e-04, step_time=1832.0ms, ETA 4d 1h
12/03/2025 06:52:13 - INFO - training.fm_trainer - Step 18280/210000 (8.70%): loss=1.8704, lr=3.00e-04, step_time=1828.9ms, ETA 4d 1h
12/03/2025 06:52:31 - INFO - training.fm_trainer - Step 18290/210000 (8.71%): loss=9.5342, lr=3.00e-04, step_time=1851.0ms, ETA 4d 1h
12/03/2025 06:52:50 - INFO - training.fm_trainer - Step 18300/210000 (8.71%): loss=2.0542, lr=3.00e-04, step_time=1824.9ms, ETA 4d 1h
12/03/2025 06:52:50 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:53:16 - INFO - training.fm_trainer - Eval Step 18300: loss=3.2546, ppl=25.91
12/03/2025 06:53:34 - INFO - training.fm_trainer - Step 18310/210000 (8.72%): loss=0.8574, lr=3.00e-04, step_time=1810.5ms, ETA 4d 1h
12/03/2025 06:53:52 - INFO - training.fm_trainer - Step 18320/210000 (8.72%): loss=1.5964, lr=3.00e-04, step_time=1820.0ms, ETA 4d 1h
12/03/2025 06:54:11 - INFO - training.fm_trainer - Step 18330/210000 (8.73%): loss=1.5118, lr=3.00e-04, step_time=1812.7ms, ETA 4d 1h
12/03/2025 06:54:29 - INFO - training.fm_trainer - Step 18340/210000 (8.73%): loss=2.5856, lr=3.00e-04, step_time=1832.6ms, ETA 4d 1h
12/03/2025 06:54:47 - INFO - training.fm_trainer - Step 18350/210000 (8.74%): loss=1.1359, lr=3.00e-04, step_time=1829.7ms, ETA 4d 1h
12/03/2025 06:54:47 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:55:13 - INFO - training.fm_trainer - Eval Step 18350: loss=3.2592, ppl=26.03
12/03/2025 06:55:32 - INFO - training.fm_trainer - Step 18360/210000 (8.74%): loss=1.1405, lr=3.00e-04, step_time=1807.7ms, ETA 4d 1h
12/03/2025 06:55:50 - INFO - training.fm_trainer - Step 18370/210000 (8.75%): loss=2.6904, lr=3.00e-04, step_time=1842.9ms, ETA 4d 1h
12/03/2025 06:56:08 - INFO - training.fm_trainer - Step 18380/210000 (8.75%): loss=0.9710, lr=3.00e-04, step_time=1809.6ms, ETA 4d 1h
12/03/2025 06:56:27 - INFO - training.fm_trainer - Step 18390/210000 (8.76%): loss=1.2576, lr=3.00e-04, step_time=1822.4ms, ETA 4d 1h
12/03/2025 06:56:45 - INFO - training.fm_trainer - Step 18400/210000 (8.76%): loss=2.4183, lr=3.00e-04, grad_norm=0.04, step_time=1856.2ms, ETA 4d 1h
12/03/2025 06:56:45 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:57:11 - INFO - training.fm_trainer - Eval Step 18400: loss=3.2640, ppl=26.15
12/03/2025 06:57:29 - INFO - training.fm_trainer - Step 18410/210000 (8.77%): loss=1.8559, lr=3.00e-04, step_time=1819.0ms, ETA 4d 1h
12/03/2025 06:57:48 - INFO - training.fm_trainer - Step 18420/210000 (8.77%): loss=4.1754, lr=3.00e-04, step_time=1896.1ms, ETA 4d 1h
12/03/2025 06:58:06 - INFO - training.fm_trainer - Step 18430/210000 (8.78%): loss=1.0571, lr=3.00e-04, step_time=1851.9ms, ETA 4d 1h
12/03/2025 06:58:25 - INFO - training.fm_trainer - Step 18440/210000 (8.78%): loss=0.6302, lr=3.00e-04, step_time=1837.3ms, ETA 4d 1h
12/03/2025 06:58:44 - INFO - training.fm_trainer - Step 18450/210000 (8.79%): loss=1.6548, lr=3.00e-04, step_time=1847.2ms, ETA 4d 1h
12/03/2025 06:58:44 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 06:59:10 - INFO - training.fm_trainer - Eval Step 18450: loss=3.2600, ppl=26.05
12/03/2025 06:59:29 - INFO - training.fm_trainer - Step 18460/210000 (8.79%): loss=2.7011, lr=3.00e-04, step_time=1834.5ms, ETA 4d 1h
12/03/2025 06:59:47 - INFO - training.fm_trainer - Step 18470/210000 (8.80%): loss=5.3525, lr=3.00e-04, step_time=1818.9ms, ETA 4d 1h
12/03/2025 07:00:05 - INFO - training.fm_trainer - Step 18480/210000 (8.80%): loss=1.1155, lr=3.00e-04, step_time=1833.0ms, ETA 4d 1h
12/03/2025 07:00:24 - INFO - training.fm_trainer - Step 18490/210000 (8.80%): loss=1.2232, lr=3.00e-04, step_time=1811.0ms, ETA 4d 1h
12/03/2025 07:00:42 - INFO - training.fm_trainer - Step 18500/210000 (8.81%): loss=0.3777, lr=3.00e-04, step_time=1835.8ms, ETA 4d 1h
12/03/2025 07:00:42 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:01:08 - INFO - training.fm_trainer - Eval Step 18500: loss=3.2946, ppl=26.97
12/03/2025 07:01:08 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-18500
12/03/2025 07:01:08 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 07:01:09 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-18500/pytorch_model_fsdp_0
12/03/2025 07:01:17 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-18500/pytorch_model_fsdp_0
12/03/2025 07:01:17 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-18500
12/03/2025 07:01:17 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 07:01:20 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-18500/optimizer_0
12/03/2025 07:01:36 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-18500/optimizer_0
12/03/2025 07:01:36 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-18500
12/03/2025 07:01:36 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-18500/scheduler.bin
12/03/2025 07:01:36 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-18500/sampler.bin
12/03/2025 07:01:36 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-18500/sampler_1.bin
12/03/2025 07:01:36 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-18500/random_states_0.pkl
12/03/2025 07:01:36 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-18500
12/03/2025 07:01:36 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-17500
12/03/2025 07:01:57 - INFO - training.fm_trainer - Step 18510/210000 (8.81%): loss=0.6359, lr=3.00e-04, step_time=1840.0ms, ETA 4d 1h
12/03/2025 07:02:16 - INFO - training.fm_trainer - Step 18520/210000 (8.82%): loss=0.9092, lr=3.00e-04, step_time=1812.6ms, ETA 4d 1h
12/03/2025 07:02:34 - INFO - training.fm_trainer - Step 18530/210000 (8.82%): loss=2.8379, lr=3.00e-04, step_time=1880.0ms, ETA 4d 1h
12/03/2025 07:02:53 - INFO - training.fm_trainer - Step 18540/210000 (8.83%): loss=2.0833, lr=3.00e-04, step_time=1833.5ms, ETA 4d 1h
12/03/2025 07:03:11 - INFO - training.fm_trainer - Step 18550/210000 (8.83%): loss=0.4958, lr=3.00e-04, step_time=1833.7ms, ETA 4d 1h
12/03/2025 07:03:11 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:03:39 - INFO - training.fm_trainer - Eval Step 18550: loss=3.3341, ppl=28.05
12/03/2025 07:03:57 - INFO - training.fm_trainer - Step 18560/210000 (8.84%): loss=1.1186, lr=3.00e-04, grad_norm=0.03, step_time=1829.9ms, ETA 4d 1h
12/03/2025 07:04:16 - INFO - training.fm_trainer - Step 18570/210000 (8.84%): loss=1.5790, lr=3.00e-04, step_time=1833.7ms, ETA 4d 1h
12/03/2025 07:04:34 - INFO - training.fm_trainer - Step 18580/210000 (8.85%): loss=0.9213, lr=3.00e-04, step_time=1838.4ms, ETA 4d 1h
12/03/2025 07:04:52 - INFO - training.fm_trainer - Step 18590/210000 (8.85%): loss=3.4439, lr=3.00e-04, step_time=1821.6ms, ETA 4d 1h
12/03/2025 07:05:11 - INFO - training.fm_trainer - Step 18600/210000 (8.86%): loss=0.6107, lr=3.00e-04, step_time=1830.6ms, ETA 4d 1h
12/03/2025 07:05:11 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:05:37 - INFO - training.fm_trainer - Eval Step 18600: loss=3.3742, ppl=29.20
12/03/2025 07:05:55 - INFO - training.fm_trainer - Step 18610/210000 (8.86%): loss=1.0518, lr=3.00e-04, step_time=1829.9ms, ETA 4d 1h
12/03/2025 07:06:14 - INFO - training.fm_trainer - Step 18620/210000 (8.87%): loss=3.5853, lr=3.00e-04, step_time=1804.8ms, ETA 4d 1h
12/03/2025 07:06:32 - INFO - training.fm_trainer - Step 18630/210000 (8.87%): loss=0.8619, lr=3.00e-04, step_time=1829.3ms, ETA 4d 1h
12/03/2025 07:06:50 - INFO - training.fm_trainer - Step 18640/210000 (8.88%): loss=5.5735, lr=3.00e-04, step_time=1815.0ms, ETA 4d 1h
12/03/2025 07:07:09 - INFO - training.fm_trainer - Step 18650/210000 (8.88%): loss=1.4496, lr=3.00e-04, step_time=1819.0ms, ETA 4d 1h
12/03/2025 07:07:09 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:07:35 - INFO - training.fm_trainer - Eval Step 18650: loss=3.3409, ppl=28.25
12/03/2025 07:07:54 - INFO - training.fm_trainer - Step 18660/210000 (8.89%): loss=0.9802, lr=3.00e-04, step_time=1862.8ms, ETA 4d 1h
12/03/2025 07:08:12 - INFO - training.fm_trainer - Step 18670/210000 (8.89%): loss=2.4680, lr=3.00e-04, step_time=1849.5ms, ETA 4d 1h
12/03/2025 07:08:31 - INFO - training.fm_trainer - Step 18680/210000 (8.90%): loss=4.1747, lr=3.00e-04, step_time=1852.6ms, ETA 4d 1h
12/03/2025 07:08:49 - INFO - training.fm_trainer - Step 18690/210000 (8.90%): loss=1.7646, lr=3.00e-04, step_time=1845.2ms, ETA 4d 1h
12/03/2025 07:09:08 - INFO - training.fm_trainer - Step 18700/210000 (8.90%): loss=0.8502, lr=3.00e-04, step_time=1835.5ms, ETA 4d 1h
12/03/2025 07:09:08 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:09:34 - INFO - training.fm_trainer - Eval Step 18700: loss=3.2067, ppl=24.70
12/03/2025 07:09:53 - INFO - training.fm_trainer - Step 18710/210000 (8.91%): loss=2.6665, lr=3.00e-04, step_time=1828.4ms, ETA 4d 1h
12/03/2025 07:10:11 - INFO - training.fm_trainer - Step 18720/210000 (8.91%): loss=5.7755, lr=3.00e-04, grad_norm=0.02, step_time=1973.1ms, ETA 4d 2h
12/03/2025 07:10:30 - INFO - training.fm_trainer - Step 18730/210000 (8.92%): loss=2.0378, lr=3.00e-04, step_time=1853.6ms, ETA 4d 2h
12/03/2025 07:10:49 - INFO - training.fm_trainer - Step 18740/210000 (8.92%): loss=7.6432, lr=3.00e-04, step_time=1832.7ms, ETA 4d 2h
12/03/2025 07:11:07 - INFO - training.fm_trainer - Step 18750/210000 (8.93%): loss=4.1784, lr=3.00e-04, step_time=1867.0ms, ETA 4d 2h
12/03/2025 07:11:07 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:11:35 - INFO - training.fm_trainer - Eval Step 18750: loss=3.3545, ppl=28.63
12/03/2025 07:11:53 - INFO - training.fm_trainer - Step 18760/210000 (8.93%): loss=6.3005, lr=3.00e-04, step_time=1829.5ms, ETA 4d 2h
12/03/2025 07:12:12 - INFO - training.fm_trainer - Step 18770/210000 (8.94%): loss=5.8662, lr=3.00e-04, step_time=1817.4ms, ETA 4d 1h
12/03/2025 07:12:30 - INFO - training.fm_trainer - Step 18780/210000 (8.94%): loss=1.5629, lr=3.00e-04, step_time=1872.7ms, ETA 4d 2h
12/03/2025 07:12:49 - INFO - training.fm_trainer - Step 18790/210000 (8.95%): loss=4.0137, lr=3.00e-04, step_time=1839.8ms, ETA 4d 2h
12/03/2025 07:13:07 - INFO - training.fm_trainer - Step 18800/210000 (8.95%): loss=1.5341, lr=3.00e-04, step_time=2053.2ms, ETA 4d 3h
12/03/2025 07:13:07 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:13:34 - INFO - training.fm_trainer - Eval Step 18800: loss=3.4344, ppl=31.01
12/03/2025 07:13:52 - INFO - training.fm_trainer - Step 18810/210000 (8.96%): loss=1.6866, lr=3.00e-04, step_time=1824.0ms, ETA 4d 2h
12/03/2025 07:14:10 - INFO - training.fm_trainer - Step 18820/210000 (8.96%): loss=1.9106, lr=3.00e-04, step_time=1822.4ms, ETA 4d 2h
12/03/2025 07:14:29 - INFO - training.fm_trainer - Step 18830/210000 (8.97%): loss=3.0413, lr=3.00e-04, step_time=1831.3ms, ETA 4d 2h
12/03/2025 07:14:47 - INFO - training.fm_trainer - Step 18840/210000 (8.97%): loss=4.6765, lr=3.00e-04, step_time=1836.6ms, ETA 4d 2h
12/03/2025 07:15:06 - INFO - training.fm_trainer - Step 18850/210000 (8.98%): loss=2.6435, lr=3.00e-04, step_time=1842.5ms, ETA 4d 2h
12/03/2025 07:15:06 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:15:32 - INFO - training.fm_trainer - Eval Step 18850: loss=3.3947, ppl=29.81
12/03/2025 07:15:51 - INFO - training.fm_trainer - Step 18860/210000 (8.98%): loss=2.4873, lr=3.00e-04, step_time=1838.7ms, ETA 4d 2h
12/03/2025 07:16:09 - INFO - training.fm_trainer - Step 18870/210000 (8.99%): loss=5.8012, lr=3.00e-04, step_time=1839.1ms, ETA 4d 2h
12/03/2025 07:16:28 - INFO - training.fm_trainer - Step 18880/210000 (8.99%): loss=2.0565, lr=3.00e-04, grad_norm=0.06, step_time=1875.7ms, ETA 4d 2h
12/03/2025 07:16:46 - INFO - training.fm_trainer - Step 18890/210000 (9.00%): loss=3.9472, lr=3.00e-04, step_time=1828.3ms, ETA 4d 2h
12/03/2025 07:17:05 - INFO - training.fm_trainer - Step 18900/210000 (9.00%): loss=4.7742, lr=3.00e-04, step_time=1844.2ms, ETA 4d 2h
12/03/2025 07:17:05 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:17:31 - INFO - training.fm_trainer - Eval Step 18900: loss=3.3300, ppl=27.94
12/03/2025 07:17:49 - INFO - training.fm_trainer - Step 18910/210000 (9.00%): loss=2.0259, lr=3.00e-04, step_time=1815.3ms, ETA 4d 2h
12/03/2025 07:18:08 - INFO - training.fm_trainer - Step 18920/210000 (9.01%): loss=0.7678, lr=3.00e-04, step_time=1834.0ms, ETA 4d 1h
12/03/2025 07:18:26 - INFO - training.fm_trainer - Step 18930/210000 (9.01%): loss=1.4081, lr=3.00e-04, step_time=1929.0ms, ETA 4d 2h
12/03/2025 07:18:45 - INFO - training.fm_trainer - Step 18940/210000 (9.02%): loss=2.0711, lr=3.00e-04, step_time=1972.8ms, ETA 4d 3h
12/03/2025 07:19:03 - INFO - training.fm_trainer - Step 18950/210000 (9.02%): loss=0.4770, lr=3.00e-04, step_time=1812.4ms, ETA 4d 2h
12/03/2025 07:19:03 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:19:29 - INFO - training.fm_trainer - Eval Step 18950: loss=3.3394, ppl=28.20
12/03/2025 07:19:48 - INFO - training.fm_trainer - Step 18960/210000 (9.03%): loss=2.5853, lr=3.00e-04, step_time=1832.1ms, ETA 4d 2h
12/03/2025 07:20:07 - INFO - training.fm_trainer - Step 18970/210000 (9.03%): loss=4.2451, lr=3.00e-04, step_time=1859.9ms, ETA 4d 2h
12/03/2025 07:20:25 - INFO - training.fm_trainer - Step 18980/210000 (9.04%): loss=0.4796, lr=3.00e-04, step_time=1855.7ms, ETA 4d 2h
12/03/2025 07:20:44 - INFO - training.fm_trainer - Step 18990/210000 (9.04%): loss=3.3451, lr=3.00e-04, step_time=1812.8ms, ETA 4d 2h
12/03/2025 07:21:02 - INFO - training.fm_trainer - Step 19000/210000 (9.05%): loss=2.4530, lr=3.00e-04, step_time=1827.4ms, ETA 4d 2h
12/03/2025 07:21:02 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:21:30 - INFO - training.fm_trainer - Eval Step 19000: loss=3.4390, ppl=31.15
12/03/2025 07:21:30 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-19000
12/03/2025 07:21:30 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 07:21:32 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-19000/pytorch_model_fsdp_0
12/03/2025 07:21:39 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-19000/pytorch_model_fsdp_0
12/03/2025 07:21:39 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-19000
12/03/2025 07:21:39 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 07:21:43 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-19000/optimizer_0
12/03/2025 07:21:59 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-19000/optimizer_0
12/03/2025 07:21:59 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-19000
12/03/2025 07:21:59 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-19000/scheduler.bin
12/03/2025 07:21:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-19000/sampler.bin
12/03/2025 07:21:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-19000/sampler_1.bin
12/03/2025 07:21:59 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-19000/random_states_0.pkl
12/03/2025 07:21:59 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-19000
12/03/2025 07:21:59 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-18000
12/03/2025 07:22:21 - INFO - training.fm_trainer - Step 19010/210000 (9.05%): loss=2.5245, lr=3.00e-04, step_time=1909.0ms, ETA 4d 2h
12/03/2025 07:22:39 - INFO - training.fm_trainer - Step 19020/210000 (9.06%): loss=9.7414, lr=3.00e-04, step_time=1856.9ms, ETA 4d 2h
12/03/2025 07:22:58 - INFO - training.fm_trainer - Step 19030/210000 (9.06%): loss=0.9390, lr=3.00e-04, step_time=1812.4ms, ETA 4d 2h
12/03/2025 07:23:16 - INFO - training.fm_trainer - Step 19040/210000 (9.07%): loss=1.7520, lr=3.00e-04, grad_norm=0.04, step_time=1862.3ms, ETA 4d 2h
12/03/2025 07:23:34 - INFO - training.fm_trainer - Step 19050/210000 (9.07%): loss=4.9508, lr=3.00e-04, step_time=1819.5ms, ETA 4d 2h
12/03/2025 07:23:34 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:24:02 - INFO - training.fm_trainer - Eval Step 19050: loss=3.5419, ppl=34.53
12/03/2025 07:24:21 - INFO - training.fm_trainer - Step 19060/210000 (9.08%): loss=3.2761, lr=3.00e-04, step_time=1829.2ms, ETA 4d 1h
12/03/2025 07:24:39 - INFO - training.fm_trainer - Step 19070/210000 (9.08%): loss=1.6511, lr=3.00e-04, step_time=1997.8ms, ETA 4d 2h
12/03/2025 07:24:58 - INFO - training.fm_trainer - Step 19080/210000 (9.09%): loss=1.2593, lr=3.00e-04, step_time=1834.8ms, ETA 4d 2h
12/03/2025 07:25:16 - INFO - training.fm_trainer - Step 19090/210000 (9.09%): loss=0.3791, lr=3.00e-04, step_time=1825.1ms, ETA 4d 2h
12/03/2025 07:25:34 - INFO - training.fm_trainer - Step 19100/210000 (9.10%): loss=1.3528, lr=3.00e-04, step_time=1804.6ms, ETA 4d 2h
12/03/2025 07:25:34 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:26:00 - INFO - training.fm_trainer - Eval Step 19100: loss=3.5553, ppl=35.00
12/03/2025 07:26:19 - INFO - training.fm_trainer - Step 19110/210000 (9.10%): loss=0.9093, lr=3.00e-04, step_time=1828.5ms, ETA 4d 2h
12/03/2025 07:26:37 - INFO - training.fm_trainer - Step 19120/210000 (9.10%): loss=5.2912, lr=3.00e-04, step_time=1822.7ms, ETA 4d 1h
12/03/2025 07:26:55 - INFO - training.fm_trainer - Step 19130/210000 (9.11%): loss=2.8955, lr=3.00e-04, step_time=1854.5ms, ETA 4d 1h
12/03/2025 07:27:14 - INFO - training.fm_trainer - Step 19140/210000 (9.11%): loss=1.1475, lr=3.00e-04, step_time=1820.8ms, ETA 4d 1h
12/03/2025 07:27:32 - INFO - training.fm_trainer - Step 19150/210000 (9.12%): loss=1.0589, lr=3.00e-04, step_time=1865.8ms, ETA 4d 1h
12/03/2025 07:27:32 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:28:00 - INFO - training.fm_trainer - Eval Step 19150: loss=3.3796, ppl=29.36
12/03/2025 07:28:19 - INFO - training.fm_trainer - Step 19160/210000 (9.12%): loss=0.3995, lr=3.00e-04, step_time=1880.5ms, ETA 4d 2h
12/03/2025 07:28:37 - INFO - training.fm_trainer - Step 19170/210000 (9.13%): loss=1.7987, lr=3.00e-04, step_time=1844.0ms, ETA 4d 2h
12/03/2025 07:28:55 - INFO - training.fm_trainer - Step 19180/210000 (9.13%): loss=0.7517, lr=3.00e-04, step_time=1868.9ms, ETA 4d 2h
12/03/2025 07:29:14 - INFO - training.fm_trainer - Step 19190/210000 (9.14%): loss=3.9187, lr=3.00e-04, step_time=1824.2ms, ETA 4d 1h
12/03/2025 07:29:32 - INFO - training.fm_trainer - Step 19200/210000 (9.14%): loss=0.4523, lr=3.00e-04, grad_norm=0.01, step_time=1844.1ms, ETA 4d 1h
12/03/2025 07:29:32 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:30:00 - INFO - training.fm_trainer - Eval Step 19200: loss=3.2322, ppl=25.34
12/03/2025 07:30:19 - INFO - training.fm_trainer - Step 19210/210000 (9.15%): loss=0.5923, lr=3.00e-04, step_time=1836.9ms, ETA 4d 1h
12/03/2025 07:30:37 - INFO - training.fm_trainer - Step 19220/210000 (9.15%): loss=1.2121, lr=3.00e-04, step_time=1890.0ms, ETA 4d 2h
12/03/2025 07:30:56 - INFO - training.fm_trainer - Step 19230/210000 (9.16%): loss=1.4927, lr=3.00e-04, step_time=1836.4ms, ETA 4d 2h
12/03/2025 07:31:14 - INFO - training.fm_trainer - Step 19240/210000 (9.16%): loss=0.7987, lr=3.00e-04, step_time=1825.9ms, ETA 4d 1h
12/03/2025 07:31:32 - INFO - training.fm_trainer - Step 19250/210000 (9.17%): loss=1.8177, lr=3.00e-04, step_time=1811.5ms, ETA 4d 1h
12/03/2025 07:31:32 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:31:59 - INFO - training.fm_trainer - Eval Step 19250: loss=3.2037, ppl=24.62
12/03/2025 07:32:18 - INFO - training.fm_trainer - Step 19260/210000 (9.17%): loss=2.1646, lr=3.00e-04, step_time=1823.8ms, ETA 4d 1h
12/03/2025 07:32:36 - INFO - training.fm_trainer - Step 19270/210000 (9.18%): loss=4.6969, lr=3.00e-04, step_time=1827.7ms, ETA 4d 1h
12/03/2025 07:32:54 - INFO - training.fm_trainer - Step 19280/210000 (9.18%): loss=2.6750, lr=3.00e-04, step_time=1816.8ms, ETA 4d 1h
12/03/2025 07:33:13 - INFO - training.fm_trainer - Step 19290/210000 (9.19%): loss=1.0956, lr=3.00e-04, step_time=1819.2ms, ETA 4d 1h
12/03/2025 07:33:32 - INFO - training.fm_trainer - Step 19300/210000 (9.19%): loss=9.6249, lr=3.00e-04, step_time=1898.2ms, ETA 4d 1h
12/03/2025 07:33:32 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:33:59 - INFO - training.fm_trainer - Eval Step 19300: loss=3.1953, ppl=24.42
12/03/2025 07:34:18 - INFO - training.fm_trainer - Step 19310/210000 (9.20%): loss=5.1354, lr=3.00e-04, step_time=1850.1ms, ETA 4d 1h
12/03/2025 07:34:37 - INFO - training.fm_trainer - Step 19320/210000 (9.20%): loss=1.1573, lr=3.00e-04, step_time=1964.2ms, ETA 4d 2h
12/03/2025 07:34:55 - INFO - training.fm_trainer - Step 19330/210000 (9.20%): loss=2.6905, lr=3.00e-04, step_time=1824.9ms, ETA 4d 2h
12/03/2025 07:35:14 - INFO - training.fm_trainer - Step 19340/210000 (9.21%): loss=1.1110, lr=3.00e-04, step_time=1805.5ms, ETA 4d 1h
12/03/2025 07:35:32 - INFO - training.fm_trainer - Step 19350/210000 (9.21%): loss=8.5856, lr=3.00e-04, step_time=1853.8ms, ETA 4d 1h
12/03/2025 07:35:32 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:35:58 - INFO - training.fm_trainer - Eval Step 19350: loss=3.2038, ppl=24.63
12/03/2025 07:36:16 - INFO - training.fm_trainer - Step 19360/210000 (9.22%): loss=1.5155, lr=3.00e-04, grad_norm=0.03, step_time=1922.9ms, ETA 4d 2h
12/03/2025 07:36:35 - INFO - training.fm_trainer - Step 19370/210000 (9.22%): loss=0.5548, lr=3.00e-04, step_time=1823.0ms, ETA 4d 2h
12/03/2025 07:36:53 - INFO - training.fm_trainer - Step 19380/210000 (9.23%): loss=9.2225, lr=3.00e-04, step_time=1835.8ms, ETA 4d 1h
12/03/2025 07:37:12 - INFO - training.fm_trainer - Step 19390/210000 (9.23%): loss=0.5972, lr=3.00e-04, step_time=1827.2ms, ETA 4d 1h
12/03/2025 07:37:30 - INFO - training.fm_trainer - Step 19400/210000 (9.24%): loss=0.5859, lr=3.00e-04, step_time=1817.3ms, ETA 4d 1h
12/03/2025 07:37:30 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:37:56 - INFO - training.fm_trainer - Eval Step 19400: loss=3.2709, ppl=26.34
12/03/2025 07:38:14 - INFO - training.fm_trainer - Step 19410/210000 (9.24%): loss=2.3916, lr=3.00e-04, step_time=1832.7ms, ETA 4d 1h
12/03/2025 07:38:33 - INFO - training.fm_trainer - Step 19420/210000 (9.25%): loss=4.3731, lr=3.00e-04, step_time=1844.3ms, ETA 4d 1h
12/03/2025 07:38:51 - INFO - training.fm_trainer - Step 19430/210000 (9.25%): loss=0.4095, lr=3.00e-04, step_time=1828.8ms, ETA 4d 1h
12/03/2025 07:39:10 - INFO - training.fm_trainer - Step 19440/210000 (9.26%): loss=1.0840, lr=3.00e-04, step_time=1836.6ms, ETA 4d 1h
12/03/2025 07:39:28 - INFO - training.fm_trainer - Step 19450/210000 (9.26%): loss=8.7702, lr=3.00e-04, step_time=1870.8ms, ETA 4d 1h
12/03/2025 07:39:28 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:39:54 - INFO - training.fm_trainer - Eval Step 19450: loss=3.3525, ppl=28.58
12/03/2025 07:40:13 - INFO - training.fm_trainer - Step 19460/210000 (9.27%): loss=9.7091, lr=3.00e-04, step_time=1825.7ms, ETA 4d 1h
12/03/2025 07:40:31 - INFO - training.fm_trainer - Step 19470/210000 (9.27%): loss=0.5947, lr=3.00e-04, step_time=1850.3ms, ETA 4d 1h
12/03/2025 07:40:50 - INFO - training.fm_trainer - Step 19480/210000 (9.28%): loss=0.9986, lr=3.00e-04, step_time=1805.5ms, ETA 4d 1h
12/03/2025 07:41:08 - INFO - training.fm_trainer - Step 19490/210000 (9.28%): loss=4.3940, lr=3.00e-04, step_time=1836.3ms, ETA 4d 1h
12/03/2025 07:41:26 - INFO - training.fm_trainer - Step 19500/210000 (9.29%): loss=1.0146, lr=3.00e-04, step_time=1834.4ms, ETA 4d 1h
12/03/2025 07:41:26 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:41:52 - INFO - training.fm_trainer - Eval Step 19500: loss=3.3660, ppl=28.96
12/03/2025 07:41:52 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-19500
12/03/2025 07:41:52 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 07:41:53 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-19500/pytorch_model_fsdp_0
12/03/2025 07:42:01 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-19500/pytorch_model_fsdp_0
12/03/2025 07:42:01 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-19500
12/03/2025 07:42:01 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 07:42:03 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-19500/optimizer_0
12/03/2025 07:42:20 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-19500/optimizer_0
12/03/2025 07:42:20 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-19500
12/03/2025 07:42:20 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-19500/scheduler.bin
12/03/2025 07:42:20 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-19500/sampler.bin
12/03/2025 07:42:20 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-19500/sampler_1.bin
12/03/2025 07:42:20 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-19500/random_states_0.pkl
12/03/2025 07:42:20 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-19500
12/03/2025 07:42:20 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-18500
12/03/2025 07:42:40 - INFO - training.fm_trainer - Step 19510/210000 (9.29%): loss=2.3275, lr=3.00e-04, step_time=1841.5ms, ETA 4d 1h
12/03/2025 07:42:59 - INFO - training.fm_trainer - Step 19520/210000 (9.30%): loss=1.2539, lr=3.00e-04, grad_norm=0.02, step_time=1884.0ms, ETA 4d 1h
12/03/2025 07:43:17 - INFO - training.fm_trainer - Step 19530/210000 (9.30%): loss=4.3545, lr=3.00e-04, step_time=1874.6ms, ETA 4d 1h
12/03/2025 07:43:36 - INFO - training.fm_trainer - Step 19540/210000 (9.30%): loss=2.9642, lr=3.00e-04, step_time=1872.6ms, ETA 4d 1h
12/03/2025 07:43:54 - INFO - training.fm_trainer - Step 19550/210000 (9.31%): loss=2.1134, lr=3.00e-04, step_time=1845.3ms, ETA 4d 1h
12/03/2025 07:43:54 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:44:20 - INFO - training.fm_trainer - Eval Step 19550: loss=3.3147, ppl=27.51
12/03/2025 07:44:39 - INFO - training.fm_trainer - Step 19560/210000 (9.31%): loss=9.7660, lr=3.00e-04, step_time=1821.1ms, ETA 4d 1h
12/03/2025 07:44:57 - INFO - training.fm_trainer - Step 19570/210000 (9.32%): loss=1.5330, lr=3.00e-04, step_time=2080.8ms, ETA 4d 2h
12/03/2025 07:45:16 - INFO - training.fm_trainer - Step 19580/210000 (9.32%): loss=2.8602, lr=3.00e-04, step_time=1880.7ms, ETA 4d 2h
12/03/2025 07:45:35 - INFO - training.fm_trainer - Step 19590/210000 (9.33%): loss=8.5208, lr=3.00e-04, step_time=1831.1ms, ETA 4d 2h
12/03/2025 07:45:53 - INFO - training.fm_trainer - Step 19600/210000 (9.33%): loss=8.8229, lr=3.00e-04, step_time=1825.5ms, ETA 4d 2h
12/03/2025 07:45:53 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:46:20 - INFO - training.fm_trainer - Eval Step 19600: loss=3.2405, ppl=25.55
12/03/2025 07:46:40 - INFO - training.fm_trainer - Step 19610/210000 (9.34%): loss=4.0324, lr=3.00e-04, step_time=1830.1ms, ETA 4d 2h
12/03/2025 07:46:58 - INFO - training.fm_trainer - Step 19620/210000 (9.34%): loss=3.0638, lr=3.00e-04, step_time=1881.1ms, ETA 4d 2h
12/03/2025 07:47:17 - INFO - training.fm_trainer - Step 19630/210000 (9.35%): loss=2.0208, lr=3.00e-04, step_time=1850.1ms, ETA 4d 2h
12/03/2025 07:47:36 - INFO - training.fm_trainer - Step 19640/210000 (9.35%): loss=1.8633, lr=3.00e-04, step_time=1819.4ms, ETA 4d 2h
12/03/2025 07:47:54 - INFO - training.fm_trainer - Step 19650/210000 (9.36%): loss=1.3789, lr=3.00e-04, step_time=1818.7ms, ETA 4d 1h
12/03/2025 07:47:54 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:48:20 - INFO - training.fm_trainer - Eval Step 19650: loss=3.1896, ppl=24.28
12/03/2025 07:48:39 - INFO - training.fm_trainer - Step 19660/210000 (9.36%): loss=0.9307, lr=3.00e-04, step_time=1850.9ms, ETA 4d 1h
12/03/2025 07:48:57 - INFO - training.fm_trainer - Step 19670/210000 (9.37%): loss=2.3753, lr=3.00e-04, step_time=1827.7ms, ETA 4d 1h
12/03/2025 07:49:16 - INFO - training.fm_trainer - Step 19680/210000 (9.37%): loss=3.1788, lr=3.00e-04, grad_norm=0.02, step_time=1851.4ms, ETA 4d 1h
12/03/2025 07:49:34 - INFO - training.fm_trainer - Step 19690/210000 (9.38%): loss=1.0258, lr=3.00e-04, step_time=1823.4ms, ETA 4d 1h
12/03/2025 07:49:52 - INFO - training.fm_trainer - Step 19700/210000 (9.38%): loss=1.8120, lr=3.00e-04, step_time=1829.5ms, ETA 4d 1h
12/03/2025 07:49:52 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:50:18 - INFO - training.fm_trainer - Eval Step 19700: loss=3.1694, ppl=23.79
12/03/2025 07:50:36 - INFO - training.fm_trainer - Step 19710/210000 (9.39%): loss=2.4581, lr=3.00e-04, step_time=1819.7ms, ETA 4d 1h
12/03/2025 07:50:55 - INFO - training.fm_trainer - Step 19720/210000 (9.39%): loss=5.3384, lr=3.00e-04, step_time=1842.7ms, ETA 4d 1h
12/03/2025 07:51:14 - INFO - training.fm_trainer - Step 19730/210000 (9.40%): loss=0.9764, lr=3.00e-04, step_time=1869.9ms, ETA 4d 1h
12/03/2025 07:51:32 - INFO - training.fm_trainer - Step 19740/210000 (9.40%): loss=8.3561, lr=3.00e-04, step_time=1841.0ms, ETA 4d 1h
12/03/2025 07:51:51 - INFO - training.fm_trainer - Step 19750/210000 (9.40%): loss=1.2560, lr=3.00e-04, step_time=1818.3ms, ETA 4d 1h
12/03/2025 07:51:51 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:52:16 - INFO - training.fm_trainer - Eval Step 19750: loss=3.1340, ppl=22.97
12/03/2025 07:52:35 - INFO - training.fm_trainer - Step 19760/210000 (9.41%): loss=3.3093, lr=3.00e-04, step_time=1834.1ms, ETA 4d 1h
12/03/2025 07:52:53 - INFO - training.fm_trainer - Step 19770/210000 (9.41%): loss=2.3701, lr=3.00e-04, step_time=1812.5ms, ETA 4d 1h
12/03/2025 07:53:11 - INFO - training.fm_trainer - Step 19780/210000 (9.42%): loss=0.9581, lr=3.00e-04, step_time=1802.2ms, ETA 4d 57m
12/03/2025 07:53:30 - INFO - training.fm_trainer - Step 19790/210000 (9.42%): loss=2.1407, lr=3.00e-04, step_time=1851.6ms, ETA 4d 1h
12/03/2025 07:53:48 - INFO - training.fm_trainer - Step 19800/210000 (9.43%): loss=1.4023, lr=3.00e-04, step_time=2038.4ms, ETA 4d 2h
12/03/2025 07:53:48 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:54:14 - INFO - training.fm_trainer - Eval Step 19800: loss=3.1026, ppl=22.25
12/03/2025 07:54:33 - INFO - training.fm_trainer - Step 19810/210000 (9.43%): loss=0.8300, lr=3.00e-04, step_time=1871.5ms, ETA 4d 2h
12/03/2025 07:54:51 - INFO - training.fm_trainer - Step 19820/210000 (9.44%): loss=1.0539, lr=3.00e-04, step_time=1838.6ms, ETA 4d 2h
12/03/2025 07:55:09 - INFO - training.fm_trainer - Step 19830/210000 (9.44%): loss=5.4726, lr=3.00e-04, step_time=1845.9ms, ETA 4d 2h
12/03/2025 07:55:28 - INFO - training.fm_trainer - Step 19840/210000 (9.45%): loss=6.9255, lr=3.00e-04, grad_norm=0.15, step_time=1870.8ms, ETA 4d 2h
12/03/2025 07:55:46 - INFO - training.fm_trainer - Step 19850/210000 (9.45%): loss=1.8151, lr=3.00e-04, step_time=1825.3ms, ETA 4d 1h
12/03/2025 07:55:46 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:56:12 - INFO - training.fm_trainer - Eval Step 19850: loss=3.0225, ppl=20.54
12/03/2025 07:56:31 - INFO - training.fm_trainer - Step 19860/210000 (9.46%): loss=4.1206, lr=3.00e-04, step_time=1829.9ms, ETA 4d 1h
12/03/2025 07:56:49 - INFO - training.fm_trainer - Step 19870/210000 (9.46%): loss=1.1430, lr=3.00e-04, step_time=1821.8ms, ETA 4d 1h
12/03/2025 07:57:07 - INFO - training.fm_trainer - Step 19880/210000 (9.47%): loss=1.1313, lr=3.00e-04, step_time=1837.8ms, ETA 4d 1h
12/03/2025 07:57:26 - INFO - training.fm_trainer - Step 19890/210000 (9.47%): loss=1.6664, lr=3.00e-04, step_time=1854.2ms, ETA 4d 1h
12/03/2025 07:57:44 - INFO - training.fm_trainer - Step 19900/210000 (9.48%): loss=2.1144, lr=3.00e-04, step_time=1820.8ms, ETA 4d 1h
12/03/2025 07:57:44 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 07:58:10 - INFO - training.fm_trainer - Eval Step 19900: loss=2.9424, ppl=18.96
12/03/2025 07:58:28 - INFO - training.fm_trainer - Step 19910/210000 (9.48%): loss=6.7479, lr=3.00e-04, step_time=2066.3ms, ETA 4d 2h
12/03/2025 07:58:47 - INFO - training.fm_trainer - Step 19920/210000 (9.49%): loss=4.0223, lr=3.00e-04, step_time=1821.7ms, ETA 4d 2h
12/03/2025 07:59:05 - INFO - training.fm_trainer - Step 19930/210000 (9.49%): loss=1.0711, lr=3.00e-04, step_time=2000.1ms, ETA 4d 3h
12/03/2025 07:59:24 - INFO - training.fm_trainer - Step 19940/210000 (9.50%): loss=1.3248, lr=3.00e-04, step_time=1834.9ms, ETA 4d 2h
12/03/2025 07:59:42 - INFO - training.fm_trainer - Step 19950/210000 (9.50%): loss=5.9926, lr=3.00e-04, step_time=1856.4ms, ETA 4d 2h
12/03/2025 07:59:42 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:00:10 - INFO - training.fm_trainer - Eval Step 19950: loss=6.2855, ppl=536.72
12/03/2025 08:00:28 - INFO - training.fm_trainer - Step 19960/210000 (9.50%): loss=0.3893, lr=3.00e-04, step_time=1859.8ms, ETA 4d 2h
12/03/2025 08:00:47 - INFO - training.fm_trainer - Step 19970/210000 (9.51%): loss=2.8834, lr=3.00e-04, step_time=1814.2ms, ETA 4d 2h
12/03/2025 08:01:05 - INFO - training.fm_trainer - Step 19980/210000 (9.51%): loss=3.6973, lr=3.00e-04, step_time=1811.7ms, ETA 4d 2h
12/03/2025 08:01:24 - INFO - training.fm_trainer - Step 19990/210000 (9.52%): loss=8.1348, lr=3.00e-04, step_time=1832.1ms, ETA 4d 1h
12/03/2025 08:01:42 - INFO - training.fm_trainer - Step 20000/210000 (9.52%): loss=2.3141, lr=3.00e-04, grad_norm=0.53, step_time=1886.0ms, ETA 4d 2h
12/03/2025 08:01:42 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:02:10 - INFO - training.fm_trainer - Eval Step 20000: loss=2.8642, ppl=17.53
12/03/2025 08:02:10 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-20000
12/03/2025 08:02:10 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 08:02:11 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-20000/pytorch_model_fsdp_0
12/03/2025 08:02:21 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-20000/pytorch_model_fsdp_0
12/03/2025 08:02:21 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-20000
12/03/2025 08:02:21 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 08:02:24 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-20000/optimizer_0
12/03/2025 08:02:40 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-20000/optimizer_0
12/03/2025 08:02:40 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-20000
12/03/2025 08:02:40 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-20000/scheduler.bin
12/03/2025 08:02:40 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-20000/sampler.bin
12/03/2025 08:02:40 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-20000/sampler_1.bin
12/03/2025 08:02:40 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-20000/random_states_0.pkl
12/03/2025 08:02:40 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-20000
12/03/2025 08:02:40 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-19000
12/03/2025 08:03:01 - INFO - training.fm_trainer - Step 20010/210000 (9.53%): loss=1.2348, lr=3.00e-04, step_time=1842.4ms, ETA 4d 2h
12/03/2025 08:03:19 - INFO - training.fm_trainer - Step 20020/210000 (9.53%): loss=0.8997, lr=3.00e-04, step_time=1848.5ms, ETA 4d 1h
12/03/2025 08:03:38 - INFO - training.fm_trainer - Step 20030/210000 (9.54%): loss=1.7364, lr=3.00e-04, step_time=1813.7ms, ETA 4d 1h
12/03/2025 08:03:56 - INFO - training.fm_trainer - Step 20040/210000 (9.54%): loss=0.9696, lr=3.00e-04, step_time=1808.8ms, ETA 4d 1h
12/03/2025 08:04:15 - INFO - training.fm_trainer - Step 20050/210000 (9.55%): loss=0.7815, lr=3.00e-04, step_time=1806.2ms, ETA 4d 1h
12/03/2025 08:04:15 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:04:41 - INFO - training.fm_trainer - Eval Step 20050: loss=2.8427, ppl=17.16
12/03/2025 08:05:00 - INFO - training.fm_trainer - Step 20060/210000 (9.55%): loss=1.6354, lr=3.00e-04, step_time=1812.3ms, ETA 4d 1h
12/03/2025 08:05:18 - INFO - training.fm_trainer - Step 20070/210000 (9.56%): loss=1.2941, lr=3.00e-04, step_time=1823.7ms, ETA 4d 1h
12/03/2025 08:05:37 - INFO - training.fm_trainer - Step 20080/210000 (9.56%): loss=3.8101, lr=3.00e-04, step_time=1861.6ms, ETA 4d 1h
12/03/2025 08:05:55 - INFO - training.fm_trainer - Step 20090/210000 (9.57%): loss=1.3023, lr=3.00e-04, step_time=1827.7ms, ETA 4d 1h
12/03/2025 08:06:14 - INFO - training.fm_trainer - Step 20100/210000 (9.57%): loss=7.8755, lr=3.00e-04, step_time=1865.0ms, ETA 4d 1h
12/03/2025 08:06:14 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:06:42 - INFO - training.fm_trainer - Eval Step 20100: loss=2.8212, ppl=16.80
12/03/2025 08:07:00 - INFO - training.fm_trainer - Step 20110/210000 (9.58%): loss=0.6333, lr=3.00e-04, step_time=1832.9ms, ETA 4d 1h
12/03/2025 08:07:19 - INFO - training.fm_trainer - Step 20120/210000 (9.58%): loss=3.4141, lr=3.00e-04, step_time=1863.6ms, ETA 4d 1h
12/03/2025 08:07:38 - INFO - training.fm_trainer - Step 20130/210000 (9.59%): loss=9.5311, lr=3.00e-04, step_time=1930.2ms, ETA 4d 1h
12/03/2025 08:07:57 - INFO - training.fm_trainer - Step 20140/210000 (9.59%): loss=5.7846, lr=3.00e-04, step_time=1868.2ms, ETA 4d 1h
12/03/2025 08:08:16 - INFO - training.fm_trainer - Step 20150/210000 (9.60%): loss=3.4647, lr=3.00e-04, step_time=1887.2ms, ETA 4d 1h
12/03/2025 08:08:16 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:08:44 - INFO - training.fm_trainer - Eval Step 20150: loss=2.7774, ppl=16.08
12/03/2025 08:09:02 - INFO - training.fm_trainer - Step 20160/210000 (9.60%): loss=1.3445, lr=3.00e-04, grad_norm=0.06, step_time=1889.4ms, ETA 4d 2h
12/03/2025 08:09:21 - INFO - training.fm_trainer - Step 20170/210000 (9.60%): loss=2.2005, lr=3.00e-04, step_time=1864.7ms, ETA 4d 2h
12/03/2025 08:09:40 - INFO - training.fm_trainer - Step 20180/210000 (9.61%): loss=0.2190, lr=3.00e-04, step_time=1902.7ms, ETA 4d 2h
12/03/2025 08:09:58 - INFO - training.fm_trainer - Step 20190/210000 (9.61%): loss=2.3696, lr=3.00e-04, step_time=1880.7ms, ETA 4d 2h
12/03/2025 08:10:17 - INFO - training.fm_trainer - Step 20200/210000 (9.62%): loss=7.9402, lr=3.00e-04, step_time=1878.0ms, ETA 4d 2h
12/03/2025 08:10:17 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:10:45 - INFO - training.fm_trainer - Eval Step 20200: loss=2.6794, ppl=14.58
12/03/2025 08:11:04 - INFO - training.fm_trainer - Step 20210/210000 (9.62%): loss=0.9212, lr=3.00e-04, step_time=1822.4ms, ETA 4d 2h
12/03/2025 08:11:22 - INFO - training.fm_trainer - Step 20220/210000 (9.63%): loss=0.5099, lr=3.00e-04, step_time=1931.0ms, ETA 4d 2h
12/03/2025 08:11:41 - INFO - training.fm_trainer - Step 20230/210000 (9.63%): loss=0.5277, lr=3.00e-04, step_time=1910.6ms, ETA 4d 2h
12/03/2025 08:12:00 - INFO - training.fm_trainer - Step 20240/210000 (9.64%): loss=0.5460, lr=3.00e-04, step_time=1860.2ms, ETA 4d 2h
12/03/2025 08:12:18 - INFO - training.fm_trainer - Step 20250/210000 (9.64%): loss=3.4573, lr=3.00e-04, step_time=1892.1ms, ETA 4d 2h
12/03/2025 08:12:18 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:12:46 - INFO - training.fm_trainer - Eval Step 20250: loss=2.6871, ppl=14.69
12/03/2025 08:13:05 - INFO - training.fm_trainer - Step 20260/210000 (9.65%): loss=0.7847, lr=3.00e-04, step_time=1843.8ms, ETA 4d 2h
12/03/2025 08:13:24 - INFO - training.fm_trainer - Step 20270/210000 (9.65%): loss=3.8892, lr=3.00e-04, step_time=1923.5ms, ETA 4d 2h
12/03/2025 08:13:42 - INFO - training.fm_trainer - Step 20280/210000 (9.66%): loss=1.0561, lr=3.00e-04, step_time=1897.3ms, ETA 4d 3h
12/03/2025 08:14:01 - INFO - training.fm_trainer - Step 20290/210000 (9.66%): loss=0.7842, lr=3.00e-04, step_time=1902.3ms, ETA 4d 3h
12/03/2025 08:14:20 - INFO - training.fm_trainer - Step 20300/210000 (9.67%): loss=3.5759, lr=3.00e-04, step_time=1896.5ms, ETA 4d 3h
12/03/2025 08:14:20 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:14:47 - INFO - training.fm_trainer - Eval Step 20300: loss=2.6598, ppl=14.29
12/03/2025 08:15:06 - INFO - training.fm_trainer - Step 20310/210000 (9.67%): loss=7.5075, lr=3.00e-04, step_time=1883.9ms, ETA 4d 3h
12/03/2025 08:15:25 - INFO - training.fm_trainer - Step 20320/210000 (9.68%): loss=0.9363, lr=3.00e-04, grad_norm=0.04, step_time=1834.9ms, ETA 4d 2h
12/03/2025 08:15:44 - INFO - training.fm_trainer - Step 20330/210000 (9.68%): loss=0.6271, lr=3.00e-04, step_time=1816.6ms, ETA 4d 2h
12/03/2025 08:16:02 - INFO - training.fm_trainer - Step 20340/210000 (9.69%): loss=1.5792, lr=3.00e-04, step_time=1826.4ms, ETA 4d 2h
12/03/2025 08:16:21 - INFO - training.fm_trainer - Step 20350/210000 (9.69%): loss=0.2664, lr=3.00e-04, step_time=1886.0ms, ETA 4d 2h
12/03/2025 08:16:21 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:16:48 - INFO - training.fm_trainer - Eval Step 20350: loss=2.6457, ppl=14.09
12/03/2025 08:17:07 - INFO - training.fm_trainer - Step 20360/210000 (9.70%): loss=0.7706, lr=3.00e-04, step_time=1875.8ms, ETA 4d 2h
12/03/2025 08:17:26 - INFO - training.fm_trainer - Step 20370/210000 (9.70%): loss=0.7026, lr=3.00e-04, step_time=1905.2ms, ETA 4d 2h
12/03/2025 08:17:44 - INFO - training.fm_trainer - Step 20380/210000 (9.70%): loss=0.3520, lr=3.00e-04, step_time=1873.4ms, ETA 4d 2h
12/03/2025 08:18:03 - INFO - training.fm_trainer - Step 20390/210000 (9.71%): loss=3.5942, lr=3.00e-04, step_time=1865.9ms, ETA 4d 2h
12/03/2025 08:18:22 - INFO - training.fm_trainer - Step 20400/210000 (9.71%): loss=0.3000, lr=3.00e-04, step_time=1860.4ms, ETA 4d 2h
12/03/2025 08:18:22 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:18:50 - INFO - training.fm_trainer - Eval Step 20400: loss=2.6413, ppl=14.03
12/03/2025 08:19:09 - INFO - training.fm_trainer - Step 20410/210000 (9.72%): loss=1.0196, lr=3.00e-04, step_time=1830.7ms, ETA 4d 2h
12/03/2025 08:19:28 - INFO - training.fm_trainer - Step 20420/210000 (9.72%): loss=4.1743, lr=3.00e-04, step_time=1930.7ms, ETA 4d 2h
12/03/2025 08:19:46 - INFO - training.fm_trainer - Step 20430/210000 (9.73%): loss=0.8287, lr=3.00e-04, step_time=1845.5ms, ETA 4d 2h
12/03/2025 08:20:05 - INFO - training.fm_trainer - Step 20440/210000 (9.73%): loss=5.8925, lr=3.00e-04, step_time=1869.8ms, ETA 4d 2h
12/03/2025 08:20:24 - INFO - training.fm_trainer - Step 20450/210000 (9.74%): loss=2.5449, lr=3.00e-04, step_time=1851.6ms, ETA 4d 2h
12/03/2025 08:20:24 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:20:51 - INFO - training.fm_trainer - Eval Step 20450: loss=2.6335, ppl=13.92
12/03/2025 08:21:10 - INFO - training.fm_trainer - Step 20460/210000 (9.74%): loss=1.7824, lr=3.00e-04, step_time=1826.4ms, ETA 4d 2h
12/03/2025 08:21:29 - INFO - training.fm_trainer - Step 20470/210000 (9.75%): loss=1.2644, lr=3.00e-04, step_time=1844.6ms, ETA 4d 2h
12/03/2025 08:21:47 - INFO - training.fm_trainer - Step 20480/210000 (9.75%): loss=3.8654, lr=3.00e-04, grad_norm=0.03, step_time=1857.2ms, ETA 4d 2h
12/03/2025 08:22:06 - INFO - training.fm_trainer - Step 20490/210000 (9.76%): loss=1.5020, lr=3.00e-04, step_time=1852.7ms, ETA 4d 1h
12/03/2025 08:22:25 - INFO - training.fm_trainer - Step 20500/210000 (9.76%): loss=4.6731, lr=3.00e-04, step_time=1838.6ms, ETA 4d 1h
12/03/2025 08:22:25 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:22:51 - INFO - training.fm_trainer - Eval Step 20500: loss=2.6339, ppl=13.93
12/03/2025 08:22:51 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-20500
12/03/2025 08:22:51 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 08:22:52 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-20500/pytorch_model_fsdp_0
12/03/2025 08:23:00 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-20500/pytorch_model_fsdp_0
12/03/2025 08:23:00 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-20500
12/03/2025 08:23:00 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 08:23:03 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-20500/optimizer_0
12/03/2025 08:23:18 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-20500/optimizer_0
12/03/2025 08:23:19 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-20500
12/03/2025 08:23:19 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-20500/scheduler.bin
12/03/2025 08:23:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-20500/sampler.bin
12/03/2025 08:23:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-20500/sampler_1.bin
12/03/2025 08:23:19 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-20500/random_states_0.pkl
12/03/2025 08:23:19 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-20500
12/03/2025 08:23:19 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-19500
12/03/2025 08:23:40 - INFO - training.fm_trainer - Step 20510/210000 (9.77%): loss=0.6762, lr=3.00e-04, step_time=1880.7ms, ETA 4d 1h
12/03/2025 08:23:59 - INFO - training.fm_trainer - Step 20520/210000 (9.77%): loss=0.2717, lr=3.00e-04, step_time=1831.7ms, ETA 4d 1h
12/03/2025 08:24:17 - INFO - training.fm_trainer - Step 20530/210000 (9.78%): loss=3.6260, lr=3.00e-04, step_time=1876.7ms, ETA 4d 1h
12/03/2025 08:24:37 - INFO - training.fm_trainer - Step 20540/210000 (9.78%): loss=8.1271, lr=3.00e-04, step_time=1823.2ms, ETA 4d 1h
12/03/2025 08:24:56 - INFO - training.fm_trainer - Step 20550/210000 (9.79%): loss=1.2778, lr=3.00e-04, step_time=1904.4ms, ETA 4d 1h
12/03/2025 08:24:56 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:25:23 - INFO - training.fm_trainer - Eval Step 20550: loss=2.6432, ppl=14.06
12/03/2025 08:25:42 - INFO - training.fm_trainer - Step 20560/210000 (9.79%): loss=1.5428, lr=3.00e-04, step_time=1876.7ms, ETA 4d 2h
12/03/2025 08:26:00 - INFO - training.fm_trainer - Step 20570/210000 (9.80%): loss=0.4555, lr=3.00e-04, step_time=1829.7ms, ETA 4d 1h
12/03/2025 08:26:19 - INFO - training.fm_trainer - Step 20580/210000 (9.80%): loss=0.4813, lr=3.00e-04, step_time=1833.3ms, ETA 4d 1h
12/03/2025 08:26:37 - INFO - training.fm_trainer - Step 20590/210000 (9.80%): loss=3.0303, lr=3.00e-04, step_time=1847.4ms, ETA 4d 1h
12/03/2025 08:26:56 - INFO - training.fm_trainer - Step 20600/210000 (9.81%): loss=0.7845, lr=3.00e-04, step_time=1839.3ms, ETA 4d 1h
12/03/2025 08:26:56 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:27:22 - INFO - training.fm_trainer - Eval Step 20600: loss=2.6251, ppl=13.81
12/03/2025 08:27:41 - INFO - training.fm_trainer - Step 20610/210000 (9.81%): loss=1.0507, lr=3.00e-04, step_time=1847.5ms, ETA 4d 1h
12/03/2025 08:27:59 - INFO - training.fm_trainer - Step 20620/210000 (9.82%): loss=7.2383, lr=3.00e-04, step_time=1863.5ms, ETA 4d 1h
12/03/2025 08:28:18 - INFO - training.fm_trainer - Step 20630/210000 (9.82%): loss=0.6587, lr=3.00e-04, step_time=1842.6ms, ETA 4d 1h
12/03/2025 08:28:36 - INFO - training.fm_trainer - Step 20640/210000 (9.83%): loss=1.3311, lr=3.00e-04, grad_norm=0.03, step_time=1876.3ms, ETA 4d 1h
12/03/2025 08:28:55 - INFO - training.fm_trainer - Step 20650/210000 (9.83%): loss=0.8627, lr=3.00e-04, step_time=1898.4ms, ETA 4d 1h
12/03/2025 08:28:55 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:29:23 - INFO - training.fm_trainer - Eval Step 20650: loss=2.6198, ppl=13.73
12/03/2025 08:29:41 - INFO - training.fm_trainer - Step 20660/210000 (9.84%): loss=0.7392, lr=3.00e-04, step_time=1896.4ms, ETA 4d 2h
12/03/2025 08:30:00 - INFO - training.fm_trainer - Step 20670/210000 (9.84%): loss=0.7978, lr=3.00e-04, step_time=1946.4ms, ETA 4d 2h
12/03/2025 08:30:18 - INFO - training.fm_trainer - Step 20680/210000 (9.85%): loss=1.1436, lr=3.00e-04, step_time=1873.0ms, ETA 4d 2h
12/03/2025 08:30:37 - INFO - training.fm_trainer - Step 20690/210000 (9.85%): loss=4.9684, lr=3.00e-04, step_time=1831.3ms, ETA 4d 2h
12/03/2025 08:30:56 - INFO - training.fm_trainer - Step 20700/210000 (9.86%): loss=0.9799, lr=3.00e-04, step_time=1973.4ms, ETA 4d 2h
12/03/2025 08:30:56 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:31:23 - INFO - training.fm_trainer - Eval Step 20700: loss=2.6432, ppl=14.06
12/03/2025 08:31:41 - INFO - training.fm_trainer - Step 20710/210000 (9.86%): loss=0.2559, lr=3.00e-04, step_time=1858.2ms, ETA 4d 2h
12/03/2025 08:32:00 - INFO - training.fm_trainer - Step 20720/210000 (9.87%): loss=6.4828, lr=3.00e-04, step_time=1835.2ms, ETA 4d 2h
12/03/2025 08:32:19 - INFO - training.fm_trainer - Step 20730/210000 (9.87%): loss=0.4988, lr=3.00e-04, step_time=2038.8ms, ETA 4d 3h
12/03/2025 08:32:37 - INFO - training.fm_trainer - Step 20740/210000 (9.88%): loss=2.1930, lr=3.00e-04, step_time=1844.7ms, ETA 4d 3h
12/03/2025 08:32:56 - INFO - training.fm_trainer - Step 20750/210000 (9.88%): loss=0.3120, lr=3.00e-04, step_time=1830.5ms, ETA 4d 2h
12/03/2025 08:32:56 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:33:23 - INFO - training.fm_trainer - Eval Step 20750: loss=2.5987, ppl=13.45
12/03/2025 08:33:41 - INFO - training.fm_trainer - Step 20760/210000 (9.89%): loss=0.4395, lr=3.00e-04, step_time=1860.2ms, ETA 4d 2h
12/03/2025 08:34:00 - INFO - training.fm_trainer - Step 20770/210000 (9.89%): loss=0.5353, lr=3.00e-04, step_time=1885.3ms, ETA 4d 2h
12/03/2025 08:34:19 - INFO - training.fm_trainer - Step 20780/210000 (9.90%): loss=0.8112, lr=3.00e-04, step_time=1831.4ms, ETA 4d 2h
12/03/2025 08:34:37 - INFO - training.fm_trainer - Step 20790/210000 (9.90%): loss=1.9601, lr=3.00e-04, step_time=1828.6ms, ETA 4d 2h
12/03/2025 08:34:56 - INFO - training.fm_trainer - Step 20800/210000 (9.90%): loss=3.1420, lr=3.00e-04, grad_norm=0.04, step_time=1886.8ms, ETA 4d 2h
12/03/2025 08:34:56 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:35:23 - INFO - training.fm_trainer - Eval Step 20800: loss=2.6015, ppl=13.48
12/03/2025 08:35:42 - INFO - training.fm_trainer - Step 20810/210000 (9.91%): loss=2.6939, lr=3.00e-04, step_time=1844.0ms, ETA 4d 2h
12/03/2025 08:36:00 - INFO - training.fm_trainer - Step 20820/210000 (9.91%): loss=2.6508, lr=3.00e-04, step_time=1837.5ms, ETA 4d 2h
12/03/2025 08:36:19 - INFO - training.fm_trainer - Step 20830/210000 (9.92%): loss=1.5769, lr=3.00e-04, step_time=1862.5ms, ETA 4d 1h
12/03/2025 08:36:38 - INFO - training.fm_trainer - Step 20840/210000 (9.92%): loss=0.5665, lr=3.00e-04, step_time=1883.7ms, ETA 4d 2h
12/03/2025 08:36:56 - INFO - training.fm_trainer - Step 20850/210000 (9.93%): loss=0.5102, lr=3.00e-04, step_time=1852.8ms, ETA 4d 2h
12/03/2025 08:36:56 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:37:24 - INFO - training.fm_trainer - Eval Step 20850: loss=2.6107, ppl=13.61
12/03/2025 08:37:43 - INFO - training.fm_trainer - Step 20860/210000 (9.93%): loss=1.5827, lr=3.00e-04, step_time=1841.7ms, ETA 4d 1h
12/03/2025 08:38:01 - INFO - training.fm_trainer - Step 20870/210000 (9.94%): loss=1.2301, lr=3.00e-04, step_time=1838.7ms, ETA 4d 1h
12/03/2025 08:38:20 - INFO - training.fm_trainer - Step 20880/210000 (9.94%): loss=0.7044, lr=3.00e-04, step_time=1797.8ms, ETA 4d 1h
12/03/2025 08:38:38 - INFO - training.fm_trainer - Step 20890/210000 (9.95%): loss=7.1229, lr=3.00e-04, step_time=1863.5ms, ETA 4d 1h
12/03/2025 08:38:56 - INFO - training.fm_trainer - Step 20900/210000 (9.95%): loss=5.3793, lr=3.00e-04, step_time=1820.4ms, ETA 4d 1h
12/03/2025 08:38:56 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:39:23 - INFO - training.fm_trainer - Eval Step 20900: loss=2.6151, ppl=13.67
12/03/2025 08:39:41 - INFO - training.fm_trainer - Step 20910/210000 (9.96%): loss=0.4726, lr=3.00e-04, step_time=1825.1ms, ETA 4d 1h
12/03/2025 08:40:00 - INFO - training.fm_trainer - Step 20920/210000 (9.96%): loss=0.5673, lr=3.00e-04, step_time=1802.6ms, ETA 4d 52m
12/03/2025 08:40:18 - INFO - training.fm_trainer - Step 20930/210000 (9.97%): loss=0.2124, lr=3.00e-04, step_time=1828.7ms, ETA 4d 46m
12/03/2025 08:40:37 - INFO - training.fm_trainer - Step 20940/210000 (9.97%): loss=0.4182, lr=3.00e-04, step_time=1827.7ms, ETA 4d 41m
12/03/2025 08:40:55 - INFO - training.fm_trainer - Step 20950/210000 (9.98%): loss=2.5527, lr=3.00e-04, step_time=1841.6ms, ETA 4d 41m
12/03/2025 08:40:55 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:41:21 - INFO - training.fm_trainer - Eval Step 20950: loss=2.6159, ppl=13.68
12/03/2025 08:41:39 - INFO - training.fm_trainer - Step 20960/210000 (9.98%): loss=2.2990, lr=3.00e-04, grad_norm=0.03, step_time=1838.3ms, ETA 4d 40m
12/03/2025 08:41:57 - INFO - training.fm_trainer - Step 20970/210000 (9.99%): loss=0.7796, lr=3.00e-04, step_time=1814.2ms, ETA 4d 31m
12/03/2025 08:42:16 - INFO - training.fm_trainer - Step 20980/210000 (9.99%): loss=4.3302, lr=3.00e-04, step_time=1816.6ms, ETA 4d 24m
12/03/2025 08:42:35 - INFO - training.fm_trainer - Step 20990/210000 (10.00%): loss=0.4598, lr=3.00e-04, step_time=1921.2ms, ETA 4d 51m
12/03/2025 08:42:53 - INFO - training.fm_trainer - Step 21000/210000 (10.00%): loss=8.8670, lr=3.00e-04, step_time=1827.6ms, ETA 4d 45m
12/03/2025 08:42:53 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:43:19 - INFO - training.fm_trainer - Eval Step 21000: loss=2.6511, ppl=14.17
12/03/2025 08:43:19 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-21000
12/03/2025 08:43:19 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 08:43:20 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-21000/pytorch_model_fsdp_0
12/03/2025 08:43:32 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-21000/pytorch_model_fsdp_0
12/03/2025 08:43:32 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-21000
12/03/2025 08:43:32 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 08:43:35 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-21000/optimizer_0
12/03/2025 08:43:55 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-21000/optimizer_0
12/03/2025 08:43:55 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-21000
12/03/2025 08:43:55 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-21000/scheduler.bin
12/03/2025 08:43:55 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-21000/sampler.bin
12/03/2025 08:43:55 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-21000/sampler_1.bin
12/03/2025 08:43:55 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-21000/random_states_0.pkl
12/03/2025 08:43:55 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-21000
12/03/2025 08:43:55 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-20000
12/03/2025 08:44:16 - INFO - training.fm_trainer - Step 21010/210000 (10.00%): loss=0.9175, lr=3.00e-04, step_time=1836.6ms, ETA 4d 43m
12/03/2025 08:44:34 - INFO - training.fm_trainer - Step 21020/210000 (10.01%): loss=0.5162, lr=3.00e-04, step_time=1822.2ms, ETA 4d 36m
12/03/2025 08:44:53 - INFO - training.fm_trainer - Step 21030/210000 (10.01%): loss=0.6715, lr=3.00e-04, step_time=1819.1ms, ETA 4d 29m
12/03/2025 08:45:11 - INFO - training.fm_trainer - Step 21040/210000 (10.02%): loss=1.7559, lr=3.00e-04, step_time=1881.4ms, ETA 4d 42m
12/03/2025 08:45:30 - INFO - training.fm_trainer - Step 21050/210000 (10.02%): loss=2.9538, lr=3.00e-04, step_time=1835.4ms, ETA 4d 40m
12/03/2025 08:45:30 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:45:56 - INFO - training.fm_trainer - Eval Step 21050: loss=2.6308, ppl=13.89
12/03/2025 08:46:14 - INFO - training.fm_trainer - Step 21060/210000 (10.03%): loss=0.3342, lr=3.00e-04, step_time=1808.1ms, ETA 4d 29m
12/03/2025 08:46:33 - INFO - training.fm_trainer - Step 21070/210000 (10.03%): loss=0.9438, lr=3.00e-04, step_time=1826.9ms, ETA 4d 25m
12/03/2025 08:46:51 - INFO - training.fm_trainer - Step 21080/210000 (10.04%): loss=1.3494, lr=3.00e-04, step_time=1834.7ms, ETA 4d 24m
12/03/2025 08:47:10 - INFO - training.fm_trainer - Step 21090/210000 (10.04%): loss=7.1765, lr=3.00e-04, step_time=1848.3ms, ETA 4d 27m
12/03/2025 08:47:28 - INFO - training.fm_trainer - Step 21100/210000 (10.05%): loss=0.3232, lr=3.00e-04, step_time=1856.7ms, ETA 4d 32m
12/03/2025 08:47:28 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:47:55 - INFO - training.fm_trainer - Eval Step 21100: loss=2.6470, ppl=14.11
12/03/2025 08:48:14 - INFO - training.fm_trainer - Step 21110/210000 (10.05%): loss=0.9815, lr=3.00e-04, step_time=1810.0ms, ETA 4d 23m
12/03/2025 08:48:32 - INFO - training.fm_trainer - Step 21120/210000 (10.06%): loss=0.7703, lr=3.00e-04, grad_norm=0.04, step_time=1841.7ms, ETA 4d 24m
12/03/2025 08:48:50 - INFO - training.fm_trainer - Step 21130/210000 (10.06%): loss=4.1480, lr=3.00e-04, step_time=1830.1ms, ETA 4d 21m
12/03/2025 08:49:09 - INFO - training.fm_trainer - Step 21140/210000 (10.07%): loss=1.9100, lr=3.00e-04, step_time=1992.8ms, ETA 4d 1h
12/03/2025 08:49:27 - INFO - training.fm_trainer - Step 21150/210000 (10.07%): loss=0.4773, lr=3.00e-04, step_time=1823.0ms, ETA 4d 1h
12/03/2025 08:49:27 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:49:53 - INFO - training.fm_trainer - Eval Step 21150: loss=2.6505, ppl=14.16
12/03/2025 08:50:11 - INFO - training.fm_trainer - Step 21160/210000 (10.08%): loss=3.1519, lr=3.00e-04, step_time=1828.1ms, ETA 4d 53m
12/03/2025 08:50:30 - INFO - training.fm_trainer - Step 21170/210000 (10.08%): loss=4.4958, lr=3.00e-04, step_time=1860.2ms, ETA 4d 57m
12/03/2025 08:50:48 - INFO - training.fm_trainer - Step 21180/210000 (10.09%): loss=1.3883, lr=3.00e-04, step_time=1845.6ms, ETA 4d 56m
12/03/2025 08:51:06 - INFO - training.fm_trainer - Step 21190/210000 (10.09%): loss=2.1245, lr=3.00e-04, step_time=1871.0ms, ETA 4d 1h
12/03/2025 08:51:25 - INFO - training.fm_trainer - Step 21200/210000 (10.10%): loss=0.2151, lr=3.00e-04, step_time=1832.4ms, ETA 4d 57m
12/03/2025 08:51:25 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:51:51 - INFO - training.fm_trainer - Eval Step 21200: loss=2.5862, ppl=13.28
12/03/2025 08:52:09 - INFO - training.fm_trainer - Step 21210/210000 (10.10%): loss=1.5734, lr=3.00e-04, step_time=1863.0ms, ETA 4d 1h
12/03/2025 08:52:28 - INFO - training.fm_trainer - Step 21220/210000 (10.10%): loss=4.7702, lr=3.00e-04, step_time=1821.1ms, ETA 4d 52m
12/03/2025 08:52:46 - INFO - training.fm_trainer - Step 21230/210000 (10.11%): loss=2.9709, lr=3.00e-04, step_time=1889.4ms, ETA 4d 1h
12/03/2025 08:53:05 - INFO - training.fm_trainer - Step 21240/210000 (10.11%): loss=0.6377, lr=3.00e-04, step_time=1812.7ms, ETA 4d 52m
12/03/2025 08:53:23 - INFO - training.fm_trainer - Step 21250/210000 (10.12%): loss=2.8241, lr=3.00e-04, step_time=1839.0ms, ETA 4d 49m
12/03/2025 08:53:23 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:53:49 - INFO - training.fm_trainer - Eval Step 21250: loss=2.6358, ppl=13.95
12/03/2025 08:54:07 - INFO - training.fm_trainer - Step 21260/210000 (10.12%): loss=0.3234, lr=3.00e-04, step_time=1812.9ms, ETA 4d 38m
12/03/2025 08:54:26 - INFO - training.fm_trainer - Step 21270/210000 (10.13%): loss=1.5887, lr=3.00e-04, step_time=1830.0ms, ETA 4d 34m
12/03/2025 08:54:44 - INFO - training.fm_trainer - Step 21280/210000 (10.13%): loss=1.4653, lr=3.00e-04, grad_norm=0.04, step_time=1854.7ms, ETA 4d 37m
12/03/2025 08:55:02 - INFO - training.fm_trainer - Step 21290/210000 (10.14%): loss=0.6416, lr=3.00e-04, step_time=1836.0ms, ETA 4d 35m
12/03/2025 08:55:21 - INFO - training.fm_trainer - Step 21300/210000 (10.14%): loss=0.3210, lr=3.00e-04, step_time=1821.5ms, ETA 4d 28m
12/03/2025 08:55:21 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:55:46 - INFO - training.fm_trainer - Eval Step 21300: loss=2.6296, ppl=13.87
12/03/2025 08:56:05 - INFO - training.fm_trainer - Step 21310/210000 (10.15%): loss=0.5154, lr=3.00e-04, step_time=1816.2ms, ETA 4d 20m
12/03/2025 08:56:23 - INFO - training.fm_trainer - Step 21320/210000 (10.15%): loss=0.7149, lr=3.00e-04, step_time=1843.4ms, ETA 4d 21m
12/03/2025 08:56:42 - INFO - training.fm_trainer - Step 21330/210000 (10.16%): loss=1.2161, lr=3.00e-04, step_time=1839.8ms, ETA 4d 21m
12/03/2025 08:57:00 - INFO - training.fm_trainer - Step 21340/210000 (10.16%): loss=0.5661, lr=3.00e-04, step_time=1831.2ms, ETA 4d 19m
12/03/2025 08:57:19 - INFO - training.fm_trainer - Step 21350/210000 (10.17%): loss=0.3194, lr=3.00e-04, step_time=1912.7ms, ETA 4d 42m
12/03/2025 08:57:19 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:57:45 - INFO - training.fm_trainer - Eval Step 21350: loss=2.6265, ppl=13.83
12/03/2025 08:58:03 - INFO - training.fm_trainer - Step 21360/210000 (10.17%): loss=0.3469, lr=3.00e-04, step_time=1828.4ms, ETA 4d 36m
12/03/2025 08:58:22 - INFO - training.fm_trainer - Step 21370/210000 (10.18%): loss=2.2716, lr=3.00e-04, step_time=1813.6ms, ETA 4d 26m
12/03/2025 08:58:40 - INFO - training.fm_trainer - Step 21380/210000 (10.18%): loss=3.1482, lr=3.00e-04, step_time=1865.9ms, ETA 4d 34m
12/03/2025 08:58:59 - INFO - training.fm_trainer - Step 21390/210000 (10.19%): loss=0.2409, lr=3.00e-04, step_time=1823.1ms, ETA 4d 27m
12/03/2025 08:59:17 - INFO - training.fm_trainer - Step 21400/210000 (10.19%): loss=0.6594, lr=3.00e-04, step_time=1831.2ms, ETA 4d 24m
12/03/2025 08:59:17 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 08:59:43 - INFO - training.fm_trainer - Eval Step 21400: loss=2.6132, ppl=13.64
12/03/2025 09:00:01 - INFO - training.fm_trainer - Step 21410/210000 (10.20%): loss=0.1633, lr=3.00e-04, step_time=1818.3ms, ETA 4d 17m
12/03/2025 09:00:20 - INFO - training.fm_trainer - Step 21420/210000 (10.20%): loss=0.2504, lr=3.00e-04, step_time=1825.6ms, ETA 4d 12m
12/03/2025 09:00:38 - INFO - training.fm_trainer - Step 21430/210000 (10.20%): loss=0.7453, lr=3.00e-04, step_time=1857.1ms, ETA 4d 19m
12/03/2025 09:00:57 - INFO - training.fm_trainer - Step 21440/210000 (10.21%): loss=0.3235, lr=3.00e-04, grad_norm=0.03, step_time=1973.7ms, ETA 4d 1h
12/03/2025 09:01:15 - INFO - training.fm_trainer - Step 21450/210000 (10.21%): loss=6.7779, lr=3.00e-04, step_time=1839.7ms, ETA 4d 56m
12/03/2025 09:01:15 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 09:01:43 - INFO - training.fm_trainer - Eval Step 21450: loss=2.5837, ppl=13.25
12/03/2025 09:02:01 - INFO - training.fm_trainer - Step 21460/210000 (10.22%): loss=0.4410, lr=3.00e-04, step_time=1837.5ms, ETA 4d 52m
12/03/2025 09:02:20 - INFO - training.fm_trainer - Step 21470/210000 (10.22%): loss=1.3156, lr=3.00e-04, step_time=1809.7ms, ETA 4d 39m
12/03/2025 09:02:38 - INFO - training.fm_trainer - Step 21480/210000 (10.23%): loss=2.3644, lr=3.00e-04, step_time=1841.0ms, ETA 4d 37m
12/03/2025 09:02:56 - INFO - training.fm_trainer - Step 21490/210000 (10.23%): loss=0.9559, lr=3.00e-04, step_time=1823.2ms, ETA 4d 30m
12/03/2025 09:03:15 - INFO - training.fm_trainer - Step 21500/210000 (10.24%): loss=1.7300, lr=3.00e-04, step_time=1853.0ms, ETA 4d 33m
12/03/2025 09:03:15 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 09:03:41 - INFO - training.fm_trainer - Eval Step 21500: loss=2.6071, ppl=13.56
12/03/2025 09:03:41 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-21500
12/03/2025 09:03:41 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 09:03:42 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-21500/pytorch_model_fsdp_0
12/03/2025 09:03:50 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-21500/pytorch_model_fsdp_0
12/03/2025 09:03:50 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-21500
12/03/2025 09:03:50 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 09:03:53 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-21500/optimizer_0
12/03/2025 09:04:10 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-21500/optimizer_0
12/03/2025 09:04:11 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-21500
12/03/2025 09:04:11 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-21500/scheduler.bin
12/03/2025 09:04:11 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-21500/sampler.bin
12/03/2025 09:04:11 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-21500/sampler_1.bin
12/03/2025 09:04:11 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-21500/random_states_0.pkl
12/03/2025 09:04:11 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-21500
12/03/2025 09:04:11 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-20500
12/03/2025 09:04:31 - INFO - training.fm_trainer - Step 21510/210000 (10.24%): loss=2.1110, lr=3.00e-04, step_time=1861.1ms, ETA 4d 38m
12/03/2025 09:04:50 - INFO - training.fm_trainer - Step 21520/210000 (10.25%): loss=0.7063, lr=3.00e-04, step_time=1828.4ms, ETA 4d 32m
12/03/2025 09:05:08 - INFO - training.fm_trainer - Step 21530/210000 (10.25%): loss=3.2203, lr=3.00e-04, step_time=1856.8ms, ETA 4d 36m
12/03/2025 09:05:27 - INFO - training.fm_trainer - Step 21540/210000 (10.26%): loss=0.2258, lr=3.00e-04, step_time=2127.1ms, ETA 4d 2h
12/03/2025 09:05:45 - INFO - training.fm_trainer - Step 21550/210000 (10.26%): loss=1.4859, lr=3.00e-04, step_time=1838.8ms, ETA 4d 1h
12/03/2025 09:05:45 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 09:06:12 - INFO - training.fm_trainer - Eval Step 21550: loss=2.6252, ppl=13.81
12/03/2025 09:06:31 - INFO - training.fm_trainer - Step 21560/210000 (10.27%): loss=0.1930, lr=3.00e-04, step_time=1837.4ms, ETA 4d 1h
12/03/2025 09:06:49 - INFO - training.fm_trainer - Step 21570/210000 (10.27%): loss=1.4078, lr=3.00e-04, step_time=1915.4ms, ETA 4d 1h
12/03/2025 09:07:08 - INFO - training.fm_trainer - Step 21580/210000 (10.28%): loss=0.3206, lr=3.00e-04, step_time=1822.6ms, ETA 4d 1h
12/03/2025 09:07:27 - INFO - training.fm_trainer - Step 21590/210000 (10.28%): loss=3.9295, lr=3.00e-04, step_time=2122.9ms, ETA 4d 3h
12/03/2025 09:07:45 - INFO - training.fm_trainer - Step 21600/210000 (10.29%): loss=0.9098, lr=3.00e-04, grad_norm=0.02, step_time=1879.2ms, ETA 4d 2h
12/03/2025 09:07:45 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 09:08:12 - INFO - training.fm_trainer - Eval Step 21600: loss=2.5766, ppl=13.15
12/03/2025 09:08:31 - INFO - training.fm_trainer - Step 21610/210000 (10.29%): loss=0.7214, lr=3.00e-04, step_time=1852.2ms, ETA 4d 2h
12/03/2025 09:08:49 - INFO - training.fm_trainer - Step 21620/210000 (10.30%): loss=1.2063, lr=3.00e-04, step_time=1832.9ms, ETA 4d 2h
12/03/2025 09:09:08 - INFO - training.fm_trainer - Step 21630/210000 (10.30%): loss=0.3718, lr=3.00e-04, step_time=1843.6ms, ETA 4d 2h
12/03/2025 09:09:26 - INFO - training.fm_trainer - Step 21640/210000 (10.30%): loss=1.3962, lr=3.00e-04, step_time=1853.4ms, ETA 4d 2h
12/03/2025 09:09:45 - INFO - training.fm_trainer - Step 21650/210000 (10.31%): loss=0.9795, lr=3.00e-04, step_time=1831.4ms, ETA 4d 1h
12/03/2025 09:09:45 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 09:10:11 - INFO - training.fm_trainer - Eval Step 21650: loss=2.5911, ppl=13.34
12/03/2025 09:10:29 - INFO - training.fm_trainer - Step 21660/210000 (10.31%): loss=0.3885, lr=3.00e-04, step_time=1826.8ms, ETA 4d 1h
12/03/2025 09:10:47 - INFO - training.fm_trainer - Step 21670/210000 (10.32%): loss=0.4204, lr=3.00e-04, step_time=1854.5ms, ETA 4d 1h
12/03/2025 09:11:06 - INFO - training.fm_trainer - Step 21680/210000 (10.32%): loss=0.5343, lr=3.00e-04, step_time=1861.4ms, ETA 4d 1h
12/03/2025 09:11:24 - INFO - training.fm_trainer - Step 21690/210000 (10.33%): loss=3.9330, lr=3.00e-04, step_time=1824.0ms, ETA 4d 1h
12/03/2025 09:11:42 - INFO - training.fm_trainer - Step 21700/210000 (10.33%): loss=4.1110, lr=3.00e-04, step_time=1850.4ms, ETA 4d 1h
12/03/2025 09:11:42 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 09:12:13 - INFO - training.fm_trainer - Eval Step 21700: loss=2.5983, ppl=13.44
12/03/2025 09:12:31 - INFO - training.fm_trainer - Step 21710/210000 (10.34%): loss=5.0275, lr=3.00e-04, step_time=1883.0ms, ETA 4d 1h
12/03/2025 09:12:50 - INFO - training.fm_trainer - Step 21720/210000 (10.34%): loss=0.1987, lr=3.00e-04, step_time=1878.6ms, ETA 4d 1h
12/03/2025 09:13:08 - INFO - training.fm_trainer - Step 21730/210000 (10.35%): loss=1.2105, lr=3.00e-04, step_time=1827.8ms, ETA 4d 1h
12/03/2025 09:13:26 - INFO - training.fm_trainer - Step 21740/210000 (10.35%): loss=6.8395, lr=3.00e-04, step_time=1860.4ms, ETA 4d 1h
12/03/2025 09:13:45 - INFO - training.fm_trainer - Step 21750/210000 (10.36%): loss=1.3644, lr=3.00e-04, step_time=1855.6ms, ETA 4d 1h
12/03/2025 09:13:45 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 09:14:12 - INFO - training.fm_trainer - Eval Step 21750: loss=2.6042, ppl=13.52
12/03/2025 09:14:30 - INFO - training.fm_trainer - Step 21760/210000 (10.36%): loss=0.5253, lr=3.00e-04, grad_norm=0.03, step_time=1883.8ms, ETA 4d 1h
12/03/2025 09:14:49 - INFO - training.fm_trainer - Step 21770/210000 (10.37%): loss=6.1501, lr=3.00e-04, step_time=1994.3ms, ETA 4d 2h
12/03/2025 09:15:07 - INFO - training.fm_trainer - Step 21780/210000 (10.37%): loss=1.0350, lr=3.00e-04, step_time=1838.0ms, ETA 4d 1h
12/03/2025 09:15:25 - INFO - training.fm_trainer - Step 21790/210000 (10.38%): loss=1.3148, lr=3.00e-04, step_time=1839.4ms, ETA 4d 1h
12/03/2025 09:15:44 - INFO - training.fm_trainer - Step 21800/210000 (10.38%): loss=0.8105, lr=3.00e-04, step_time=1866.9ms, ETA 4d 1h
12/03/2025 09:15:44 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 09:16:11 - INFO - training.fm_trainer - Eval Step 21800: loss=2.6181, ppl=13.71
12/03/2025 09:16:29 - INFO - training.fm_trainer - Step 21810/210000 (10.39%): loss=1.5787, lr=3.00e-04, step_time=1818.8ms, ETA 4d 1h
12/03/2025 09:16:47 - INFO - training.fm_trainer - Step 21820/210000 (10.39%): loss=1.0840, lr=3.00e-04, step_time=1881.9ms, ETA 4d 1h
12/03/2025 09:17:06 - INFO - training.fm_trainer - Step 21830/210000 (10.40%): loss=0.3064, lr=3.00e-04, step_time=1858.8ms, ETA 4d 1h
12/03/2025 09:17:25 - INFO - training.fm_trainer - Step 21840/210000 (10.40%): loss=1.8944, lr=3.00e-04, step_time=1813.7ms, ETA 4d 1h
12/03/2025 09:17:44 - INFO - training.fm_trainer - Step 21850/210000 (10.40%): loss=4.9076, lr=3.00e-04, step_time=1848.6ms, ETA 4d 1h
12/03/2025 09:17:44 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 09:18:10 - INFO - training.fm_trainer - Eval Step 21850: loss=2.6186, ppl=13.72
12/03/2025 09:18:29 - INFO - training.fm_trainer - Step 21860/210000 (10.41%): loss=2.9655, lr=3.00e-04, step_time=1816.3ms, ETA 4d 54m
12/03/2025 09:18:47 - INFO - training.fm_trainer - Step 21870/210000 (10.41%): loss=1.0943, lr=3.00e-04, step_time=1847.1ms, ETA 4d 51m
12/03/2025 09:19:05 - INFO - training.fm_trainer - Step 21880/210000 (10.42%): loss=7.7285, lr=3.00e-04, step_time=1823.9ms, ETA 4d 41m
12/03/2025 09:19:24 - INFO - training.fm_trainer - Step 21890/210000 (10.42%): loss=0.8242, lr=3.00e-04, step_time=1938.9ms, ETA 4d 1h
12/03/2025 09:19:42 - INFO - training.fm_trainer - Step 21900/210000 (10.43%): loss=1.2361, lr=3.00e-04, step_time=1864.8ms, ETA 4d 1h
12/03/2025 09:19:42 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 09:20:08 - INFO - training.fm_trainer - Eval Step 21900: loss=2.6162, ppl=13.68
12/03/2025 09:20:27 - INFO - training.fm_trainer - Step 21910/210000 (10.43%): loss=1.9715, lr=3.00e-04, step_time=1835.6ms, ETA 4d 1h
12/03/2025 09:20:46 - INFO - training.fm_trainer - Step 21920/210000 (10.44%): loss=4.2454, lr=3.00e-04, grad_norm=0.03, step_time=1868.7ms, ETA 4d 1h
12/03/2025 09:21:04 - INFO - training.fm_trainer - Step 21930/210000 (10.44%): loss=0.5233, lr=3.00e-04, step_time=1800.6ms, ETA 4d 47m
12/03/2025 09:21:22 - INFO - training.fm_trainer - Step 21940/210000 (10.45%): loss=0.6102, lr=3.00e-04, step_time=1816.5ms, ETA 4d 35m
12/03/2025 09:21:41 - INFO - training.fm_trainer - Step 21950/210000 (10.45%): loss=0.2320, lr=3.00e-04, step_time=1821.5ms, ETA 4d 26m
12/03/2025 09:21:41 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 09:22:07 - INFO - training.fm_trainer - Eval Step 21950: loss=2.6216, ppl=13.76
12/03/2025 09:22:25 - INFO - training.fm_trainer - Step 21960/210000 (10.46%): loss=2.4775, lr=3.00e-04, step_time=1818.1ms, ETA 4d 17m
12/03/2025 09:22:44 - INFO - training.fm_trainer - Step 21970/210000 (10.46%): loss=2.9638, lr=3.00e-04, step_time=1830.3ms, ETA 4d 13m
12/03/2025 09:23:02 - INFO - training.fm_trainer - Step 21980/210000 (10.47%): loss=3.1886, lr=3.00e-04, step_time=1926.7ms, ETA 4d 39m
12/03/2025 09:23:21 - INFO - training.fm_trainer - Step 21990/210000 (10.47%): loss=3.4627, lr=3.00e-04, step_time=1805.4ms, ETA 4d 24m
12/03/2025 09:23:39 - INFO - training.fm_trainer - Step 22000/210000 (10.48%): loss=3.2251, lr=3.00e-04, step_time=1831.5ms, ETA 4d 20m
12/03/2025 09:23:39 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 09:24:06 - INFO - training.fm_trainer - Eval Step 22000: loss=2.6371, ppl=13.97
12/03/2025 09:24:06 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-22000
12/03/2025 09:24:06 - INFO - accelerate.accelerator - Saving FSDP model
12/03/2025 09:24:07 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-22000/pytorch_model_fsdp_0
12/03/2025 09:24:15 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-22000/pytorch_model_fsdp_0
12/03/2025 09:24:16 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-22000
12/03/2025 09:24:16 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/03/2025 09:24:18 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-22000/optimizer_0
12/03/2025 09:24:34 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-22000/optimizer_0
12/03/2025 09:24:34 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-22000
12/03/2025 09:24:34 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-22000/scheduler.bin
12/03/2025 09:24:34 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-22000/sampler.bin
12/03/2025 09:24:34 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-22000/sampler_1.bin
12/03/2025 09:24:34 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-22000/random_states_0.pkl
12/03/2025 09:24:34 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-22000
12/03/2025 09:24:34 - INFO - training.fm_trainer - Deleting old checkpoint: checkpoints/flow_matching_qwen_1.7b/checkpoint-21000
12/03/2025 09:24:55 - INFO - training.fm_trainer - Step 22010/210000 (10.48%): loss=0.9278, lr=3.00e-04, step_time=1839.6ms, ETA 4d 18m
12/03/2025 09:25:13 - INFO - training.fm_trainer - Step 22020/210000 (10.49%): loss=0.7568, lr=3.00e-04, step_time=1820.4ms, ETA 4d 10m
12/03/2025 09:25:32 - INFO - training.fm_trainer - Step 22030/210000 (10.49%): loss=0.4206, lr=3.00e-04, step_time=1826.9ms, ETA 4d 5m
12/03/2025 09:25:50 - INFO - training.fm_trainer - Step 22040/210000 (10.50%): loss=7.2609, lr=3.00e-04, step_time=1835.3ms, ETA 4d 3m
12/03/2025 09:26:08 - INFO - training.fm_trainer - Step 22050/210000 (10.50%): loss=3.8758, lr=3.00e-04, step_time=1901.9ms, ETA 4d 22m
12/03/2025 09:26:08 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 09:26:34 - INFO - training.fm_trainer - Eval Step 22050: loss=2.6113, ppl=13.62
12/03/2025 09:26:53 - INFO - training.fm_trainer - Step 22060/210000 (10.50%): loss=0.3912, lr=3.00e-04, step_time=1809.4ms, ETA 4d 10m
12/03/2025 09:27:11 - INFO - training.fm_trainer - Step 22070/210000 (10.51%): loss=3.0301, lr=3.00e-04, step_time=1824.9ms, ETA 4d 5m
12/03/2025 09:27:29 - INFO - training.fm_trainer - Step 22080/210000 (10.51%): loss=3.4983, lr=3.00e-04, grad_norm=0.02, step_time=1886.8ms, ETA 4d 19m
12/03/2025 09:27:48 - INFO - training.fm_trainer - Step 22090/210000 (10.52%): loss=0.7348, lr=3.00e-04, step_time=1822.7ms, ETA 4d 11m
12/03/2025 09:28:06 - INFO - training.fm_trainer - Step 22100/210000 (10.52%): loss=3.0980, lr=3.00e-04, step_time=1811.1ms, ETA 4d 1m
12/03/2025 09:28:06 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 09:28:34 - INFO - training.fm_trainer - Eval Step 22100: loss=2.6216, ppl=13.76
12/03/2025 09:28:52 - INFO - training.fm_trainer - Step 22110/210000 (10.53%): loss=5.5880, lr=3.00e-04, step_time=1823.5ms, ETA 3d 23h
12/03/2025 09:29:11 - INFO - training.fm_trainer - Step 22120/210000 (10.53%): loss=0.9019, lr=3.00e-04, step_time=2136.9ms, ETA 4d 1h
12/03/2025 09:29:29 - INFO - training.fm_trainer - Step 22130/210000 (10.54%): loss=0.8730, lr=3.00e-04, step_time=1979.6ms, ETA 4d 2h
12/03/2025 09:29:48 - INFO - training.fm_trainer - Step 22140/210000 (10.54%): loss=0.9808, lr=3.00e-04, step_time=1879.9ms, ETA 4d 2h
12/03/2025 09:30:06 - INFO - training.fm_trainer - Step 22150/210000 (10.55%): loss=0.5230, lr=3.00e-04, step_time=1834.4ms, ETA 4d 1h
12/03/2025 09:30:06 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 09:30:34 - INFO - training.fm_trainer - Eval Step 22150: loss=2.6246, ppl=13.80
12/03/2025 09:30:53 - INFO - training.fm_trainer - Step 22160/210000 (10.55%): loss=2.0450, lr=3.00e-04, step_time=1860.7ms, ETA 4d 1h
12/03/2025 09:31:11 - INFO - training.fm_trainer - Step 22170/210000 (10.56%): loss=0.5098, lr=3.00e-04, step_time=1864.6ms, ETA 4d 1h
12/03/2025 09:31:30 - INFO - training.fm_trainer - Step 22180/210000 (10.56%): loss=0.7148, lr=3.00e-04, step_time=1868.3ms, ETA 4d 1h
12/03/2025 09:31:48 - INFO - training.fm_trainer - Step 22190/210000 (10.57%): loss=0.4335, lr=3.00e-04, step_time=1820.1ms, ETA 4d 1h
12/03/2025 09:32:07 - INFO - training.fm_trainer - Step 22200/210000 (10.57%): loss=1.9114, lr=3.00e-04, step_time=1819.9ms, ETA 4d 1h
12/03/2025 09:32:07 - INFO - training.fm_trainer - Running evaluation on 50 batches...
12/03/2025 09:32:33 - INFO - training.fm_trainer - Eval Step 22200: loss=2.5901, ppl=13.33
