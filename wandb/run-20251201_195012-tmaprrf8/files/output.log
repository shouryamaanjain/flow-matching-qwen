12/01/2025 19:50:14 - INFO - training.fm_trainer - Starting training
12/01/2025 19:50:14 - INFO - training.fm_trainer -   Max steps: 2
12/01/2025 19:50:14 - INFO - training.fm_trainer -   Global batch size: 128
12/01/2025 19:50:14 - INFO - training.fm_trainer -   Gradient accumulation steps: 32
12/01/2025 19:50:14 - INFO - data.data_loader - Including The Stack v1 datasets (RECOMMENDED):
- Contains actual code content directly
- No AWS credentials needed
- Just login with: huggingface-cli login
See: https://huggingface.co/datasets/bigcode/the-stack-dedup
12/01/2025 19:50:14 - INFO - data.data_loader - Loading dataset: dclm-baseline from mlfoundations/dclm-baseline-1.0
12/01/2025 19:50:38 - INFO - data.data_loader - ✓ Loaded dclm-baseline with weight 60.17
12/01/2025 19:50:38 - INFO - data.data_loader - Loading dataset: openwebmath from open-web-math/open-web-math
12/01/2025 19:50:42 - INFO - data.data_loader - ✓ Loaded openwebmath with weight 12.98
12/01/2025 19:50:42 - INFO - data.data_loader - Loading dataset: arxiv-summarization from ccdv/arxiv-summarization
12/01/2025 19:50:46 - INFO - data.data_loader - ✓ Loaded arxiv-summarization with weight 9.18
12/01/2025 19:50:46 - INFO - data.data_loader - Loading dataset: wikipedia-en from wikimedia/wikipedia
12/01/2025 19:50:50 - INFO - data.data_loader - ✓ Loaded wikipedia-en with weight 5.41
12/01/2025 19:50:50 - INFO - data.data_loader - Loading dataset: mathpile from zwhe99/mathpile-text
12/01/2025 19:50:54 - INFO - data.data_loader - ✓ Loaded mathpile with weight 3.24
12/01/2025 19:50:54 - INFO - data.data_loader - Loading dataset: stack-v1-python from bigcode/the-stack-dedup
12/01/2025 19:50:58 - INFO - data.data_loader - ✓ Loaded stack-v1-python with weight 50.75
12/01/2025 19:50:58 - INFO - data.data_loader - Loading dataset: stack-v1-javascript from bigcode/the-stack-dedup
12/01/2025 19:51:01 - INFO - data.data_loader - ✓ Loaded stack-v1-javascript with weight 20.0
12/01/2025 19:51:01 - INFO - data.data_loader - Loading dataset: stack-v1-java from bigcode/the-stack-dedup
12/01/2025 19:51:04 - INFO - data.data_loader - ✓ Loaded stack-v1-java with weight 15.0
12/01/2025 19:51:04 - INFO - data.data_loader - Loading dataset: stack-v1-cpp from bigcode/the-stack-dedup
12/01/2025 19:51:07 - INFO - data.data_loader - ✓ Loaded stack-v1-cpp with weight 10.0
12/01/2025 19:51:07 - INFO - data.data_loader - Creating mixture with 9 datasets
12/01/2025 19:51:07 - INFO - data.data_loader - Sampling probabilities: {'dclm-baseline': 0.3222299576929256, 'openwebmath': 0.06951212981309914, 'arxiv-summarization': 0.04916189150109784, 'wikipedia-en': 0.028972312965243937, 'mathpile': 0.01735125582391689, 'stack-v1-python': 0.27178278798264877, 'stack-v1-javascript': 0.10710651743158572, 'stack-v1-java': 0.08032988807368929, 'stack-v1-cpp': 0.05355325871579286}
12/01/2025 19:51:25 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-2
12/01/2025 19:51:25 - INFO - accelerate.accelerator - Saving FSDP model
/usr/local/lib/python3.12/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:678: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:860: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/distributed/fsdp/_state_dict_utils.py:722: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  local_shape = tensor.shape
/usr/local/lib/python3.12/dist-packages/torch/distributed/fsdp/_state_dict_utils.py:739: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.shape,
/usr/local/lib/python3.12/dist-packages/torch/distributed/fsdp/_state_dict_utils.py:741: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.dtype,
12/01/2025 19:51:26 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-2/pytorch_model_fsdp_0
12/01/2025 19:51:35 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-2/pytorch_model_fsdp_0
12/01/2025 19:51:35 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-2
12/01/2025 19:51:35 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/01/2025 19:51:35 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-2/optimizer_0
12/01/2025 19:51:35 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-2/optimizer_0
12/01/2025 19:51:35 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-2
12/01/2025 19:51:35 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-2/scheduler.bin
12/01/2025 19:51:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-2/sampler.bin
12/01/2025 19:51:35 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-2/random_states_0.pkl
12/01/2025 19:51:35 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-2
12/01/2025 19:51:35 - INFO - training.fm_trainer - Training complete!
