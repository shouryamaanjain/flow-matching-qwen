12/01/2025 20:29:53 - INFO - training.fm_trainer - Starting training
12/01/2025 20:29:53 - INFO - training.fm_trainer -   Max steps: 210000
12/01/2025 20:29:53 - INFO - training.fm_trainer -   Global batch size: 128
12/01/2025 20:29:53 - INFO - training.fm_trainer -   Gradient accumulation steps: 32
12/01/2025 20:29:53 - INFO - data.data_loader - Including The Stack v1 datasets (RECOMMENDED):
- Contains actual code content directly
- No AWS credentials needed
- Just login with: huggingface-cli login
See: https://huggingface.co/datasets/bigcode/the-stack-dedup
12/01/2025 20:29:53 - INFO - data.data_loader - Loading dataset: dclm-baseline from mlfoundations/dclm-baseline-1.0
Resolving data files: 100%|█████████████████████████████████████████████████████████████████████| 27838/27838 [00:00<00:00, 67532.06it/s]
12/01/2025 20:30:13 - INFO - data.data_loader - ✓ Loaded dclm-baseline with weight 60.17
12/01/2025 20:30:13 - INFO - data.data_loader - Loading dataset: openwebmath from open-web-math/open-web-math
Resolving data files: 100%|█████████████████████████████████████████████████████████████████████████| 114/114 [00:00<00:00, 64095.26it/s]
12/01/2025 20:30:14 - INFO - data.data_loader - ✓ Loaded openwebmath with weight 12.98
12/01/2025 20:30:14 - INFO - data.data_loader - Loading dataset: arxiv-summarization from ccdv/arxiv-summarization
12/01/2025 20:30:16 - INFO - data.data_loader - ✓ Loaded arxiv-summarization with weight 9.18
12/01/2025 20:30:16 - INFO - data.data_loader - Loading dataset: wikipedia-en from wikimedia/wikipedia
Resolving data files: 100%|███████████████████████████████████████████████████████████████████████████| 41/41 [00:00<00:00, 57360.39it/s]
12/01/2025 20:30:18 - INFO - data.data_loader - ✓ Loaded wikipedia-en with weight 5.41
12/01/2025 20:30:18 - INFO - data.data_loader - Loading dataset: mathpile from zwhe99/mathpile-text
Resolving data files: 100%|███████████████████████████████████████████████████████████████████████████| 53/53 [00:00<00:00, 61154.91it/s]
Resolving data files: 100%|███████████████████████████████████████████████████████████████████████████| 53/53 [00:00<00:00, 62146.52it/s]
12/01/2025 20:30:20 - INFO - data.data_loader - ✓ Loaded mathpile with weight 3.24
12/01/2025 20:30:20 - INFO - data.data_loader - Loading dataset: stack-v1-python from bigcode/the-stack-dedup
Resolving data files: 100%|█████████████████████████████████████████████████████████████████████████| 144/144 [00:00<00:00, 70550.14it/s]
12/01/2025 20:30:22 - INFO - data.data_loader - ✓ Loaded stack-v1-python with weight 50.75
12/01/2025 20:30:22 - INFO - data.data_loader - Loading dataset: stack-v1-javascript from bigcode/the-stack-dedup
Resolving data files: 100%|█████████████████████████████████████████████████████████████████████████| 310/310 [00:00<00:00, 69271.94it/s]
12/01/2025 20:30:24 - INFO - data.data_loader - ✓ Loaded stack-v1-javascript with weight 20.0
12/01/2025 20:30:24 - INFO - data.data_loader - Loading dataset: stack-v1-java from bigcode/the-stack-dedup
Resolving data files: 100%|█████████████████████████████████████████████████████████████████████████| 208/208 [00:00<00:00, 71409.94it/s]
12/01/2025 20:30:26 - INFO - data.data_loader - ✓ Loaded stack-v1-java with weight 15.0
12/01/2025 20:30:26 - INFO - data.data_loader - Loading dataset: stack-v1-cpp from bigcode/the-stack-dedup
Resolving data files: 100%|█████████████████████████████████████████████████████████████████████████| 110/110 [00:00<00:00, 66797.95it/s]
12/01/2025 20:30:28 - INFO - data.data_loader - ✓ Loaded stack-v1-cpp with weight 10.0
12/01/2025 20:30:28 - INFO - data.data_loader - Creating mixture with 9 datasets
12/01/2025 20:30:28 - INFO - data.data_loader - Sampling probabilities: {'dclm-baseline': 0.3222299576929256, 'openwebmath': 0.06951212981309914, 'arxiv-summarization': 0.04916189150109784, 'wikipedia-en': 0.028972312965243937, 'mathpile': 0.01735125582391689, 'stack-v1-python': 0.27178278798264877, 'stack-v1-javascript': 0.10710651743158572, 'stack-v1-java': 0.08032988807368929, 'stack-v1-cpp': 0.05355325871579286}
12/01/2025 20:31:27 - INFO - training.fm_trainer - Step 10/210000 (0.00%): loss=12.2037, lr=0.00e+00, step_time=2019.8ms, ETA 4d 21h
12/01/2025 20:31:45 - INFO - training.fm_trainer - Step 20/210000 (0.01%): loss=9.6228, lr=0.00e+00, step_time=1820.7ms, ETA 4d 20h
12/01/2025 20:32:04 - INFO - training.fm_trainer - Step 30/210000 (0.01%): loss=11.4017, lr=0.00e+00, step_time=1845.4ms, ETA 4d 19h
12/01/2025 20:32:22 - INFO - training.fm_trainer - Step 40/210000 (0.02%): loss=10.5461, lr=6.00e-07, step_time=1830.0ms, ETA 4d 18h
12/01/2025 20:32:41 - INFO - training.fm_trainer - Step 50/210000 (0.02%): loss=10.7578, lr=6.00e-07, step_time=1843.9ms, ETA 4d 18h
12/01/2025 20:33:00 - INFO - training.fm_trainer - Step 60/210000 (0.03%): loss=18.4578, lr=6.00e-07, step_time=1826.9ms, ETA 4d 17h
12/01/2025 20:33:18 - INFO - training.fm_trainer - Step 70/210000 (0.03%): loss=11.1964, lr=1.20e-06, step_time=1830.8ms, ETA 4d 16h
12/01/2025 20:33:37 - INFO - training.fm_trainer - Step 80/210000 (0.04%): loss=13.8857, lr=1.20e-06, step_time=1879.4ms, ETA 4d 16h
12/01/2025 20:33:55 - INFO - training.fm_trainer - Step 90/210000 (0.04%): loss=10.6338, lr=1.20e-06, step_time=1873.4ms, ETA 4d 16h
12/01/2025 20:34:14 - INFO - training.fm_trainer - Step 100/210000 (0.05%): loss=16.1632, lr=1.80e-06, step_time=1826.5ms, ETA 4d 15h
12/01/2025 20:34:32 - INFO - training.fm_trainer - Step 110/210000 (0.05%): loss=12.3544, lr=1.80e-06, step_time=1839.6ms, ETA 4d 15h
12/01/2025 20:34:51 - INFO - training.fm_trainer - Step 120/210000 (0.06%): loss=12.2260, lr=1.80e-06, step_time=1860.6ms, ETA 4d 14h
12/01/2025 20:35:09 - INFO - training.fm_trainer - Step 130/210000 (0.06%): loss=13.6618, lr=2.40e-06, step_time=1840.4ms, ETA 4d 14h
12/01/2025 20:35:28 - INFO - training.fm_trainer - Step 140/210000 (0.07%): loss=16.1070, lr=2.40e-06, step_time=1843.0ms, ETA 4d 14h
Token indices sequence length is longer than the specified maximum sequence length for this model (246929 > 131072). Running this sequence through the model will result in indexing errors
12/01/2025 20:35:47 - INFO - training.fm_trainer - Step 150/210000 (0.07%): loss=11.9288, lr=2.40e-06, step_time=1848.3ms, ETA 4d 13h
12/01/2025 20:36:06 - INFO - training.fm_trainer - Step 160/210000 (0.08%): loss=12.4288, lr=3.00e-06, step_time=1796.2ms, ETA 4d 13h
12/01/2025 20:36:24 - INFO - training.fm_trainer - Step 170/210000 (0.08%): loss=9.7111, lr=3.00e-06, step_time=1822.1ms, ETA 4d 13h
12/01/2025 20:36:43 - INFO - training.fm_trainer - Step 180/210000 (0.09%): loss=7.9196, lr=3.00e-06, step_time=1845.9ms, ETA 4d 12h
12/01/2025 20:37:01 - INFO - training.fm_trainer - Step 190/210000 (0.09%): loss=13.6773, lr=3.00e-06, step_time=1859.3ms, ETA 4d 12h
12/01/2025 20:37:20 - INFO - training.fm_trainer - Step 200/210000 (0.10%): loss=11.5870, lr=3.60e-06, step_time=1852.0ms, ETA 4d 12h
12/01/2025 20:37:38 - INFO - training.fm_trainer - Step 210/210000 (0.10%): loss=11.1237, lr=3.60e-06, step_time=1868.2ms, ETA 4d 12h
12/01/2025 20:37:57 - INFO - training.fm_trainer - Step 220/210000 (0.10%): loss=13.6155, lr=3.60e-06, step_time=1846.4ms, ETA 4d 12h
12/01/2025 20:38:15 - INFO - training.fm_trainer - Step 230/210000 (0.11%): loss=1.6647, lr=4.20e-06, step_time=1836.2ms, ETA 4d 12h
12/01/2025 20:38:34 - INFO - training.fm_trainer - Step 240/210000 (0.11%): loss=12.9921, lr=4.20e-06, step_time=1823.3ms, ETA 4d 12h
12/01/2025 20:38:53 - INFO - training.fm_trainer - Step 250/210000 (0.12%): loss=12.8709, lr=4.20e-06, step_time=1963.4ms, ETA 4d 12h
12/01/2025 20:39:11 - INFO - training.fm_trainer - Step 260/210000 (0.12%): loss=10.6669, lr=4.80e-06, step_time=1841.1ms, ETA 4d 12h
12/01/2025 20:39:30 - INFO - training.fm_trainer - Step 270/210000 (0.13%): loss=16.0124, lr=4.80e-06, step_time=1834.9ms, ETA 4d 12h
12/01/2025 20:39:48 - INFO - training.fm_trainer - Step 280/210000 (0.13%): loss=5.6526, lr=4.80e-06, step_time=1874.8ms, ETA 4d 12h
12/01/2025 20:40:07 - INFO - training.fm_trainer - Step 290/210000 (0.14%): loss=11.5253, lr=5.40e-06, step_time=1820.2ms, ETA 4d 12h
12/01/2025 20:40:25 - INFO - training.fm_trainer - Step 300/210000 (0.14%): loss=10.7132, lr=5.40e-06, step_time=1846.5ms, ETA 4d 12h
12/01/2025 20:40:43 - INFO - training.fm_trainer - Step 310/210000 (0.15%): loss=10.4153, lr=5.40e-06, step_time=1841.5ms, ETA 4d 12h
12/01/2025 20:41:02 - INFO - training.fm_trainer - Step 320/210000 (0.15%): loss=9.9763, lr=6.00e-06, step_time=1817.2ms, ETA 4d 11h
12/01/2025 20:41:21 - INFO - training.fm_trainer - Step 330/210000 (0.16%): loss=11.2110, lr=6.00e-06, step_time=1833.4ms, ETA 4d 11h
12/01/2025 20:41:39 - INFO - training.fm_trainer - Step 340/210000 (0.16%): loss=8.2528, lr=6.00e-06, step_time=1836.1ms, ETA 4d 11h
12/01/2025 20:41:57 - INFO - training.fm_trainer - Step 350/210000 (0.17%): loss=11.2758, lr=6.00e-06, step_time=1899.4ms, ETA 4d 11h
12/01/2025 20:42:16 - INFO - training.fm_trainer - Step 360/210000 (0.17%): loss=9.9817, lr=6.60e-06, step_time=1839.7ms, ETA 4d 11h
12/01/2025 20:42:35 - INFO - training.fm_trainer - Step 370/210000 (0.18%): loss=2.7861, lr=6.60e-06, step_time=1832.8ms, ETA 4d 11h
12/01/2025 20:42:53 - INFO - training.fm_trainer - Step 380/210000 (0.18%): loss=11.1124, lr=6.60e-06, step_time=1955.8ms, ETA 4d 12h
12/01/2025 20:43:12 - INFO - training.fm_trainer - Step 390/210000 (0.19%): loss=7.8673, lr=7.20e-06, step_time=1864.6ms, ETA 4d 12h
12/01/2025 20:43:30 - INFO - training.fm_trainer - Step 400/210000 (0.19%): loss=10.8271, lr=7.20e-06, step_time=1829.3ms, ETA 4d 12h
12/01/2025 20:43:49 - INFO - training.fm_trainer - Step 410/210000 (0.20%): loss=3.3642, lr=7.20e-06, step_time=1884.8ms, ETA 4d 12h
12/01/2025 20:44:07 - INFO - training.fm_trainer - Step 420/210000 (0.20%): loss=4.7046, lr=7.80e-06, step_time=1817.7ms, ETA 4d 12h
12/01/2025 20:44:26 - INFO - training.fm_trainer - Step 430/210000 (0.20%): loss=8.0781, lr=7.80e-06, step_time=1853.0ms, ETA 4d 12h
12/01/2025 20:44:44 - INFO - training.fm_trainer - Step 440/210000 (0.21%): loss=10.1455, lr=7.80e-06, step_time=1879.0ms, ETA 4d 12h
12/01/2025 20:45:03 - INFO - training.fm_trainer - Step 450/210000 (0.21%): loss=9.2619, lr=8.40e-06, step_time=1821.7ms, ETA 4d 11h
12/01/2025 20:45:21 - INFO - training.fm_trainer - Step 460/210000 (0.22%): loss=8.4449, lr=8.40e-06, step_time=1832.8ms, ETA 4d 11h
12/01/2025 20:45:40 - INFO - training.fm_trainer - Step 470/210000 (0.22%): loss=6.2766, lr=8.40e-06, step_time=1874.5ms, ETA 4d 11h
12/01/2025 20:45:58 - INFO - training.fm_trainer - Step 480/210000 (0.23%): loss=5.8084, lr=9.00e-06, step_time=1814.9ms, ETA 4d 11h
12/01/2025 20:46:17 - INFO - training.fm_trainer - Step 490/210000 (0.23%): loss=2.7133, lr=9.00e-06, step_time=1965.7ms, ETA 4d 12h
12/01/2025 20:46:35 - INFO - training.fm_trainer - Step 500/210000 (0.24%): loss=2.9612, lr=9.00e-06, step_time=1842.6ms, ETA 4d 12h
12/01/2025 20:46:35 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-500
12/01/2025 20:46:35 - INFO - accelerate.accelerator - Saving FSDP model
/root/miniconda3/lib/python3.13/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/miniconda3/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:860: UserWarning: `_get_pg_default_device` will be deprecated, it only stays for backward-compatiblity reason. If you need to find a device for object collectives, please use `_get_object_coll_device`. If you need to query the device types supported by group, please use `_device_capability(group)`.
  warnings.warn(
/root/miniconda3/lib/python3.13/site-packages/torch/distributed/fsdp/_state_dict_utils.py:722: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  local_shape = tensor.shape
/root/miniconda3/lib/python3.13/site-packages/torch/distributed/fsdp/_state_dict_utils.py:739: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.shape,
/root/miniconda3/lib/python3.13/site-packages/torch/distributed/fsdp/_state_dict_utils.py:741: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
  tensor.dtype,
12/01/2025 20:46:36 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-500/pytorch_model_fsdp_0
12/01/2025 20:46:45 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-500/pytorch_model_fsdp_0
12/01/2025 20:46:45 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-500
12/01/2025 20:46:45 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/01/2025 20:46:48 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-500/optimizer_0
12/01/2025 20:47:03 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-500/optimizer_0
12/01/2025 20:47:03 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-500
12/01/2025 20:47:03 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-500/scheduler.bin
12/01/2025 20:47:03 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-500/sampler.bin
12/01/2025 20:47:03 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-500/random_states_0.pkl
12/01/2025 20:47:03 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-500
12/01/2025 20:47:22 - INFO - training.fm_trainer - Step 510/210000 (0.24%): loss=5.0962, lr=9.00e-06, step_time=1883.6ms, ETA 4d 12h
12/01/2025 20:47:41 - INFO - training.fm_trainer - Step 520/210000 (0.25%): loss=5.9445, lr=9.60e-06, step_time=1860.0ms, ETA 4d 12h
12/01/2025 20:47:59 - INFO - training.fm_trainer - Step 530/210000 (0.25%): loss=2.4062, lr=9.60e-06, step_time=1862.6ms, ETA 4d 12h
12/01/2025 20:48:18 - INFO - training.fm_trainer - Step 540/210000 (0.26%): loss=9.2189, lr=9.60e-06, step_time=1848.3ms, ETA 4d 12h
12/01/2025 20:48:36 - INFO - training.fm_trainer - Step 550/210000 (0.26%): loss=6.6498, lr=1.02e-05, step_time=1840.2ms, ETA 4d 12h
12/01/2025 20:48:55 - INFO - training.fm_trainer - Step 560/210000 (0.27%): loss=8.0890, lr=1.02e-05, step_time=1822.5ms, ETA 4d 11h
12/01/2025 20:49:14 - INFO - training.fm_trainer - Step 570/210000 (0.27%): loss=13.1797, lr=1.02e-05, step_time=1836.5ms, ETA 4d 11h
12/01/2025 20:49:32 - INFO - training.fm_trainer - Step 580/210000 (0.28%): loss=9.3300, lr=1.08e-05, step_time=1813.5ms, ETA 4d 11h
12/01/2025 20:49:51 - INFO - training.fm_trainer - Step 590/210000 (0.28%): loss=7.7089, lr=1.08e-05, step_time=1834.3ms, ETA 4d 11h
12/01/2025 20:50:09 - INFO - training.fm_trainer - Step 600/210000 (0.29%): loss=5.2783, lr=1.08e-05, step_time=1885.0ms, ETA 4d 11h
12/01/2025 20:50:28 - INFO - training.fm_trainer - Step 610/210000 (0.29%): loss=4.4700, lr=1.14e-05, step_time=1825.6ms, ETA 4d 11h
12/01/2025 20:50:46 - INFO - training.fm_trainer - Step 620/210000 (0.30%): loss=1.3858, lr=1.14e-05, step_time=1879.6ms, ETA 4d 11h
12/01/2025 20:51:05 - INFO - training.fm_trainer - Step 630/210000 (0.30%): loss=8.0285, lr=1.14e-05, step_time=1836.1ms, ETA 4d 11h
12/01/2025 20:51:25 - INFO - training.fm_trainer - Step 640/210000 (0.30%): loss=3.0633, lr=1.20e-05, step_time=2839.2ms, ETA 4d 17h
12/01/2025 20:51:43 - INFO - training.fm_trainer - Step 650/210000 (0.31%): loss=5.9848, lr=1.20e-05, step_time=1844.6ms, ETA 4d 16h
12/01/2025 20:52:02 - INFO - training.fm_trainer - Step 660/210000 (0.31%): loss=13.5774, lr=1.20e-05, step_time=2016.3ms, ETA 4d 17h
12/01/2025 20:52:20 - INFO - training.fm_trainer - Step 670/210000 (0.32%): loss=1.8776, lr=1.20e-05, step_time=1827.2ms, ETA 4d 16h
12/01/2025 20:52:39 - INFO - training.fm_trainer - Step 680/210000 (0.32%): loss=3.8369, lr=1.26e-05, step_time=1848.7ms, ETA 4d 15h
12/01/2025 20:52:57 - INFO - training.fm_trainer - Step 690/210000 (0.33%): loss=9.1864, lr=1.26e-05, step_time=1844.7ms, ETA 4d 15h
12/01/2025 20:53:16 - INFO - training.fm_trainer - Step 700/210000 (0.33%): loss=1.6417, lr=1.26e-05, step_time=1819.3ms, ETA 4d 14h
12/01/2025 20:53:34 - INFO - training.fm_trainer - Step 710/210000 (0.34%): loss=3.9204, lr=1.32e-05, step_time=1856.7ms, ETA 4d 14h
12/01/2025 20:53:53 - INFO - training.fm_trainer - Step 720/210000 (0.34%): loss=5.3680, lr=1.32e-05, step_time=1835.2ms, ETA 4d 14h
12/01/2025 20:54:11 - INFO - training.fm_trainer - Step 730/210000 (0.35%): loss=6.9591, lr=1.32e-05, step_time=1857.3ms, ETA 4d 13h
12/01/2025 20:54:30 - INFO - training.fm_trainer - Step 740/210000 (0.35%): loss=1.4022, lr=1.38e-05, step_time=1967.4ms, ETA 4d 14h
12/01/2025 20:54:48 - INFO - training.fm_trainer - Step 750/210000 (0.36%): loss=4.6461, lr=1.38e-05, step_time=1832.3ms, ETA 4d 14h
12/01/2025 20:55:07 - INFO - training.fm_trainer - Step 760/210000 (0.36%): loss=3.5654, lr=1.38e-05, step_time=1815.3ms, ETA 4d 13h
12/01/2025 20:55:25 - INFO - training.fm_trainer - Step 770/210000 (0.37%): loss=1.6840, lr=1.44e-05, step_time=1836.9ms, ETA 4d 13h
12/01/2025 20:55:44 - INFO - training.fm_trainer - Step 780/210000 (0.37%): loss=7.0680, lr=1.44e-05, step_time=1852.4ms, ETA 4d 13h
12/01/2025 20:56:03 - INFO - training.fm_trainer - Step 790/210000 (0.38%): loss=1.8693, lr=1.44e-05, step_time=1849.2ms, ETA 4d 12h
12/01/2025 20:56:21 - INFO - training.fm_trainer - Step 800/210000 (0.38%): loss=4.3457, lr=1.50e-05, step_time=1842.1ms, ETA 4d 12h
12/01/2025 20:56:40 - INFO - training.fm_trainer - Step 810/210000 (0.39%): loss=3.2766, lr=1.50e-05, step_time=1835.9ms, ETA 4d 12h
12/01/2025 20:56:58 - INFO - training.fm_trainer - Step 820/210000 (0.39%): loss=1.0716, lr=1.50e-05, step_time=1849.9ms, ETA 4d 12h
12/01/2025 20:57:17 - INFO - training.fm_trainer - Step 830/210000 (0.40%): loss=1.1566, lr=1.50e-05, step_time=2016.6ms, ETA 4d 13h
12/01/2025 20:57:35 - INFO - training.fm_trainer - Step 840/210000 (0.40%): loss=2.6921, lr=1.56e-05, step_time=1838.1ms, ETA 4d 13h
12/01/2025 20:57:54 - INFO - training.fm_trainer - Step 850/210000 (0.40%): loss=12.3112, lr=1.56e-05, step_time=1827.6ms, ETA 4d 12h
12/01/2025 20:58:12 - INFO - training.fm_trainer - Step 860/210000 (0.41%): loss=7.6874, lr=1.56e-05, step_time=1834.1ms, ETA 4d 12h
12/01/2025 20:58:31 - INFO - training.fm_trainer - Step 870/210000 (0.41%): loss=4.8361, lr=1.62e-05, step_time=1821.5ms, ETA 4d 12h
12/01/2025 20:58:49 - INFO - training.fm_trainer - Step 880/210000 (0.42%): loss=10.6722, lr=1.62e-05, step_time=1861.6ms, ETA 4d 12h
12/01/2025 20:59:08 - INFO - training.fm_trainer - Step 890/210000 (0.42%): loss=3.9047, lr=1.62e-05, step_time=1859.4ms, ETA 4d 12h
12/01/2025 20:59:26 - INFO - training.fm_trainer - Step 900/210000 (0.43%): loss=8.6361, lr=1.68e-05, step_time=1836.5ms, ETA 4d 12h
12/01/2025 20:59:45 - INFO - training.fm_trainer - Step 910/210000 (0.43%): loss=0.4936, lr=1.68e-05, step_time=1880.0ms, ETA 4d 12h
12/01/2025 21:00:03 - INFO - training.fm_trainer - Step 920/210000 (0.44%): loss=1.6380, lr=1.68e-05, step_time=1839.2ms, ETA 4d 12h
12/01/2025 21:00:22 - INFO - training.fm_trainer - Step 930/210000 (0.44%): loss=5.6531, lr=1.74e-05, step_time=1843.4ms, ETA 4d 11h
12/01/2025 21:00:40 - INFO - training.fm_trainer - Step 940/210000 (0.45%): loss=6.4095, lr=1.74e-05, step_time=1855.8ms, ETA 4d 11h
12/01/2025 21:00:59 - INFO - training.fm_trainer - Step 950/210000 (0.45%): loss=2.4375, lr=1.74e-05, step_time=2162.4ms, ETA 4d 13h
12/01/2025 21:01:18 - INFO - training.fm_trainer - Step 960/210000 (0.46%): loss=7.9551, lr=1.80e-05, step_time=1845.4ms, ETA 4d 13h
12/01/2025 21:01:36 - INFO - training.fm_trainer - Step 970/210000 (0.46%): loss=5.4883, lr=1.80e-05, step_time=1932.6ms, ETA 4d 13h
12/01/2025 21:01:55 - INFO - training.fm_trainer - Step 980/210000 (0.47%): loss=0.8624, lr=1.80e-05, step_time=1841.6ms, ETA 4d 13h
12/01/2025 21:02:13 - INFO - training.fm_trainer - Step 990/210000 (0.47%): loss=1.2998, lr=1.80e-05, step_time=1820.0ms, ETA 4d 13h
12/01/2025 21:02:31 - INFO - training.fm_trainer - Step 1000/210000 (0.48%): loss=7.4380, lr=1.86e-05, step_time=1839.2ms, ETA 4d 12h
12/01/2025 21:02:31 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-1000
12/01/2025 21:02:31 - INFO - accelerate.accelerator - Saving FSDP model
12/01/2025 21:02:33 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-1000/pytorch_model_fsdp_0
12/01/2025 21:02:40 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-1000/pytorch_model_fsdp_0
12/01/2025 21:02:40 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-1000
12/01/2025 21:02:40 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/01/2025 21:02:42 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-1000/optimizer_0
12/01/2025 21:02:58 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-1000/optimizer_0
12/01/2025 21:02:58 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-1000
12/01/2025 21:02:58 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-1000/scheduler.bin
12/01/2025 21:02:58 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-1000/sampler.bin
12/01/2025 21:02:58 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-1000/random_states_0.pkl
12/01/2025 21:02:58 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-1000
12/01/2025 21:03:17 - INFO - training.fm_trainer - Step 1010/210000 (0.48%): loss=14.9467, lr=1.86e-05, step_time=1811.5ms, ETA 4d 12h
12/01/2025 21:03:36 - INFO - training.fm_trainer - Step 1020/210000 (0.49%): loss=1.4080, lr=1.86e-05, step_time=1936.6ms, ETA 4d 12h
12/01/2025 21:03:54 - INFO - training.fm_trainer - Step 1030/210000 (0.49%): loss=7.7984, lr=1.92e-05, step_time=1870.7ms, ETA 4d 12h
12/01/2025 21:04:13 - INFO - training.fm_trainer - Step 1040/210000 (0.50%): loss=2.1418, lr=1.92e-05, step_time=1826.6ms, ETA 4d 12h
12/01/2025 21:04:32 - INFO - training.fm_trainer - Step 1050/210000 (0.50%): loss=2.1077, lr=1.92e-05, step_time=1828.8ms, ETA 4d 12h
12/01/2025 21:04:50 - INFO - training.fm_trainer - Step 1060/210000 (0.50%): loss=8.9152, lr=1.98e-05, step_time=1847.5ms, ETA 4d 12h
12/01/2025 21:05:08 - INFO - training.fm_trainer - Step 1070/210000 (0.51%): loss=3.5995, lr=1.98e-05, step_time=1849.2ms, ETA 4d 12h
12/01/2025 21:05:27 - INFO - training.fm_trainer - Step 1080/210000 (0.51%): loss=1.2021, lr=1.98e-05, step_time=1847.5ms, ETA 4d 11h
12/01/2025 21:05:45 - INFO - training.fm_trainer - Step 1090/210000 (0.52%): loss=13.0889, lr=2.04e-05, step_time=1832.5ms, ETA 4d 11h
12/01/2025 21:06:04 - INFO - training.fm_trainer - Step 1100/210000 (0.52%): loss=4.3577, lr=2.04e-05, step_time=1838.2ms, ETA 4d 11h
12/01/2025 21:06:23 - INFO - training.fm_trainer - Step 1110/210000 (0.53%): loss=0.7799, lr=2.04e-05, step_time=1811.6ms, ETA 4d 11h
12/01/2025 21:06:41 - INFO - training.fm_trainer - Step 1120/210000 (0.53%): loss=2.7071, lr=2.10e-05, step_time=1806.6ms, ETA 4d 11h
12/01/2025 21:07:00 - INFO - training.fm_trainer - Step 1130/210000 (0.54%): loss=0.9708, lr=2.10e-05, step_time=1819.9ms, ETA 4d 11h
12/01/2025 21:07:18 - INFO - training.fm_trainer - Step 1140/210000 (0.54%): loss=4.4005, lr=2.10e-05, step_time=1835.5ms, ETA 4d 10h
12/01/2025 21:07:37 - INFO - training.fm_trainer - Step 1150/210000 (0.55%): loss=0.9712, lr=2.10e-05, step_time=1823.2ms, ETA 4d 10h
12/01/2025 21:07:55 - INFO - training.fm_trainer - Step 1160/210000 (0.55%): loss=8.6580, lr=2.16e-05, step_time=1837.6ms, ETA 4d 10h
12/01/2025 21:08:13 - INFO - training.fm_trainer - Step 1170/210000 (0.56%): loss=2.5491, lr=2.16e-05, step_time=1826.0ms, ETA 4d 10h
12/01/2025 21:08:32 - INFO - training.fm_trainer - Step 1180/210000 (0.56%): loss=1.0914, lr=2.16e-05, step_time=1823.0ms, ETA 4d 10h
12/01/2025 21:08:51 - INFO - training.fm_trainer - Step 1190/210000 (0.57%): loss=5.2492, lr=2.22e-05, step_time=1879.0ms, ETA 4d 10h
12/01/2025 21:09:09 - INFO - training.fm_trainer - Step 1200/210000 (0.57%): loss=13.2226, lr=2.22e-05, step_time=1827.5ms, ETA 4d 10h
12/01/2025 21:09:28 - INFO - training.fm_trainer - Step 1210/210000 (0.58%): loss=2.1514, lr=2.22e-05, step_time=1842.1ms, ETA 4d 10h
12/01/2025 21:09:46 - INFO - training.fm_trainer - Step 1220/210000 (0.58%): loss=17.7805, lr=2.28e-05, step_time=1844.3ms, ETA 4d 10h
12/01/2025 21:10:04 - INFO - training.fm_trainer - Step 1230/210000 (0.59%): loss=0.7091, lr=2.28e-05, step_time=1846.6ms, ETA 4d 10h
12/01/2025 21:10:23 - INFO - training.fm_trainer - Step 1240/210000 (0.59%): loss=16.5588, lr=2.28e-05, step_time=1830.8ms, ETA 4d 10h
12/01/2025 21:10:42 - INFO - training.fm_trainer - Step 1250/210000 (0.60%): loss=1.0394, lr=2.34e-05, step_time=1866.2ms, ETA 4d 10h
12/01/2025 21:11:00 - INFO - training.fm_trainer - Step 1260/210000 (0.60%): loss=5.6913, lr=2.34e-05, step_time=1868.9ms, ETA 4d 11h
12/01/2025 21:11:18 - INFO - training.fm_trainer - Step 1270/210000 (0.60%): loss=0.7858, lr=2.34e-05, step_time=1856.7ms, ETA 4d 11h
12/01/2025 21:11:38 - INFO - training.fm_trainer - Step 1280/210000 (0.61%): loss=6.6247, lr=2.40e-05, step_time=1782.6ms, ETA 4d 10h
12/01/2025 21:11:57 - INFO - training.fm_trainer - Step 1290/210000 (0.61%): loss=3.5296, lr=2.40e-05, step_time=2095.5ms, ETA 4d 12h
12/01/2025 21:12:15 - INFO - training.fm_trainer - Step 1300/210000 (0.62%): loss=1.5856, lr=2.40e-05, step_time=1988.8ms, ETA 4d 12h
12/01/2025 21:12:34 - INFO - training.fm_trainer - Step 1310/210000 (0.62%): loss=0.9335, lr=2.40e-05, step_time=1825.3ms, ETA 4d 12h
12/01/2025 21:12:52 - INFO - training.fm_trainer - Step 1320/210000 (0.63%): loss=8.7769, lr=2.46e-05, step_time=1870.2ms, ETA 4d 12h
12/01/2025 21:13:11 - INFO - training.fm_trainer - Step 1330/210000 (0.63%): loss=1.3825, lr=2.46e-05, step_time=1881.5ms, ETA 4d 12h
12/01/2025 21:13:29 - INFO - training.fm_trainer - Step 1340/210000 (0.64%): loss=11.8172, lr=2.46e-05, step_time=1870.5ms, ETA 4d 12h
12/01/2025 21:13:48 - INFO - training.fm_trainer - Step 1350/210000 (0.64%): loss=9.6800, lr=2.52e-05, step_time=1863.9ms, ETA 4d 12h
12/01/2025 21:14:06 - INFO - training.fm_trainer - Step 1360/210000 (0.65%): loss=7.5207, lr=2.52e-05, step_time=1837.0ms, ETA 4d 12h
12/01/2025 21:14:25 - INFO - training.fm_trainer - Step 1370/210000 (0.65%): loss=13.1474, lr=2.52e-05, step_time=1825.3ms, ETA 4d 12h
12/01/2025 21:14:44 - INFO - training.fm_trainer - Step 1380/210000 (0.66%): loss=5.4545, lr=2.58e-05, step_time=1822.7ms, ETA 4d 11h
12/01/2025 21:15:02 - INFO - training.fm_trainer - Step 1390/210000 (0.66%): loss=9.2839, lr=2.58e-05, step_time=1844.9ms, ETA 4d 11h
12/01/2025 21:15:20 - INFO - training.fm_trainer - Step 1400/210000 (0.67%): loss=11.2906, lr=2.58e-05, step_time=1837.4ms, ETA 4d 11h
12/01/2025 21:15:39 - INFO - training.fm_trainer - Step 1410/210000 (0.67%): loss=3.2511, lr=2.64e-05, step_time=1873.4ms, ETA 4d 11h
12/01/2025 21:15:57 - INFO - training.fm_trainer - Step 1420/210000 (0.68%): loss=1.4708, lr=2.64e-05, step_time=1826.5ms, ETA 4d 11h
12/01/2025 21:16:16 - INFO - training.fm_trainer - Step 1430/210000 (0.68%): loss=7.4778, lr=2.64e-05, step_time=1833.1ms, ETA 4d 11h
12/01/2025 21:16:34 - INFO - training.fm_trainer - Step 1440/210000 (0.69%): loss=3.0696, lr=2.70e-05, step_time=1837.7ms, ETA 4d 11h
12/01/2025 21:16:53 - INFO - training.fm_trainer - Step 1450/210000 (0.69%): loss=1.1445, lr=2.70e-05, step_time=1832.4ms, ETA 4d 11h
12/01/2025 21:17:11 - INFO - training.fm_trainer - Step 1460/210000 (0.70%): loss=4.5215, lr=2.70e-05, step_time=1821.8ms, ETA 4d 10h
12/01/2025 21:17:30 - INFO - training.fm_trainer - Step 1470/210000 (0.70%): loss=8.9715, lr=2.70e-05, step_time=1810.9ms, ETA 4d 10h
12/01/2025 21:17:49 - INFO - training.fm_trainer - Step 1480/210000 (0.70%): loss=10.8470, lr=2.76e-05, step_time=1829.9ms, ETA 4d 10h
12/01/2025 21:18:07 - INFO - training.fm_trainer - Step 1490/210000 (0.71%): loss=10.8821, lr=2.76e-05, step_time=1826.6ms, ETA 4d 10h
12/01/2025 21:18:26 - INFO - training.fm_trainer - Step 1500/210000 (0.71%): loss=6.0053, lr=2.76e-05, step_time=1820.5ms, ETA 4d 10h
12/01/2025 21:18:26 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-1500
12/01/2025 21:18:26 - INFO - accelerate.accelerator - Saving FSDP model
12/01/2025 21:18:27 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-1500/pytorch_model_fsdp_0
12/01/2025 21:18:35 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-1500/pytorch_model_fsdp_0
12/01/2025 21:18:35 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-1500
12/01/2025 21:18:35 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/01/2025 21:18:37 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-1500/optimizer_0
12/01/2025 21:18:53 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-1500/optimizer_0
12/01/2025 21:18:53 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-1500
12/01/2025 21:18:53 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-1500/scheduler.bin
12/01/2025 21:18:53 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-1500/sampler.bin
12/01/2025 21:18:53 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-1500/random_states_0.pkl
12/01/2025 21:18:53 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-1500
12/01/2025 21:19:12 - INFO - training.fm_trainer - Step 1510/210000 (0.72%): loss=10.6017, lr=2.82e-05, step_time=1825.6ms, ETA 4d 10h
12/01/2025 21:19:30 - INFO - training.fm_trainer - Step 1520/210000 (0.72%): loss=6.9179, lr=2.82e-05, step_time=1871.7ms, ETA 4d 10h
12/01/2025 21:19:49 - INFO - training.fm_trainer - Step 1530/210000 (0.73%): loss=1.1887, lr=2.82e-05, step_time=1841.6ms, ETA 4d 10h
12/01/2025 21:20:07 - INFO - training.fm_trainer - Step 1540/210000 (0.73%): loss=5.1925, lr=2.88e-05, step_time=1887.6ms, ETA 4d 10h
12/01/2025 21:20:26 - INFO - training.fm_trainer - Step 1550/210000 (0.74%): loss=9.6093, lr=2.88e-05, step_time=1814.7ms, ETA 4d 10h
12/01/2025 21:20:45 - INFO - training.fm_trainer - Step 1560/210000 (0.74%): loss=4.2216, lr=2.88e-05, step_time=1804.5ms, ETA 4d 10h
12/01/2025 21:21:04 - INFO - training.fm_trainer - Step 1570/210000 (0.75%): loss=7.6423, lr=2.94e-05, step_time=2025.8ms, ETA 4d 11h
12/01/2025 21:21:22 - INFO - training.fm_trainer - Step 1580/210000 (0.75%): loss=11.5734, lr=2.94e-05, step_time=1881.7ms, ETA 4d 11h
12/01/2025 21:21:41 - INFO - training.fm_trainer - Step 1590/210000 (0.76%): loss=5.9316, lr=2.94e-05, step_time=1849.4ms, ETA 4d 11h
12/01/2025 21:21:59 - INFO - training.fm_trainer - Step 1600/210000 (0.76%): loss=3.5768, lr=3.00e-05, step_time=1865.9ms, ETA 4d 11h
12/01/2025 21:22:18 - INFO - training.fm_trainer - Step 1610/210000 (0.77%): loss=11.0549, lr=3.00e-05, step_time=1866.4ms, ETA 4d 11h
12/01/2025 21:22:37 - INFO - training.fm_trainer - Step 1620/210000 (0.77%): loss=2.9700, lr=3.00e-05, step_time=1820.2ms, ETA 4d 11h
12/01/2025 21:22:55 - INFO - training.fm_trainer - Step 1630/210000 (0.78%): loss=8.0539, lr=3.00e-05, step_time=1821.6ms, ETA 4d 11h
12/01/2025 21:23:14 - INFO - training.fm_trainer - Step 1640/210000 (0.78%): loss=5.8856, lr=3.06e-05, step_time=1833.1ms, ETA 4d 11h
12/01/2025 21:23:32 - INFO - training.fm_trainer - Step 1650/210000 (0.79%): loss=10.1595, lr=3.06e-05, step_time=1850.3ms, ETA 4d 11h
12/01/2025 21:23:51 - INFO - training.fm_trainer - Step 1660/210000 (0.79%): loss=9.7259, lr=3.06e-05, step_time=1865.5ms, ETA 4d 11h
12/01/2025 21:24:09 - INFO - training.fm_trainer - Step 1670/210000 (0.80%): loss=9.8530, lr=3.12e-05, step_time=1871.8ms, ETA 4d 11h
12/01/2025 21:24:28 - INFO - training.fm_trainer - Step 1680/210000 (0.80%): loss=2.9669, lr=3.12e-05, step_time=1825.4ms, ETA 4d 11h
12/01/2025 21:24:47 - INFO - training.fm_trainer - Step 1690/210000 (0.80%): loss=1.1914, lr=3.12e-05, step_time=1858.8ms, ETA 4d 11h
12/01/2025 21:25:05 - INFO - training.fm_trainer - Step 1700/210000 (0.81%): loss=1.8236, lr=3.18e-05, step_time=1832.5ms, ETA 4d 11h
12/01/2025 21:25:24 - INFO - training.fm_trainer - Step 1710/210000 (0.81%): loss=2.5300, lr=3.18e-05, step_time=1822.8ms, ETA 4d 10h
12/01/2025 21:25:42 - INFO - training.fm_trainer - Step 1720/210000 (0.82%): loss=14.8973, lr=3.18e-05, step_time=1832.3ms, ETA 4d 10h
12/01/2025 21:26:01 - INFO - training.fm_trainer - Step 1730/210000 (0.82%): loss=3.6727, lr=3.24e-05, step_time=1813.4ms, ETA 4d 10h
12/01/2025 21:26:20 - INFO - training.fm_trainer - Step 1740/210000 (0.83%): loss=12.4289, lr=3.24e-05, step_time=1855.5ms, ETA 4d 10h
12/01/2025 21:26:38 - INFO - training.fm_trainer - Step 1750/210000 (0.83%): loss=1.0392, lr=3.24e-05, step_time=1831.0ms, ETA 4d 10h
12/01/2025 21:26:57 - INFO - training.fm_trainer - Step 1760/210000 (0.84%): loss=2.9672, lr=3.30e-05, step_time=1765.6ms, ETA 4d 10h
12/01/2025 21:27:15 - INFO - training.fm_trainer - Step 1770/210000 (0.84%): loss=2.2265, lr=3.30e-05, step_time=1879.2ms, ETA 4d 10h
12/01/2025 21:27:34 - INFO - training.fm_trainer - Step 1780/210000 (0.85%): loss=7.0097, lr=3.30e-05, step_time=1890.7ms, ETA 4d 10h
12/01/2025 21:27:52 - INFO - training.fm_trainer - Step 1790/210000 (0.85%): loss=12.3757, lr=3.30e-05, step_time=1810.2ms, ETA 4d 10h
12/01/2025 21:28:11 - INFO - training.fm_trainer - Step 1800/210000 (0.86%): loss=0.5312, lr=3.36e-05, step_time=1823.2ms, ETA 4d 10h
12/01/2025 21:28:30 - INFO - training.fm_trainer - Step 1810/210000 (0.86%): loss=3.9135, lr=3.36e-05, step_time=1860.2ms, ETA 4d 10h
12/01/2025 21:28:48 - INFO - training.fm_trainer - Step 1820/210000 (0.87%): loss=8.9200, lr=3.36e-05, step_time=1828.3ms, ETA 4d 10h
12/01/2025 21:29:06 - INFO - training.fm_trainer - Step 1830/210000 (0.87%): loss=1.6219, lr=3.42e-05, step_time=1953.4ms, ETA 4d 11h
12/01/2025 21:29:25 - INFO - training.fm_trainer - Step 1840/210000 (0.88%): loss=9.4274, lr=3.42e-05, step_time=1880.6ms, ETA 4d 11h
12/01/2025 21:29:43 - INFO - training.fm_trainer - Step 1850/210000 (0.88%): loss=3.8346, lr=3.42e-05, step_time=1829.3ms, ETA 4d 11h
12/01/2025 21:30:02 - INFO - training.fm_trainer - Step 1860/210000 (0.89%): loss=1.2597, lr=3.48e-05, step_time=1840.9ms, ETA 4d 11h
12/01/2025 21:30:21 - INFO - training.fm_trainer - Step 1870/210000 (0.89%): loss=6.5428, lr=3.48e-05, step_time=1832.7ms, ETA 4d 10h
12/01/2025 21:30:39 - INFO - training.fm_trainer - Step 1880/210000 (0.90%): loss=1.1790, lr=3.48e-05, step_time=1832.3ms, ETA 4d 10h
12/01/2025 21:30:58 - INFO - training.fm_trainer - Step 1890/210000 (0.90%): loss=6.7575, lr=3.54e-05, step_time=1860.3ms, ETA 4d 10h
12/01/2025 21:31:16 - INFO - training.fm_trainer - Step 1900/210000 (0.90%): loss=5.2278, lr=3.54e-05, step_time=1880.8ms, ETA 4d 11h
12/01/2025 21:31:35 - INFO - training.fm_trainer - Step 1910/210000 (0.91%): loss=2.9641, lr=3.54e-05, step_time=1852.9ms, ETA 4d 11h
12/01/2025 21:31:53 - INFO - training.fm_trainer - Step 1920/210000 (0.91%): loss=0.3880, lr=3.60e-05, step_time=1791.6ms, ETA 4d 10h
12/01/2025 21:32:12 - INFO - training.fm_trainer - Step 1930/210000 (0.92%): loss=9.9847, lr=3.60e-05, step_time=1821.1ms, ETA 4d 10h
12/01/2025 21:32:30 - INFO - training.fm_trainer - Step 1940/210000 (0.92%): loss=6.6556, lr=3.60e-05, step_time=1831.7ms, ETA 4d 10h
12/01/2025 21:32:49 - INFO - training.fm_trainer - Step 1950/210000 (0.93%): loss=0.9769, lr=3.60e-05, step_time=1985.3ms, ETA 4d 11h
12/01/2025 21:33:08 - INFO - training.fm_trainer - Step 1960/210000 (0.93%): loss=13.3885, lr=3.66e-05, step_time=1831.7ms, ETA 4d 11h
12/01/2025 21:33:26 - INFO - training.fm_trainer - Step 1970/210000 (0.94%): loss=4.6304, lr=3.66e-05, step_time=1828.5ms, ETA 4d 10h
12/01/2025 21:33:45 - INFO - training.fm_trainer - Step 1980/210000 (0.94%): loss=0.5379, lr=3.66e-05, step_time=1842.8ms, ETA 4d 10h
12/01/2025 21:34:03 - INFO - training.fm_trainer - Step 1990/210000 (0.95%): loss=8.7779, lr=3.72e-05, step_time=1831.0ms, ETA 4d 10h
12/01/2025 21:34:22 - INFO - training.fm_trainer - Step 2000/210000 (0.95%): loss=0.8516, lr=3.72e-05, step_time=1853.5ms, ETA 4d 10h
12/01/2025 21:34:22 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-2000
12/01/2025 21:34:22 - INFO - accelerate.accelerator - Saving FSDP model
12/01/2025 21:34:23 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-2000/pytorch_model_fsdp_0
12/01/2025 21:34:31 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-2000/pytorch_model_fsdp_0
12/01/2025 21:34:31 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-2000
12/01/2025 21:34:31 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/01/2025 21:34:33 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-2000/optimizer_0
12/01/2025 21:34:49 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-2000/optimizer_0
12/01/2025 21:34:49 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-2000
12/01/2025 21:34:49 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-2000/scheduler.bin
12/01/2025 21:34:49 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-2000/sampler.bin
12/01/2025 21:34:49 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-2000/random_states_0.pkl
12/01/2025 21:34:49 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-2000
12/01/2025 21:35:07 - INFO - training.fm_trainer - Step 2010/210000 (0.96%): loss=4.3032, lr=3.72e-05, step_time=1848.5ms, ETA 4d 10h
12/01/2025 21:35:26 - INFO - training.fm_trainer - Step 2020/210000 (0.96%): loss=1.3251, lr=3.78e-05, step_time=1838.9ms, ETA 4d 10h
12/01/2025 21:35:45 - INFO - training.fm_trainer - Step 2030/210000 (0.97%): loss=1.2243, lr=3.78e-05, step_time=1864.3ms, ETA 4d 10h
12/01/2025 21:36:03 - INFO - training.fm_trainer - Step 2040/210000 (0.97%): loss=1.5580, lr=3.78e-05, step_time=1849.8ms, ETA 4d 10h
12/01/2025 21:36:21 - INFO - training.fm_trainer - Step 2050/210000 (0.98%): loss=10.7977, lr=3.84e-05, step_time=1874.4ms, ETA 4d 10h
12/01/2025 21:36:40 - INFO - training.fm_trainer - Step 2060/210000 (0.98%): loss=0.5239, lr=3.84e-05, step_time=1909.6ms, ETA 4d 11h
12/01/2025 21:36:58 - INFO - training.fm_trainer - Step 2070/210000 (0.99%): loss=3.0739, lr=3.84e-05, step_time=1840.7ms, ETA 4d 11h
12/01/2025 21:37:17 - INFO - training.fm_trainer - Step 2080/210000 (0.99%): loss=2.9685, lr=3.90e-05, step_time=1779.7ms, ETA 4d 10h
12/01/2025 21:37:36 - INFO - training.fm_trainer - Step 2090/210000 (1.00%): loss=12.8971, lr=3.90e-05, step_time=1813.4ms, ETA 4d 10h
12/01/2025 21:37:54 - INFO - training.fm_trainer - Step 2100/210000 (1.00%): loss=1.8930, lr=3.90e-05, step_time=1847.7ms, ETA 4d 10h
12/01/2025 21:38:13 - INFO - training.fm_trainer - Step 2110/210000 (1.00%): loss=1.2302, lr=3.90e-05, step_time=1842.3ms, ETA 4d 10h
12/01/2025 21:38:32 - INFO - training.fm_trainer - Step 2120/210000 (1.01%): loss=4.5255, lr=3.96e-05, step_time=1838.3ms, ETA 4d 10h
12/01/2025 21:38:50 - INFO - training.fm_trainer - Step 2130/210000 (1.01%): loss=4.7435, lr=3.96e-05, step_time=1854.1ms, ETA 4d 10h
12/01/2025 21:39:09 - INFO - training.fm_trainer - Step 2140/210000 (1.02%): loss=1.3645, lr=3.96e-05, step_time=1931.7ms, ETA 4d 11h
12/01/2025 21:39:27 - INFO - training.fm_trainer - Step 2150/210000 (1.02%): loss=6.1357, lr=4.02e-05, step_time=1848.2ms, ETA 4d 11h
12/01/2025 21:39:46 - INFO - training.fm_trainer - Step 2160/210000 (1.03%): loss=2.0065, lr=4.02e-05, step_time=1842.8ms, ETA 4d 10h
12/01/2025 21:40:04 - INFO - training.fm_trainer - Step 2170/210000 (1.03%): loss=0.6058, lr=4.02e-05, step_time=1833.6ms, ETA 4d 10h
12/01/2025 21:40:23 - INFO - training.fm_trainer - Step 2180/210000 (1.04%): loss=0.7731, lr=4.08e-05, step_time=1846.4ms, ETA 4d 10h
12/01/2025 21:40:41 - INFO - training.fm_trainer - Step 2190/210000 (1.04%): loss=1.5127, lr=4.08e-05, step_time=1884.6ms, ETA 4d 10h
12/01/2025 21:41:00 - INFO - training.fm_trainer - Step 2200/210000 (1.05%): loss=1.3472, lr=4.08e-05, step_time=1822.0ms, ETA 4d 10h
12/01/2025 21:41:19 - INFO - training.fm_trainer - Step 2210/210000 (1.05%): loss=9.1680, lr=4.14e-05, step_time=1830.8ms, ETA 4d 10h
12/01/2025 21:41:37 - INFO - training.fm_trainer - Step 2220/210000 (1.06%): loss=2.0520, lr=4.14e-05, step_time=1818.1ms, ETA 4d 10h
12/01/2025 21:41:56 - INFO - training.fm_trainer - Step 2230/210000 (1.06%): loss=5.7710, lr=4.14e-05, step_time=1890.0ms, ETA 4d 10h
12/01/2025 21:42:14 - INFO - training.fm_trainer - Step 2240/210000 (1.07%): loss=0.6491, lr=4.20e-05, step_time=1820.1ms, ETA 4d 10h
12/01/2025 21:42:32 - INFO - training.fm_trainer - Step 2250/210000 (1.07%): loss=1.5646, lr=4.20e-05, step_time=1837.0ms, ETA 4d 10h
12/01/2025 21:42:51 - INFO - training.fm_trainer - Step 2260/210000 (1.08%): loss=0.7466, lr=4.20e-05, step_time=1850.8ms, ETA 4d 10h
12/01/2025 21:43:10 - INFO - training.fm_trainer - Step 2270/210000 (1.08%): loss=7.3514, lr=4.20e-05, step_time=1828.8ms, ETA 4d 10h
12/01/2025 21:43:29 - INFO - training.fm_trainer - Step 2280/210000 (1.09%): loss=0.2974, lr=4.26e-05, step_time=1877.9ms, ETA 4d 10h
12/01/2025 21:43:47 - INFO - training.fm_trainer - Step 2290/210000 (1.09%): loss=1.0903, lr=4.26e-05, step_time=1875.7ms, ETA 4d 10h
12/01/2025 21:44:06 - INFO - training.fm_trainer - Step 2300/210000 (1.10%): loss=11.0610, lr=4.26e-05, step_time=1941.5ms, ETA 4d 11h
12/01/2025 21:44:25 - INFO - training.fm_trainer - Step 2310/210000 (1.10%): loss=11.6986, lr=4.32e-05, step_time=1923.4ms, ETA 4d 11h
12/01/2025 21:44:48 - INFO - training.fm_trainer - Step 2320/210000 (1.10%): loss=1.2526, lr=4.32e-05, step_time=1871.5ms, ETA 4d 11h
12/01/2025 21:45:07 - INFO - training.fm_trainer - Step 2330/210000 (1.11%): loss=1.0729, lr=4.32e-05, step_time=1830.6ms, ETA 4d 11h
12/01/2025 21:45:25 - INFO - training.fm_trainer - Step 2340/210000 (1.11%): loss=1.4407, lr=4.38e-05, step_time=1825.2ms, ETA 4d 11h
12/01/2025 21:45:44 - INFO - training.fm_trainer - Step 2350/210000 (1.12%): loss=2.2529, lr=4.38e-05, step_time=1849.5ms, ETA 4d 11h
12/01/2025 21:46:02 - INFO - training.fm_trainer - Step 2360/210000 (1.12%): loss=4.0186, lr=4.38e-05, step_time=1842.1ms, ETA 4d 11h
12/01/2025 21:46:21 - INFO - training.fm_trainer - Step 2370/210000 (1.13%): loss=1.8616, lr=4.44e-05, step_time=1815.8ms, ETA 4d 10h
12/01/2025 21:46:39 - INFO - training.fm_trainer - Step 2380/210000 (1.13%): loss=1.6261, lr=4.44e-05, step_time=1871.5ms, ETA 4d 10h
12/01/2025 21:46:58 - INFO - training.fm_trainer - Step 2390/210000 (1.14%): loss=0.6675, lr=4.44e-05, step_time=1830.5ms, ETA 4d 10h
12/01/2025 21:47:17 - INFO - training.fm_trainer - Step 2400/210000 (1.14%): loss=10.7716, lr=4.50e-05, step_time=1815.7ms, ETA 4d 10h
12/01/2025 21:47:35 - INFO - training.fm_trainer - Step 2410/210000 (1.15%): loss=1.3910, lr=4.50e-05, step_time=1837.7ms, ETA 4d 10h
12/01/2025 21:47:53 - INFO - training.fm_trainer - Step 2420/210000 (1.15%): loss=1.8176, lr=4.50e-05, step_time=1852.9ms, ETA 4d 10h
12/01/2025 21:48:12 - INFO - training.fm_trainer - Step 2430/210000 (1.16%): loss=1.9424, lr=4.50e-05, step_time=1852.9ms, ETA 4d 10h
12/01/2025 21:48:30 - INFO - training.fm_trainer - Step 2440/210000 (1.16%): loss=0.7741, lr=4.56e-05, step_time=1851.1ms, ETA 4d 10h
12/01/2025 21:48:49 - INFO - training.fm_trainer - Step 2450/210000 (1.17%): loss=0.8923, lr=4.56e-05, step_time=1857.0ms, ETA 4d 10h
12/01/2025 21:49:08 - INFO - training.fm_trainer - Step 2460/210000 (1.17%): loss=3.5971, lr=4.56e-05, step_time=1837.0ms, ETA 4d 10h
12/01/2025 21:49:26 - INFO - training.fm_trainer - Step 2470/210000 (1.18%): loss=9.0470, lr=4.62e-05, step_time=1846.4ms, ETA 4d 10h
12/01/2025 21:49:45 - INFO - training.fm_trainer - Step 2480/210000 (1.18%): loss=4.0873, lr=4.62e-05, step_time=1848.0ms, ETA 4d 10h
12/01/2025 21:50:04 - INFO - training.fm_trainer - Step 2490/210000 (1.19%): loss=0.8993, lr=4.62e-05, step_time=1810.3ms, ETA 4d 10h
12/01/2025 21:50:23 - INFO - training.fm_trainer - Step 2500/210000 (1.19%): loss=6.5179, lr=4.68e-05, step_time=1810.7ms, ETA 4d 10h
12/01/2025 21:50:23 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-2500
12/01/2025 21:50:23 - INFO - accelerate.accelerator - Saving FSDP model
12/01/2025 21:50:24 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-2500/pytorch_model_fsdp_0
12/01/2025 21:50:32 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-2500/pytorch_model_fsdp_0
12/01/2025 21:50:32 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-2500
12/01/2025 21:50:32 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/01/2025 21:50:34 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-2500/optimizer_0
12/01/2025 21:50:50 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-2500/optimizer_0
12/01/2025 21:50:50 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-2500
12/01/2025 21:50:50 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-2500/scheduler.bin
12/01/2025 21:50:50 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-2500/sampler.bin
12/01/2025 21:50:50 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-2500/random_states_0.pkl
12/01/2025 21:50:50 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-2500
12/01/2025 21:51:09 - INFO - training.fm_trainer - Step 2510/210000 (1.20%): loss=1.8016, lr=4.68e-05, step_time=1833.5ms, ETA 4d 10h
12/01/2025 21:51:27 - INFO - training.fm_trainer - Step 2520/210000 (1.20%): loss=2.4556, lr=4.68e-05, step_time=1874.4ms, ETA 4d 10h
12/01/2025 21:51:46 - INFO - training.fm_trainer - Step 2530/210000 (1.20%): loss=1.5587, lr=4.74e-05, step_time=1852.8ms, ETA 4d 10h
12/01/2025 21:52:04 - INFO - training.fm_trainer - Step 2540/210000 (1.21%): loss=1.6910, lr=4.74e-05, step_time=1854.1ms, ETA 4d 10h
12/01/2025 21:52:23 - INFO - training.fm_trainer - Step 2550/210000 (1.21%): loss=2.4569, lr=4.74e-05, step_time=1844.3ms, ETA 4d 10h
12/01/2025 21:52:41 - INFO - training.fm_trainer - Step 2560/210000 (1.22%): loss=5.0386, lr=4.80e-05, step_time=1808.1ms, ETA 4d 10h
12/01/2025 21:53:00 - INFO - training.fm_trainer - Step 2570/210000 (1.22%): loss=6.0246, lr=4.80e-05, step_time=1898.2ms, ETA 4d 10h
12/01/2025 21:53:19 - INFO - training.fm_trainer - Step 2580/210000 (1.23%): loss=8.1105, lr=4.80e-05, step_time=1823.9ms, ETA 4d 10h
12/01/2025 21:53:38 - INFO - training.fm_trainer - Step 2590/210000 (1.23%): loss=1.5699, lr=4.80e-05, step_time=1815.6ms, ETA 4d 10h
12/01/2025 21:53:57 - INFO - training.fm_trainer - Step 2600/210000 (1.24%): loss=5.0667, lr=4.86e-05, step_time=1880.8ms, ETA 4d 10h
12/01/2025 21:54:15 - INFO - training.fm_trainer - Step 2610/210000 (1.24%): loss=2.7309, lr=4.86e-05, step_time=1841.6ms, ETA 4d 10h
12/01/2025 21:54:34 - INFO - training.fm_trainer - Step 2620/210000 (1.25%): loss=5.4238, lr=4.86e-05, step_time=1855.8ms, ETA 4d 10h
12/01/2025 21:54:53 - INFO - training.fm_trainer - Step 2630/210000 (1.25%): loss=8.2614, lr=4.92e-05, step_time=1837.6ms, ETA 4d 10h
12/01/2025 21:55:11 - INFO - training.fm_trainer - Step 2640/210000 (1.26%): loss=3.3783, lr=4.92e-05, step_time=1827.9ms, ETA 4d 10h
12/01/2025 21:55:30 - INFO - training.fm_trainer - Step 2650/210000 (1.26%): loss=4.8685, lr=4.92e-05, step_time=1871.2ms, ETA 4d 10h
12/01/2025 21:55:49 - INFO - training.fm_trainer - Step 2660/210000 (1.27%): loss=9.6903, lr=4.98e-05, step_time=1851.8ms, ETA 4d 10h
12/01/2025 21:56:07 - INFO - training.fm_trainer - Step 2670/210000 (1.27%): loss=5.2860, lr=4.98e-05, step_time=1836.3ms, ETA 4d 10h
12/01/2025 21:56:26 - INFO - training.fm_trainer - Step 2680/210000 (1.28%): loss=1.1385, lr=4.98e-05, step_time=1858.0ms, ETA 4d 10h
12/01/2025 21:56:44 - INFO - training.fm_trainer - Step 2690/210000 (1.28%): loss=0.6636, lr=5.04e-05, step_time=1857.3ms, ETA 4d 10h
12/01/2025 21:57:03 - INFO - training.fm_trainer - Step 2700/210000 (1.29%): loss=9.3600, lr=5.04e-05, step_time=1961.0ms, ETA 4d 11h
12/01/2025 21:57:22 - INFO - training.fm_trainer - Step 2710/210000 (1.29%): loss=4.5178, lr=5.04e-05, step_time=1841.4ms, ETA 4d 10h
12/01/2025 21:57:40 - INFO - training.fm_trainer - Step 2720/210000 (1.30%): loss=5.4380, lr=5.10e-05, step_time=1816.9ms, ETA 4d 10h
12/01/2025 21:57:59 - INFO - training.fm_trainer - Step 2730/210000 (1.30%): loss=2.7899, lr=5.10e-05, step_time=2062.4ms, ETA 4d 11h
12/01/2025 21:58:18 - INFO - training.fm_trainer - Step 2740/210000 (1.30%): loss=5.6412, lr=5.10e-05, step_time=1879.0ms, ETA 4d 11h
12/01/2025 21:58:36 - INFO - training.fm_trainer - Step 2750/210000 (1.31%): loss=0.8504, lr=5.10e-05, step_time=1858.8ms, ETA 4d 11h
12/01/2025 21:58:55 - INFO - training.fm_trainer - Step 2760/210000 (1.31%): loss=2.8977, lr=5.16e-05, step_time=1838.1ms, ETA 4d 11h
12/01/2025 21:59:14 - INFO - training.fm_trainer - Step 2770/210000 (1.32%): loss=8.2810, lr=5.16e-05, step_time=1837.9ms, ETA 4d 11h
12/01/2025 21:59:32 - INFO - training.fm_trainer - Step 2780/210000 (1.32%): loss=2.2494, lr=5.16e-05, step_time=1855.9ms, ETA 4d 11h
12/01/2025 21:59:51 - INFO - training.fm_trainer - Step 2790/210000 (1.33%): loss=2.0218, lr=5.22e-05, step_time=1851.7ms, ETA 4d 11h
12/01/2025 22:00:09 - INFO - training.fm_trainer - Step 2800/210000 (1.33%): loss=5.1914, lr=5.22e-05, step_time=1832.9ms, ETA 4d 11h
12/01/2025 22:00:28 - INFO - training.fm_trainer - Step 2810/210000 (1.34%): loss=1.7760, lr=5.22e-05, step_time=2057.1ms, ETA 4d 12h
12/01/2025 22:00:47 - INFO - training.fm_trainer - Step 2820/210000 (1.34%): loss=1.8965, lr=5.28e-05, step_time=1851.7ms, ETA 4d 12h
12/01/2025 22:01:05 - INFO - training.fm_trainer - Step 2830/210000 (1.35%): loss=0.9329, lr=5.28e-05, step_time=1884.1ms, ETA 4d 12h
12/01/2025 22:01:24 - INFO - training.fm_trainer - Step 2840/210000 (1.35%): loss=3.0667, lr=5.28e-05, step_time=1839.5ms, ETA 4d 11h
12/01/2025 22:01:43 - INFO - training.fm_trainer - Step 2850/210000 (1.36%): loss=3.6274, lr=5.34e-05, step_time=1909.2ms, ETA 4d 12h
12/01/2025 22:02:01 - INFO - training.fm_trainer - Step 2860/210000 (1.36%): loss=9.7289, lr=5.34e-05, step_time=1898.6ms, ETA 4d 12h
12/01/2025 22:02:20 - INFO - training.fm_trainer - Step 2870/210000 (1.37%): loss=0.9640, lr=5.34e-05, step_time=1825.1ms, ETA 4d 11h
12/01/2025 22:02:39 - INFO - training.fm_trainer - Step 2880/210000 (1.37%): loss=2.4138, lr=5.40e-05, step_time=1809.4ms, ETA 4d 11h
12/01/2025 22:02:57 - INFO - training.fm_trainer - Step 2890/210000 (1.38%): loss=5.9425, lr=5.40e-05, step_time=1895.4ms, ETA 4d 11h
12/01/2025 22:03:16 - INFO - training.fm_trainer - Step 2900/210000 (1.38%): loss=0.8229, lr=5.40e-05, step_time=1838.2ms, ETA 4d 11h
12/01/2025 22:03:35 - INFO - training.fm_trainer - Step 2910/210000 (1.39%): loss=0.3494, lr=5.40e-05, step_time=1849.7ms, ETA 4d 11h
12/01/2025 22:03:53 - INFO - training.fm_trainer - Step 2920/210000 (1.39%): loss=5.7400, lr=5.46e-05, step_time=1823.7ms, ETA 4d 11h
12/01/2025 22:04:12 - INFO - training.fm_trainer - Step 2930/210000 (1.40%): loss=3.7812, lr=5.46e-05, step_time=1837.2ms, ETA 4d 10h
12/01/2025 22:04:30 - INFO - training.fm_trainer - Step 2940/210000 (1.40%): loss=6.4472, lr=5.46e-05, step_time=1958.8ms, ETA 4d 11h
12/01/2025 22:04:49 - INFO - training.fm_trainer - Step 2950/210000 (1.40%): loss=14.0416, lr=5.52e-05, step_time=1845.2ms, ETA 4d 11h
12/01/2025 22:05:07 - INFO - training.fm_trainer - Step 2960/210000 (1.41%): loss=7.5106, lr=5.52e-05, step_time=1861.6ms, ETA 4d 11h
12/01/2025 22:05:26 - INFO - training.fm_trainer - Step 2970/210000 (1.41%): loss=0.6867, lr=5.52e-05, step_time=1829.6ms, ETA 4d 11h
12/01/2025 22:05:44 - INFO - training.fm_trainer - Step 2980/210000 (1.42%): loss=0.9928, lr=5.58e-05, step_time=1869.8ms, ETA 4d 11h
12/01/2025 22:06:03 - INFO - training.fm_trainer - Step 2990/210000 (1.42%): loss=0.5239, lr=5.58e-05, step_time=1846.5ms, ETA 4d 11h
12/01/2025 22:06:22 - INFO - training.fm_trainer - Step 3000/210000 (1.43%): loss=0.6512, lr=5.58e-05, step_time=1886.0ms, ETA 4d 11h
12/01/2025 22:06:22 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-3000
12/01/2025 22:06:22 - INFO - accelerate.accelerator - Saving FSDP model
12/01/2025 22:06:23 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-3000/pytorch_model_fsdp_0
12/01/2025 22:06:30 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-3000/pytorch_model_fsdp_0
12/01/2025 22:06:31 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-3000
12/01/2025 22:06:31 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/01/2025 22:06:33 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-3000/optimizer_0
12/01/2025 22:06:49 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-3000/optimizer_0
12/01/2025 22:06:49 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-3000
12/01/2025 22:06:49 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-3000/scheduler.bin
12/01/2025 22:06:49 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-3000/sampler.bin
12/01/2025 22:06:49 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-3000/random_states_0.pkl
12/01/2025 22:06:49 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-3000
12/01/2025 22:07:07 - INFO - training.fm_trainer - Step 3010/210000 (1.43%): loss=1.5357, lr=5.64e-05, step_time=1837.4ms, ETA 4d 11h
12/01/2025 22:07:26 - INFO - training.fm_trainer - Step 3020/210000 (1.44%): loss=4.0585, lr=5.64e-05, step_time=1819.4ms, ETA 4d 10h
12/01/2025 22:07:45 - INFO - training.fm_trainer - Step 3030/210000 (1.44%): loss=9.4600, lr=5.64e-05, step_time=1826.7ms, ETA 4d 10h
12/01/2025 22:08:03 - INFO - training.fm_trainer - Step 3040/210000 (1.45%): loss=0.9912, lr=5.70e-05, step_time=2067.5ms, ETA 4d 11h
12/01/2025 22:08:22 - INFO - training.fm_trainer - Step 3050/210000 (1.45%): loss=5.5686, lr=5.70e-05, step_time=1835.8ms, ETA 4d 11h
12/01/2025 22:08:40 - INFO - training.fm_trainer - Step 3060/210000 (1.46%): loss=10.8984, lr=5.70e-05, step_time=1844.5ms, ETA 4d 11h
12/01/2025 22:08:59 - INFO - training.fm_trainer - Step 3070/210000 (1.46%): loss=2.0012, lr=5.70e-05, step_time=1854.3ms, ETA 4d 11h
12/01/2025 22:09:18 - INFO - training.fm_trainer - Step 3080/210000 (1.47%): loss=0.5401, lr=5.76e-05, step_time=1829.8ms, ETA 4d 11h
12/01/2025 22:09:36 - INFO - training.fm_trainer - Step 3090/210000 (1.47%): loss=1.7413, lr=5.76e-05, step_time=1817.0ms, ETA 4d 10h
12/01/2025 22:09:54 - INFO - training.fm_trainer - Step 3100/210000 (1.48%): loss=1.4107, lr=5.76e-05, step_time=1835.4ms, ETA 4d 10h
12/01/2025 22:10:14 - INFO - training.fm_trainer - Step 3110/210000 (1.48%): loss=9.6464, lr=5.82e-05, step_time=1836.1ms, ETA 4d 10h
12/01/2025 22:10:33 - INFO - training.fm_trainer - Step 3120/210000 (1.49%): loss=1.0713, lr=5.82e-05, step_time=1835.8ms, ETA 4d 10h
12/01/2025 22:10:51 - INFO - training.fm_trainer - Step 3130/210000 (1.49%): loss=6.5082, lr=5.82e-05, step_time=1830.7ms, ETA 4d 10h
12/01/2025 22:11:10 - INFO - training.fm_trainer - Step 3140/210000 (1.50%): loss=3.9172, lr=5.88e-05, step_time=1903.4ms, ETA 4d 10h
12/01/2025 22:11:29 - INFO - training.fm_trainer - Step 3150/210000 (1.50%): loss=1.7960, lr=5.88e-05, step_time=1855.7ms, ETA 4d 10h
12/01/2025 22:11:47 - INFO - training.fm_trainer - Step 3160/210000 (1.50%): loss=2.3455, lr=5.88e-05, step_time=1854.3ms, ETA 4d 10h
12/01/2025 22:12:05 - INFO - training.fm_trainer - Step 3170/210000 (1.51%): loss=4.1475, lr=5.94e-05, step_time=1851.1ms, ETA 4d 10h
12/01/2025 22:12:24 - INFO - training.fm_trainer - Step 3180/210000 (1.51%): loss=1.0456, lr=5.94e-05, step_time=1823.7ms, ETA 4d 10h
12/01/2025 22:12:43 - INFO - training.fm_trainer - Step 3190/210000 (1.52%): loss=2.7322, lr=5.94e-05, step_time=1854.5ms, ETA 4d 10h
12/01/2025 22:13:01 - INFO - training.fm_trainer - Step 3200/210000 (1.52%): loss=8.0769, lr=6.00e-05, step_time=1777.3ms, ETA 4d 9h
12/01/2025 22:13:20 - INFO - training.fm_trainer - Step 3210/210000 (1.53%): loss=5.4988, lr=6.00e-05, step_time=1857.8ms, ETA 4d 10h
12/01/2025 22:13:39 - INFO - training.fm_trainer - Step 3220/210000 (1.53%): loss=0.9862, lr=6.00e-05, step_time=1986.9ms, ETA 4d 10h
12/01/2025 22:14:02 - INFO - training.fm_trainer - Step 3230/210000 (1.54%): loss=4.5273, lr=6.00e-05, step_time=1881.8ms, ETA 4d 10h
12/01/2025 22:14:21 - INFO - training.fm_trainer - Step 3240/210000 (1.54%): loss=1.3130, lr=6.06e-05, step_time=1847.5ms, ETA 4d 10h
12/01/2025 22:14:39 - INFO - training.fm_trainer - Step 3250/210000 (1.55%): loss=2.2554, lr=6.06e-05, step_time=1828.5ms, ETA 4d 10h
12/01/2025 22:14:58 - INFO - training.fm_trainer - Step 3260/210000 (1.55%): loss=2.3522, lr=6.06e-05, step_time=1820.7ms, ETA 4d 10h
12/01/2025 22:15:16 - INFO - training.fm_trainer - Step 3270/210000 (1.56%): loss=1.8207, lr=6.12e-05, step_time=1815.5ms, ETA 4d 10h
12/01/2025 22:15:40 - INFO - training.fm_trainer - Step 3280/210000 (1.56%): loss=4.1219, lr=6.12e-05, step_time=3434.7ms, ETA 4d 19h
12/01/2025 22:16:03 - INFO - training.fm_trainer - Step 3290/210000 (1.57%): loss=1.2482, lr=6.12e-05, step_time=1834.3ms, ETA 4d 18h
12/01/2025 22:16:22 - INFO - training.fm_trainer - Step 3300/210000 (1.57%): loss=3.9987, lr=6.18e-05, step_time=1837.3ms, ETA 4d 17h
12/01/2025 22:16:40 - INFO - training.fm_trainer - Step 3310/210000 (1.58%): loss=1.0985, lr=6.18e-05, step_time=2012.6ms, ETA 4d 17h
12/01/2025 22:16:59 - INFO - training.fm_trainer - Step 3320/210000 (1.58%): loss=1.8153, lr=6.18e-05, step_time=1830.9ms, ETA 4d 16h
12/01/2025 22:17:18 - INFO - training.fm_trainer - Step 3330/210000 (1.59%): loss=8.4922, lr=6.24e-05, step_time=1841.3ms, ETA 4d 16h
12/01/2025 22:17:36 - INFO - training.fm_trainer - Step 3340/210000 (1.59%): loss=0.9351, lr=6.24e-05, step_time=1828.1ms, ETA 4d 15h
12/01/2025 22:17:55 - INFO - training.fm_trainer - Step 3350/210000 (1.60%): loss=3.0258, lr=6.24e-05, step_time=1841.4ms, ETA 4d 14h
12/01/2025 22:18:13 - INFO - training.fm_trainer - Step 3360/210000 (1.60%): loss=1.3886, lr=6.30e-05, step_time=1786.5ms, ETA 4d 13h
12/01/2025 22:18:32 - INFO - training.fm_trainer - Step 3370/210000 (1.60%): loss=3.4853, lr=6.30e-05, step_time=1858.4ms, ETA 4d 13h
12/01/2025 22:18:50 - INFO - training.fm_trainer - Step 3380/210000 (1.61%): loss=1.8503, lr=6.30e-05, step_time=1981.5ms, ETA 4d 14h
12/01/2025 22:19:09 - INFO - training.fm_trainer - Step 3390/210000 (1.61%): loss=1.8479, lr=6.30e-05, step_time=1832.2ms, ETA 4d 13h
12/01/2025 22:19:27 - INFO - training.fm_trainer - Step 3400/210000 (1.62%): loss=5.6851, lr=6.36e-05, step_time=1830.7ms, ETA 4d 13h
12/01/2025 22:19:46 - INFO - training.fm_trainer - Step 3410/210000 (1.62%): loss=1.4296, lr=6.36e-05, step_time=1845.1ms, ETA 4d 12h
12/01/2025 22:20:04 - INFO - training.fm_trainer - Step 3420/210000 (1.63%): loss=4.7959, lr=6.36e-05, step_time=1854.7ms, ETA 4d 12h
12/01/2025 22:20:23 - INFO - training.fm_trainer - Step 3430/210000 (1.63%): loss=2.1448, lr=6.42e-05, step_time=1839.8ms, ETA 4d 12h
12/01/2025 22:20:41 - INFO - training.fm_trainer - Step 3440/210000 (1.64%): loss=6.8450, lr=6.42e-05, step_time=1841.4ms, ETA 4d 11h
12/01/2025 22:21:00 - INFO - training.fm_trainer - Step 3450/210000 (1.64%): loss=2.5092, lr=6.42e-05, step_time=1845.6ms, ETA 4d 11h
12/01/2025 22:21:18 - INFO - training.fm_trainer - Step 3460/210000 (1.65%): loss=1.3975, lr=6.48e-05, step_time=1873.2ms, ETA 4d 11h
12/01/2025 22:21:37 - INFO - training.fm_trainer - Step 3470/210000 (1.65%): loss=4.3353, lr=6.48e-05, step_time=1832.8ms, ETA 4d 11h
12/01/2025 22:21:56 - INFO - training.fm_trainer - Step 3480/210000 (1.66%): loss=1.8884, lr=6.48e-05, step_time=1846.4ms, ETA 4d 11h
12/01/2025 22:22:14 - INFO - training.fm_trainer - Step 3490/210000 (1.66%): loss=3.7009, lr=6.54e-05, step_time=1864.2ms, ETA 4d 11h
12/01/2025 22:22:33 - INFO - training.fm_trainer - Step 3500/210000 (1.67%): loss=4.7206, lr=6.54e-05, step_time=1840.7ms, ETA 4d 11h
12/01/2025 22:22:33 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-3500
12/01/2025 22:22:33 - INFO - accelerate.accelerator - Saving FSDP model
12/01/2025 22:22:34 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-3500/pytorch_model_fsdp_0
12/01/2025 22:22:41 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-3500/pytorch_model_fsdp_0
12/01/2025 22:22:41 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-3500
12/01/2025 22:22:41 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/01/2025 22:22:44 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-3500/optimizer_0
12/01/2025 22:22:59 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-3500/optimizer_0
12/01/2025 22:22:59 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-3500
12/01/2025 22:22:59 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-3500/scheduler.bin
12/01/2025 22:22:59 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-3500/sampler.bin
12/01/2025 22:22:59 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-3500/random_states_0.pkl
12/01/2025 22:22:59 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-3500
12/01/2025 22:23:18 - INFO - training.fm_trainer - Step 3510/210000 (1.67%): loss=2.5883, lr=6.54e-05, step_time=1904.8ms, ETA 4d 11h
12/01/2025 22:23:37 - INFO - training.fm_trainer - Step 3520/210000 (1.68%): loss=1.5145, lr=6.60e-05, step_time=1790.2ms, ETA 4d 10h
12/01/2025 22:23:55 - INFO - training.fm_trainer - Step 3530/210000 (1.68%): loss=1.4520, lr=6.60e-05, step_time=1810.6ms, ETA 4d 10h
12/01/2025 22:24:14 - INFO - training.fm_trainer - Step 3540/210000 (1.69%): loss=0.6406, lr=6.60e-05, step_time=1833.5ms, ETA 4d 10h
12/01/2025 22:24:32 - INFO - training.fm_trainer - Step 3550/210000 (1.69%): loss=5.1726, lr=6.60e-05, step_time=1836.7ms, ETA 4d 10h
12/01/2025 22:24:51 - INFO - training.fm_trainer - Step 3560/210000 (1.70%): loss=2.6360, lr=6.66e-05, step_time=1826.4ms, ETA 4d 10h
12/01/2025 22:25:10 - INFO - training.fm_trainer - Step 3570/210000 (1.70%): loss=2.9992, lr=6.66e-05, step_time=1860.7ms, ETA 4d 10h
12/01/2025 22:25:28 - INFO - training.fm_trainer - Step 3580/210000 (1.70%): loss=1.2660, lr=6.66e-05, step_time=1835.2ms, ETA 4d 10h
12/01/2025 22:25:47 - INFO - training.fm_trainer - Step 3590/210000 (1.71%): loss=2.5386, lr=6.72e-05, step_time=1850.5ms, ETA 4d 10h
12/01/2025 22:26:05 - INFO - training.fm_trainer - Step 3600/210000 (1.71%): loss=1.7010, lr=6.72e-05, step_time=1862.2ms, ETA 4d 10h
12/01/2025 22:26:24 - INFO - training.fm_trainer - Step 3610/210000 (1.72%): loss=1.1132, lr=6.72e-05, step_time=1812.2ms, ETA 4d 9h
12/01/2025 22:26:43 - INFO - training.fm_trainer - Step 3620/210000 (1.72%): loss=1.4282, lr=6.78e-05, step_time=1858.1ms, ETA 4d 9h
12/01/2025 22:27:01 - INFO - training.fm_trainer - Step 3630/210000 (1.73%): loss=9.8700, lr=6.78e-05, step_time=1842.4ms, ETA 4d 9h
12/01/2025 22:27:19 - INFO - training.fm_trainer - Step 3640/210000 (1.73%): loss=1.0894, lr=6.78e-05, step_time=1819.8ms, ETA 4d 9h
12/01/2025 22:27:38 - INFO - training.fm_trainer - Step 3650/210000 (1.74%): loss=0.8789, lr=6.84e-05, step_time=1977.4ms, ETA 4d 10h
12/01/2025 22:27:57 - INFO - training.fm_trainer - Step 3660/210000 (1.74%): loss=1.4762, lr=6.84e-05, step_time=1882.6ms, ETA 4d 10h
12/01/2025 22:28:15 - INFO - training.fm_trainer - Step 3670/210000 (1.75%): loss=2.1305, lr=6.84e-05, step_time=1859.9ms, ETA 4d 10h
12/01/2025 22:28:34 - INFO - training.fm_trainer - Step 3680/210000 (1.75%): loss=1.2925, lr=6.90e-05, step_time=1801.2ms, ETA 4d 10h
12/01/2025 22:28:53 - INFO - training.fm_trainer - Step 3690/210000 (1.76%): loss=0.5851, lr=6.90e-05, step_time=2016.2ms, ETA 4d 11h
12/01/2025 22:29:11 - INFO - training.fm_trainer - Step 3700/210000 (1.76%): loss=5.5329, lr=6.90e-05, step_time=1946.1ms, ETA 4d 11h
12/01/2025 22:29:30 - INFO - training.fm_trainer - Step 3710/210000 (1.77%): loss=8.3830, lr=6.90e-05, step_time=1883.8ms, ETA 4d 11h
12/01/2025 22:29:49 - INFO - training.fm_trainer - Step 3720/210000 (1.77%): loss=1.1412, lr=6.96e-05, step_time=1822.1ms, ETA 4d 11h
12/01/2025 22:30:07 - INFO - training.fm_trainer - Step 3730/210000 (1.78%): loss=12.1400, lr=6.96e-05, step_time=1828.2ms, ETA 4d 11h
12/01/2025 22:30:26 - INFO - training.fm_trainer - Step 3740/210000 (1.78%): loss=3.6539, lr=6.96e-05, step_time=1860.7ms, ETA 4d 11h
12/01/2025 22:30:45 - INFO - training.fm_trainer - Step 3750/210000 (1.79%): loss=4.4523, lr=7.02e-05, step_time=1847.6ms, ETA 4d 10h
12/01/2025 22:31:03 - INFO - training.fm_trainer - Step 3760/210000 (1.79%): loss=3.8290, lr=7.02e-05, step_time=1851.1ms, ETA 4d 10h
12/01/2025 22:31:22 - INFO - training.fm_trainer - Step 3770/210000 (1.80%): loss=1.6563, lr=7.02e-05, step_time=1874.0ms, ETA 4d 10h
12/01/2025 22:31:40 - INFO - training.fm_trainer - Step 3780/210000 (1.80%): loss=1.8529, lr=7.08e-05, step_time=1838.1ms, ETA 4d 10h
12/01/2025 22:31:59 - INFO - training.fm_trainer - Step 3790/210000 (1.80%): loss=1.5242, lr=7.08e-05, step_time=1810.5ms, ETA 4d 10h
12/01/2025 22:32:17 - INFO - training.fm_trainer - Step 3800/210000 (1.81%): loss=2.5797, lr=7.08e-05, step_time=1888.4ms, ETA 4d 10h
12/01/2025 22:32:36 - INFO - training.fm_trainer - Step 3810/210000 (1.81%): loss=5.2792, lr=7.14e-05, step_time=1842.7ms, ETA 4d 10h
12/01/2025 22:32:54 - INFO - training.fm_trainer - Step 3820/210000 (1.82%): loss=2.9641, lr=7.14e-05, step_time=1839.5ms, ETA 4d 10h
12/01/2025 22:33:13 - INFO - training.fm_trainer - Step 3830/210000 (1.82%): loss=4.5238, lr=7.14e-05, step_time=1824.4ms, ETA 4d 10h
12/01/2025 22:33:31 - INFO - training.fm_trainer - Step 3840/210000 (1.83%): loss=1.8656, lr=7.20e-05, step_time=1837.6ms, ETA 4d 10h
12/01/2025 22:33:50 - INFO - training.fm_trainer - Step 3850/210000 (1.83%): loss=1.8930, lr=7.20e-05, step_time=1835.9ms, ETA 4d 9h
12/01/2025 22:34:08 - INFO - training.fm_trainer - Step 3860/210000 (1.84%): loss=2.7868, lr=7.20e-05, step_time=1875.0ms, ETA 4d 10h
12/01/2025 22:34:27 - INFO - training.fm_trainer - Step 3870/210000 (1.84%): loss=5.6344, lr=7.20e-05, step_time=1855.7ms, ETA 4d 10h
12/01/2025 22:34:45 - INFO - training.fm_trainer - Step 3880/210000 (1.85%): loss=7.9647, lr=7.26e-05, step_time=1850.6ms, ETA 4d 10h
12/01/2025 22:35:04 - INFO - training.fm_trainer - Step 3890/210000 (1.85%): loss=4.3109, lr=7.26e-05, step_time=1980.3ms, ETA 4d 10h
12/01/2025 22:35:23 - INFO - training.fm_trainer - Step 3900/210000 (1.86%): loss=5.5413, lr=7.26e-05, step_time=1826.0ms, ETA 4d 10h
12/01/2025 22:35:41 - INFO - training.fm_trainer - Step 3910/210000 (1.86%): loss=1.7106, lr=7.32e-05, step_time=1803.8ms, ETA 4d 10h
12/01/2025 22:36:00 - INFO - training.fm_trainer - Step 3920/210000 (1.87%): loss=2.6025, lr=7.32e-05, step_time=1847.7ms, ETA 4d 10h
12/01/2025 22:36:18 - INFO - training.fm_trainer - Step 3930/210000 (1.87%): loss=2.3931, lr=7.32e-05, step_time=1850.5ms, ETA 4d 10h
12/01/2025 22:36:37 - INFO - training.fm_trainer - Step 3940/210000 (1.88%): loss=8.5828, lr=7.38e-05, step_time=2007.5ms, ETA 4d 11h
12/01/2025 22:36:55 - INFO - training.fm_trainer - Step 3950/210000 (1.88%): loss=1.2962, lr=7.38e-05, step_time=1831.9ms, ETA 4d 10h
12/01/2025 22:37:14 - INFO - training.fm_trainer - Step 3960/210000 (1.89%): loss=2.0990, lr=7.38e-05, step_time=1868.4ms, ETA 4d 10h
12/01/2025 22:37:33 - INFO - training.fm_trainer - Step 3970/210000 (1.89%): loss=10.4440, lr=7.44e-05, step_time=1848.0ms, ETA 4d 10h
12/01/2025 22:37:51 - INFO - training.fm_trainer - Step 3980/210000 (1.90%): loss=2.5926, lr=7.44e-05, step_time=1829.8ms, ETA 4d 10h
12/01/2025 22:38:10 - INFO - training.fm_trainer - Step 3990/210000 (1.90%): loss=2.5274, lr=7.44e-05, step_time=1822.4ms, ETA 4d 10h
12/01/2025 22:38:29 - INFO - training.fm_trainer - Step 4000/210000 (1.90%): loss=5.8045, lr=7.50e-05, step_time=1776.5ms, ETA 4d 9h
12/01/2025 22:38:29 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-4000
12/01/2025 22:38:29 - INFO - accelerate.accelerator - Saving FSDP model
12/01/2025 22:38:30 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-4000/pytorch_model_fsdp_0
12/01/2025 22:38:38 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-4000/pytorch_model_fsdp_0
12/01/2025 22:38:38 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-4000
12/01/2025 22:38:38 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/01/2025 22:38:40 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-4000/optimizer_0
12/01/2025 22:38:56 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-4000/optimizer_0
12/01/2025 22:38:56 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-4000
12/01/2025 22:38:56 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-4000/scheduler.bin
12/01/2025 22:38:56 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-4000/sampler.bin
12/01/2025 22:38:56 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-4000/random_states_0.pkl
12/01/2025 22:38:56 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-4000
12/01/2025 22:39:15 - INFO - training.fm_trainer - Step 4010/210000 (1.91%): loss=3.0674, lr=7.50e-05, step_time=1859.8ms, ETA 4d 9h
12/01/2025 22:39:33 - INFO - training.fm_trainer - Step 4020/210000 (1.91%): loss=1.4989, lr=7.50e-05, step_time=1859.3ms, ETA 4d 9h
12/01/2025 22:39:52 - INFO - training.fm_trainer - Step 4030/210000 (1.92%): loss=1.1729, lr=7.50e-05, step_time=1854.7ms, ETA 4d 9h
12/01/2025 22:40:10 - INFO - training.fm_trainer - Step 4040/210000 (1.92%): loss=2.6628, lr=7.56e-05, step_time=1847.1ms, ETA 4d 9h
12/01/2025 22:40:29 - INFO - training.fm_trainer - Step 4050/210000 (1.93%): loss=1.7077, lr=7.56e-05, step_time=1826.2ms, ETA 4d 9h
12/01/2025 22:40:47 - INFO - training.fm_trainer - Step 4060/210000 (1.93%): loss=2.3782, lr=7.56e-05, step_time=1832.8ms, ETA 4d 9h
12/01/2025 22:41:06 - INFO - training.fm_trainer - Step 4070/210000 (1.94%): loss=1.1173, lr=7.62e-05, step_time=1862.9ms, ETA 4d 9h
12/01/2025 22:41:24 - INFO - training.fm_trainer - Step 4080/210000 (1.94%): loss=2.9649, lr=7.62e-05, step_time=1831.4ms, ETA 4d 9h
12/01/2025 22:41:44 - INFO - training.fm_trainer - Step 4090/210000 (1.95%): loss=1.3123, lr=7.62e-05, step_time=1848.1ms, ETA 4d 9h
12/01/2025 22:42:02 - INFO - training.fm_trainer - Step 4100/210000 (1.95%): loss=3.1373, lr=7.68e-05, step_time=1818.7ms, ETA 4d 9h
12/01/2025 22:42:21 - INFO - training.fm_trainer - Step 4110/210000 (1.96%): loss=8.5312, lr=7.68e-05, step_time=1851.4ms, ETA 4d 9h
12/01/2025 22:42:40 - INFO - training.fm_trainer - Step 4120/210000 (1.96%): loss=2.0190, lr=7.68e-05, step_time=1869.9ms, ETA 4d 9h
12/01/2025 22:43:00 - INFO - training.fm_trainer - Step 4130/210000 (1.97%): loss=1.2577, lr=7.74e-05, step_time=1860.4ms, ETA 4d 9h
12/01/2025 22:43:19 - INFO - training.fm_trainer - Step 4140/210000 (1.97%): loss=5.8406, lr=7.74e-05, step_time=1819.1ms, ETA 4d 9h
12/01/2025 22:43:38 - INFO - training.fm_trainer - Step 4150/210000 (1.98%): loss=0.4127, lr=7.74e-05, step_time=1823.7ms, ETA 4d 9h
12/01/2025 22:43:56 - INFO - training.fm_trainer - Step 4160/210000 (1.98%): loss=2.4990, lr=7.80e-05, step_time=1782.6ms, ETA 4d 9h
12/01/2025 22:44:15 - INFO - training.fm_trainer - Step 4170/210000 (1.99%): loss=3.2619, lr=7.80e-05, step_time=1874.4ms, ETA 4d 9h
12/01/2025 22:44:34 - INFO - training.fm_trainer - Step 4180/210000 (1.99%): loss=1.5683, lr=7.80e-05, step_time=1841.4ms, ETA 4d 9h
12/01/2025 22:44:53 - INFO - training.fm_trainer - Step 4190/210000 (2.00%): loss=4.4047, lr=7.80e-05, step_time=1842.0ms, ETA 4d 9h
12/01/2025 22:45:11 - INFO - training.fm_trainer - Step 4200/210000 (2.00%): loss=0.5798, lr=7.86e-05, step_time=1846.5ms, ETA 4d 9h
12/01/2025 22:45:30 - INFO - training.fm_trainer - Step 4210/210000 (2.00%): loss=0.5939, lr=7.86e-05, step_time=1818.3ms, ETA 4d 9h
12/01/2025 22:45:48 - INFO - training.fm_trainer - Step 4220/210000 (2.01%): loss=3.1256, lr=7.86e-05, step_time=1827.7ms, ETA 4d 9h
12/01/2025 22:46:07 - INFO - training.fm_trainer - Step 4230/210000 (2.01%): loss=2.2599, lr=7.92e-05, step_time=1842.1ms, ETA 4d 9h
12/01/2025 22:46:26 - INFO - training.fm_trainer - Step 4240/210000 (2.02%): loss=0.6765, lr=7.92e-05, step_time=1835.6ms, ETA 4d 9h
12/01/2025 22:46:45 - INFO - training.fm_trainer - Step 4250/210000 (2.02%): loss=2.4144, lr=7.92e-05, step_time=1829.3ms, ETA 4d 9h
12/01/2025 22:47:03 - INFO - training.fm_trainer - Step 4260/210000 (2.03%): loss=2.2514, lr=7.98e-05, step_time=1829.3ms, ETA 4d 8h
12/01/2025 22:47:22 - INFO - training.fm_trainer - Step 4270/210000 (2.03%): loss=1.1962, lr=7.98e-05, step_time=1826.2ms, ETA 4d 8h
12/01/2025 22:47:40 - INFO - training.fm_trainer - Step 4280/210000 (2.04%): loss=0.9919, lr=7.98e-05, step_time=1905.8ms, ETA 4d 9h
12/01/2025 22:47:59 - INFO - training.fm_trainer - Step 4290/210000 (2.04%): loss=11.4324, lr=8.04e-05, step_time=1858.0ms, ETA 4d 9h
12/01/2025 22:48:17 - INFO - training.fm_trainer - Step 4300/210000 (2.05%): loss=2.1407, lr=8.04e-05, step_time=1849.1ms, ETA 4d 9h
12/01/2025 22:48:37 - INFO - training.fm_trainer - Step 4310/210000 (2.05%): loss=10.4744, lr=8.04e-05, step_time=1857.4ms, ETA 4d 9h
12/01/2025 22:48:55 - INFO - training.fm_trainer - Step 4320/210000 (2.06%): loss=4.4228, lr=8.10e-05, step_time=1811.0ms, ETA 4d 9h
12/01/2025 22:49:14 - INFO - training.fm_trainer - Step 4330/210000 (2.06%): loss=1.6987, lr=8.10e-05, step_time=1877.3ms, ETA 4d 9h
12/01/2025 22:49:33 - INFO - training.fm_trainer - Step 4340/210000 (2.07%): loss=9.0225, lr=8.10e-05, step_time=1894.8ms, ETA 4d 9h
12/01/2025 22:49:52 - INFO - training.fm_trainer - Step 4350/210000 (2.07%): loss=2.2609, lr=8.10e-05, step_time=1822.2ms, ETA 4d 9h
12/01/2025 22:50:10 - INFO - training.fm_trainer - Step 4360/210000 (2.08%): loss=5.7550, lr=8.16e-05, step_time=1832.2ms, ETA 4d 9h
12/01/2025 22:50:30 - INFO - training.fm_trainer - Step 4370/210000 (2.08%): loss=9.8647, lr=8.16e-05, step_time=1809.2ms, ETA 4d 9h
12/01/2025 22:50:48 - INFO - training.fm_trainer - Step 4380/210000 (2.09%): loss=1.1835, lr=8.16e-05, step_time=1949.4ms, ETA 4d 9h
12/01/2025 22:51:06 - INFO - training.fm_trainer - Step 4390/210000 (2.09%): loss=3.4545, lr=8.22e-05, step_time=1833.6ms, ETA 4d 9h
12/01/2025 22:51:25 - INFO - training.fm_trainer - Step 4400/210000 (2.10%): loss=4.8977, lr=8.22e-05, step_time=1830.1ms, ETA 4d 9h
12/01/2025 22:51:43 - INFO - training.fm_trainer - Step 4410/210000 (2.10%): loss=2.5467, lr=8.22e-05, step_time=1831.9ms, ETA 4d 9h
12/01/2025 22:52:02 - INFO - training.fm_trainer - Step 4420/210000 (2.10%): loss=0.7798, lr=8.28e-05, step_time=1836.1ms, ETA 4d 9h
12/01/2025 22:52:20 - INFO - training.fm_trainer - Step 4430/210000 (2.11%): loss=1.0950, lr=8.28e-05, step_time=1856.6ms, ETA 4d 9h
12/01/2025 22:52:39 - INFO - training.fm_trainer - Step 4440/210000 (2.11%): loss=3.2691, lr=8.28e-05, step_time=1849.2ms, ETA 4d 9h
12/01/2025 22:52:57 - INFO - training.fm_trainer - Step 4450/210000 (2.12%): loss=0.8807, lr=8.34e-05, step_time=1844.2ms, ETA 4d 9h
12/01/2025 22:53:16 - INFO - training.fm_trainer - Step 4460/210000 (2.12%): loss=3.2734, lr=8.34e-05, step_time=1880.6ms, ETA 4d 9h
12/01/2025 22:53:35 - INFO - training.fm_trainer - Step 4470/210000 (2.13%): loss=1.5210, lr=8.34e-05, step_time=1825.5ms, ETA 4d 9h
12/01/2025 22:53:53 - INFO - training.fm_trainer - Step 4480/210000 (2.13%): loss=1.4460, lr=8.40e-05, step_time=1802.9ms, ETA 4d 9h
12/01/2025 22:54:11 - INFO - training.fm_trainer - Step 4490/210000 (2.14%): loss=1.6349, lr=8.40e-05, step_time=1857.2ms, ETA 4d 9h
12/01/2025 22:54:30 - INFO - training.fm_trainer - Step 4500/210000 (2.14%): loss=1.0073, lr=8.40e-05, step_time=1817.4ms, ETA 4d 9h
12/01/2025 22:54:30 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-4500
12/01/2025 22:54:30 - INFO - accelerate.accelerator - Saving FSDP model
12/01/2025 22:54:31 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-4500/pytorch_model_fsdp_0
12/01/2025 22:54:39 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-4500/pytorch_model_fsdp_0
12/01/2025 22:54:39 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-4500
12/01/2025 22:54:39 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/01/2025 22:54:41 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-4500/optimizer_0
12/01/2025 22:54:58 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-4500/optimizer_0
12/01/2025 22:54:58 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-4500
12/01/2025 22:54:58 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-4500/scheduler.bin
12/01/2025 22:54:58 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-4500/sampler.bin
12/01/2025 22:54:58 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-4500/random_states_0.pkl
12/01/2025 22:54:58 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-4500
12/01/2025 22:55:16 - INFO - training.fm_trainer - Step 4510/210000 (2.15%): loss=5.2343, lr=8.40e-05, step_time=1823.7ms, ETA 4d 9h
12/01/2025 22:55:35 - INFO - training.fm_trainer - Step 4520/210000 (2.15%): loss=0.6361, lr=8.46e-05, step_time=1819.9ms, ETA 4d 8h
12/01/2025 22:55:53 - INFO - training.fm_trainer - Step 4530/210000 (2.16%): loss=1.4405, lr=8.46e-05, step_time=1888.7ms, ETA 4d 9h
12/01/2025 22:56:12 - INFO - training.fm_trainer - Step 4540/210000 (2.16%): loss=1.1358, lr=8.46e-05, step_time=1836.2ms, ETA 4d 9h
12/01/2025 22:56:30 - INFO - training.fm_trainer - Step 4550/210000 (2.17%): loss=6.5155, lr=8.52e-05, step_time=1853.0ms, ETA 4d 9h
12/01/2025 22:56:49 - INFO - training.fm_trainer - Step 4560/210000 (2.17%): loss=0.9219, lr=8.52e-05, step_time=1910.2ms, ETA 4d 9h
12/01/2025 22:57:07 - INFO - training.fm_trainer - Step 4570/210000 (2.18%): loss=2.7419, lr=8.52e-05, step_time=1854.9ms, ETA 4d 9h
12/01/2025 22:57:26 - INFO - training.fm_trainer - Step 4580/210000 (2.18%): loss=4.9954, lr=8.58e-05, step_time=1811.4ms, ETA 4d 9h
12/01/2025 22:57:44 - INFO - training.fm_trainer - Step 4590/210000 (2.19%): loss=1.2149, lr=8.58e-05, step_time=1839.3ms, ETA 4d 9h
12/01/2025 22:58:03 - INFO - training.fm_trainer - Step 4600/210000 (2.19%): loss=1.0230, lr=8.58e-05, step_time=1873.8ms, ETA 4d 9h
12/01/2025 22:58:22 - INFO - training.fm_trainer - Step 4610/210000 (2.20%): loss=2.8664, lr=8.64e-05, step_time=1843.4ms, ETA 4d 9h
12/01/2025 22:58:40 - INFO - training.fm_trainer - Step 4620/210000 (2.20%): loss=3.2029, lr=8.64e-05, step_time=1878.5ms, ETA 4d 9h
12/01/2025 22:58:59 - INFO - training.fm_trainer - Step 4630/210000 (2.20%): loss=1.9438, lr=8.64e-05, step_time=1838.2ms, ETA 4d 9h
12/01/2025 22:59:17 - INFO - training.fm_trainer - Step 4640/210000 (2.21%): loss=11.1502, lr=8.70e-05, step_time=1785.1ms, ETA 4d 9h
12/01/2025 22:59:36 - INFO - training.fm_trainer - Step 4650/210000 (2.21%): loss=1.6080, lr=8.70e-05, step_time=1958.7ms, ETA 4d 9h
12/01/2025 22:59:55 - INFO - training.fm_trainer - Step 4660/210000 (2.22%): loss=0.8801, lr=8.70e-05, step_time=1820.9ms, ETA 4d 9h
12/01/2025 23:00:13 - INFO - training.fm_trainer - Step 4670/210000 (2.22%): loss=2.1906, lr=8.70e-05, step_time=1830.6ms, ETA 4d 9h
12/01/2025 23:00:31 - INFO - training.fm_trainer - Step 4680/210000 (2.23%): loss=1.9288, lr=8.76e-05, step_time=1830.4ms, ETA 4d 9h
12/01/2025 23:00:50 - INFO - training.fm_trainer - Step 4690/210000 (2.23%): loss=0.9357, lr=8.76e-05, step_time=1997.6ms, ETA 4d 10h
12/01/2025 23:01:09 - INFO - training.fm_trainer - Step 4700/210000 (2.24%): loss=1.3110, lr=8.76e-05, step_time=1847.4ms, ETA 4d 10h
12/01/2025 23:01:28 - INFO - training.fm_trainer - Step 4710/210000 (2.24%): loss=3.0032, lr=8.82e-05, step_time=2052.6ms, ETA 4d 11h
12/01/2025 23:01:47 - INFO - training.fm_trainer - Step 4720/210000 (2.25%): loss=6.7016, lr=8.82e-05, step_time=1837.8ms, ETA 4d 10h
12/01/2025 23:02:05 - INFO - training.fm_trainer - Step 4730/210000 (2.25%): loss=2.5499, lr=8.82e-05, step_time=1865.8ms, ETA 4d 10h
12/01/2025 23:02:24 - INFO - training.fm_trainer - Step 4740/210000 (2.26%): loss=5.1638, lr=8.88e-05, step_time=1895.9ms, ETA 4d 11h
12/01/2025 23:02:42 - INFO - training.fm_trainer - Step 4750/210000 (2.26%): loss=3.4597, lr=8.88e-05, step_time=1854.0ms, ETA 4d 10h
12/01/2025 23:03:01 - INFO - training.fm_trainer - Step 4760/210000 (2.27%): loss=6.5037, lr=8.88e-05, step_time=1857.9ms, ETA 4d 10h
12/01/2025 23:03:19 - INFO - training.fm_trainer - Step 4770/210000 (2.27%): loss=3.0562, lr=8.94e-05, step_time=1844.5ms, ETA 4d 10h
12/01/2025 23:03:38 - INFO - training.fm_trainer - Step 4780/210000 (2.28%): loss=4.3424, lr=8.94e-05, step_time=1808.1ms, ETA 4d 10h
12/01/2025 23:03:56 - INFO - training.fm_trainer - Step 4790/210000 (2.28%): loss=1.4665, lr=8.94e-05, step_time=1829.0ms, ETA 4d 10h
12/01/2025 23:04:15 - INFO - training.fm_trainer - Step 4800/210000 (2.29%): loss=4.5889, lr=9.00e-05, step_time=1808.6ms, ETA 4d 9h
12/01/2025 23:04:34 - INFO - training.fm_trainer - Step 4810/210000 (2.29%): loss=2.2866, lr=9.00e-05, step_time=1850.7ms, ETA 4d 9h
12/01/2025 23:04:52 - INFO - training.fm_trainer - Step 4820/210000 (2.30%): loss=6.7607, lr=9.00e-05, step_time=1891.4ms, ETA 4d 9h
12/01/2025 23:05:11 - INFO - training.fm_trainer - Step 4830/210000 (2.30%): loss=7.4783, lr=9.00e-05, step_time=1850.8ms, ETA 4d 9h
12/01/2025 23:05:29 - INFO - training.fm_trainer - Step 4840/210000 (2.30%): loss=3.9338, lr=9.06e-05, step_time=1827.5ms, ETA 4d 9h
12/01/2025 23:05:48 - INFO - training.fm_trainer - Step 4850/210000 (2.31%): loss=1.5310, lr=9.06e-05, step_time=1827.4ms, ETA 4d 9h
12/01/2025 23:06:07 - INFO - training.fm_trainer - Step 4860/210000 (2.31%): loss=1.8255, lr=9.06e-05, step_time=1828.3ms, ETA 4d 9h
12/01/2025 23:06:26 - INFO - training.fm_trainer - Step 4870/210000 (2.32%): loss=3.4193, lr=9.12e-05, step_time=1842.6ms, ETA 4d 9h
12/01/2025 23:06:44 - INFO - training.fm_trainer - Step 4880/210000 (2.32%): loss=1.2653, lr=9.12e-05, step_time=1844.8ms, ETA 4d 9h
12/01/2025 23:07:03 - INFO - training.fm_trainer - Step 4890/210000 (2.33%): loss=0.8843, lr=9.12e-05, step_time=2003.8ms, ETA 4d 10h
12/01/2025 23:07:22 - INFO - training.fm_trainer - Step 4900/210000 (2.33%): loss=0.9839, lr=9.18e-05, step_time=2004.7ms, ETA 4d 10h
12/01/2025 23:07:41 - INFO - training.fm_trainer - Step 4910/210000 (2.34%): loss=2.2654, lr=9.18e-05, step_time=1835.9ms, ETA 4d 10h
12/01/2025 23:07:59 - INFO - training.fm_trainer - Step 4920/210000 (2.34%): loss=1.6942, lr=9.18e-05, step_time=1831.1ms, ETA 4d 10h
12/01/2025 23:08:18 - INFO - training.fm_trainer - Step 4930/210000 (2.35%): loss=1.1624, lr=9.24e-05, step_time=1961.6ms, ETA 4d 11h
12/01/2025 23:08:36 - INFO - training.fm_trainer - Step 4940/210000 (2.35%): loss=4.6890, lr=9.24e-05, step_time=1834.2ms, ETA 4d 10h
12/01/2025 23:08:55 - INFO - training.fm_trainer - Step 4950/210000 (2.36%): loss=0.6133, lr=9.24e-05, step_time=1864.7ms, ETA 4d 10h
12/01/2025 23:09:13 - INFO - training.fm_trainer - Step 4960/210000 (2.36%): loss=8.8212, lr=9.30e-05, step_time=1796.7ms, ETA 4d 10h
12/01/2025 23:09:32 - INFO - training.fm_trainer - Step 4970/210000 (2.37%): loss=4.2132, lr=9.30e-05, step_time=1848.4ms, ETA 4d 10h
12/01/2025 23:09:51 - INFO - training.fm_trainer - Step 4980/210000 (2.37%): loss=2.6084, lr=9.30e-05, step_time=1832.9ms, ETA 4d 9h
12/01/2025 23:10:09 - INFO - training.fm_trainer - Step 4990/210000 (2.38%): loss=11.9651, lr=9.30e-05, step_time=1810.4ms, ETA 4d 9h
12/01/2025 23:10:28 - INFO - training.fm_trainer - Step 5000/210000 (2.38%): loss=1.0687, lr=9.36e-05, step_time=2044.6ms, ETA 4d 10h
12/01/2025 23:10:28 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-5000
12/01/2025 23:10:28 - INFO - accelerate.accelerator - Saving FSDP model
12/01/2025 23:10:29 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-5000/pytorch_model_fsdp_0
12/01/2025 23:10:37 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-5000/pytorch_model_fsdp_0
12/01/2025 23:10:37 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-5000
12/01/2025 23:10:37 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/01/2025 23:10:39 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-5000/optimizer_0
12/01/2025 23:10:55 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-5000/optimizer_0
12/01/2025 23:10:55 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-5000
12/01/2025 23:10:55 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-5000/scheduler.bin
12/01/2025 23:10:55 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-5000/sampler.bin
12/01/2025 23:10:55 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-5000/random_states_0.pkl
12/01/2025 23:10:55 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-5000
12/01/2025 23:11:14 - INFO - training.fm_trainer - Step 5010/210000 (2.39%): loss=1.1040, lr=9.36e-05, step_time=1816.8ms, ETA 4d 10h
12/01/2025 23:11:32 - INFO - training.fm_trainer - Step 5020/210000 (2.39%): loss=0.3671, lr=9.36e-05, step_time=1871.4ms, ETA 4d 10h
12/01/2025 23:11:51 - INFO - training.fm_trainer - Step 5030/210000 (2.40%): loss=4.4734, lr=9.42e-05, step_time=1848.7ms, ETA 4d 10h
12/01/2025 23:12:09 - INFO - training.fm_trainer - Step 5040/210000 (2.40%): loss=4.7918, lr=9.42e-05, step_time=1822.3ms, ETA 4d 10h
12/01/2025 23:12:28 - INFO - training.fm_trainer - Step 5050/210000 (2.40%): loss=1.4617, lr=9.42e-05, step_time=1992.6ms, ETA 4d 10h
12/01/2025 23:12:46 - INFO - training.fm_trainer - Step 5060/210000 (2.41%): loss=7.2944, lr=9.48e-05, step_time=1829.5ms, ETA 4d 10h
12/01/2025 23:13:05 - INFO - training.fm_trainer - Step 5070/210000 (2.41%): loss=8.3585, lr=9.48e-05, step_time=1811.3ms, ETA 4d 10h
12/01/2025 23:13:24 - INFO - training.fm_trainer - Step 5080/210000 (2.42%): loss=0.8684, lr=9.48e-05, step_time=1825.9ms, ETA 4d 9h
12/01/2025 23:13:42 - INFO - training.fm_trainer - Step 5090/210000 (2.42%): loss=2.6220, lr=9.54e-05, step_time=1850.1ms, ETA 4d 9h
12/01/2025 23:14:01 - INFO - training.fm_trainer - Step 5100/210000 (2.43%): loss=3.6911, lr=9.54e-05, step_time=1869.7ms, ETA 4d 9h
12/01/2025 23:14:19 - INFO - training.fm_trainer - Step 5110/210000 (2.43%): loss=2.4485, lr=9.54e-05, step_time=1842.2ms, ETA 4d 9h
12/01/2025 23:14:38 - INFO - training.fm_trainer - Step 5120/210000 (2.44%): loss=0.9931, lr=9.60e-05, step_time=1776.4ms, ETA 4d 9h
12/01/2025 23:14:57 - INFO - training.fm_trainer - Step 5130/210000 (2.44%): loss=3.7751, lr=9.60e-05, step_time=1850.3ms, ETA 4d 9h
12/01/2025 23:15:15 - INFO - training.fm_trainer - Step 5140/210000 (2.45%): loss=3.6995, lr=9.60e-05, step_time=1902.3ms, ETA 4d 9h
12/01/2025 23:15:34 - INFO - training.fm_trainer - Step 5150/210000 (2.45%): loss=4.7575, lr=9.60e-05, step_time=1816.3ms, ETA 4d 9h
12/01/2025 23:15:52 - INFO - training.fm_trainer - Step 5160/210000 (2.46%): loss=8.7157, lr=9.66e-05, step_time=1812.1ms, ETA 4d 9h
12/01/2025 23:16:11 - INFO - training.fm_trainer - Step 5170/210000 (2.46%): loss=3.6577, lr=9.66e-05, step_time=1840.0ms, ETA 4d 9h
12/01/2025 23:16:29 - INFO - training.fm_trainer - Step 5180/210000 (2.47%): loss=3.5294, lr=9.66e-05, step_time=1884.5ms, ETA 4d 9h
12/01/2025 23:16:48 - INFO - training.fm_trainer - Step 5190/210000 (2.47%): loss=3.3921, lr=9.72e-05, step_time=1993.3ms, ETA 4d 10h
12/01/2025 23:17:06 - INFO - training.fm_trainer - Step 5200/210000 (2.48%): loss=2.3241, lr=9.72e-05, step_time=1939.1ms, ETA 4d 10h
12/01/2025 23:17:25 - INFO - training.fm_trainer - Step 5210/210000 (2.48%): loss=4.6187, lr=9.72e-05, step_time=1847.5ms, ETA 4d 10h
12/01/2025 23:17:44 - INFO - training.fm_trainer - Step 5220/210000 (2.49%): loss=4.5984, lr=9.78e-05, step_time=1837.5ms, ETA 4d 10h
12/01/2025 23:18:02 - INFO - training.fm_trainer - Step 5230/210000 (2.49%): loss=2.3173, lr=9.78e-05, step_time=1985.0ms, ETA 4d 10h
12/01/2025 23:18:21 - INFO - training.fm_trainer - Step 5240/210000 (2.50%): loss=3.9896, lr=9.78e-05, step_time=1842.0ms, ETA 4d 10h
12/01/2025 23:18:39 - INFO - training.fm_trainer - Step 5250/210000 (2.50%): loss=1.0747, lr=9.84e-05, step_time=1834.7ms, ETA 4d 10h
12/01/2025 23:18:58 - INFO - training.fm_trainer - Step 5260/210000 (2.50%): loss=7.3096, lr=9.84e-05, step_time=1840.1ms, ETA 4d 10h
12/01/2025 23:19:16 - INFO - training.fm_trainer - Step 5270/210000 (2.51%): loss=0.7057, lr=9.84e-05, step_time=1850.7ms, ETA 4d 10h
12/01/2025 23:19:35 - INFO - training.fm_trainer - Step 5280/210000 (2.51%): loss=4.1001, lr=9.90e-05, step_time=1810.1ms, ETA 4d 9h
12/01/2025 23:19:53 - INFO - training.fm_trainer - Step 5290/210000 (2.52%): loss=0.6984, lr=9.90e-05, step_time=1815.0ms, ETA 4d 9h
12/01/2025 23:20:12 - INFO - training.fm_trainer - Step 5300/210000 (2.52%): loss=1.5054, lr=9.90e-05, step_time=1851.1ms, ETA 4d 9h
12/01/2025 23:20:30 - INFO - training.fm_trainer - Step 5310/210000 (2.53%): loss=1.0820, lr=9.90e-05, step_time=1832.4ms, ETA 4d 9h
12/01/2025 23:20:49 - INFO - training.fm_trainer - Step 5320/210000 (2.53%): loss=2.4670, lr=9.96e-05, step_time=1861.4ms, ETA 4d 9h
12/01/2025 23:21:07 - INFO - training.fm_trainer - Step 5330/210000 (2.54%): loss=2.9477, lr=9.96e-05, step_time=1949.9ms, ETA 4d 9h
12/01/2025 23:21:26 - INFO - training.fm_trainer - Step 5340/210000 (2.54%): loss=1.1101, lr=9.96e-05, step_time=1832.4ms, ETA 4d 9h
12/01/2025 23:21:44 - INFO - training.fm_trainer - Step 5350/210000 (2.55%): loss=7.3281, lr=1.00e-04, step_time=1822.7ms, ETA 4d 9h
12/01/2025 23:22:03 - INFO - training.fm_trainer - Step 5360/210000 (2.55%): loss=0.7324, lr=1.00e-04, step_time=1828.1ms, ETA 4d 9h
12/01/2025 23:22:21 - INFO - training.fm_trainer - Step 5370/210000 (2.56%): loss=0.5639, lr=1.00e-04, step_time=1860.2ms, ETA 4d 9h
12/01/2025 23:22:40 - INFO - training.fm_trainer - Step 5380/210000 (2.56%): loss=1.3331, lr=1.01e-04, step_time=1838.3ms, ETA 4d 9h
12/01/2025 23:22:58 - INFO - training.fm_trainer - Step 5390/210000 (2.57%): loss=0.8408, lr=1.01e-04, step_time=1823.8ms, ETA 4d 9h
12/01/2025 23:23:17 - INFO - training.fm_trainer - Step 5400/210000 (2.57%): loss=2.0723, lr=1.01e-04, step_time=1821.3ms, ETA 4d 8h
12/01/2025 23:23:35 - INFO - training.fm_trainer - Step 5410/210000 (2.58%): loss=1.0490, lr=1.01e-04, step_time=1830.2ms, ETA 4d 8h
12/01/2025 23:23:53 - INFO - training.fm_trainer - Step 5420/210000 (2.58%): loss=1.6634, lr=1.01e-04, step_time=1822.2ms, ETA 4d 8h
12/01/2025 23:24:12 - INFO - training.fm_trainer - Step 5430/210000 (2.59%): loss=0.3737, lr=1.01e-04, step_time=1837.7ms, ETA 4d 8h
12/01/2025 23:24:30 - INFO - training.fm_trainer - Step 5440/210000 (2.59%): loss=1.1078, lr=1.02e-04, step_time=1799.8ms, ETA 4d 8h
12/01/2025 23:24:49 - INFO - training.fm_trainer - Step 5450/210000 (2.60%): loss=1.1587, lr=1.02e-04, step_time=1859.9ms, ETA 4d 8h
12/01/2025 23:25:07 - INFO - training.fm_trainer - Step 5460/210000 (2.60%): loss=0.6405, lr=1.02e-04, step_time=1834.2ms, ETA 4d 8h
12/01/2025 23:25:26 - INFO - training.fm_trainer - Step 5470/210000 (2.60%): loss=2.6129, lr=1.02e-04, step_time=1864.9ms, ETA 4d 8h
12/01/2025 23:25:44 - INFO - training.fm_trainer - Step 5480/210000 (2.61%): loss=9.9066, lr=1.03e-04, step_time=1826.3ms, ETA 4d 8h
12/01/2025 23:26:03 - INFO - training.fm_trainer - Step 5490/210000 (2.61%): loss=4.4857, lr=1.03e-04, step_time=1832.4ms, ETA 4d 8h
12/01/2025 23:26:21 - INFO - training.fm_trainer - Step 5500/210000 (2.62%): loss=1.7307, lr=1.03e-04, step_time=1841.4ms, ETA 4d 8h
12/01/2025 23:26:21 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-5500
12/01/2025 23:26:21 - INFO - accelerate.accelerator - Saving FSDP model
12/01/2025 23:26:23 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-5500/pytorch_model_fsdp_0
12/01/2025 23:26:31 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-5500/pytorch_model_fsdp_0
12/01/2025 23:26:31 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-5500
12/01/2025 23:26:31 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/01/2025 23:26:33 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-5500/optimizer_0
12/01/2025 23:26:49 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-5500/optimizer_0
12/01/2025 23:26:49 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-5500
12/01/2025 23:26:49 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-5500/scheduler.bin
12/01/2025 23:26:49 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-5500/sampler.bin
12/01/2025 23:26:49 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-5500/random_states_0.pkl
12/01/2025 23:26:49 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-5500
12/01/2025 23:27:08 - INFO - training.fm_trainer - Step 5510/210000 (2.62%): loss=8.8467, lr=1.03e-04, step_time=1864.3ms, ETA 4d 8h
12/01/2025 23:27:26 - INFO - training.fm_trainer - Step 5520/210000 (2.63%): loss=0.8556, lr=1.03e-04, step_time=1845.5ms, ETA 4d 8h
12/01/2025 23:27:45 - INFO - training.fm_trainer - Step 5530/210000 (2.63%): loss=0.4581, lr=1.03e-04, step_time=1856.3ms, ETA 4d 8h
12/01/2025 23:28:03 - INFO - training.fm_trainer - Step 5540/210000 (2.64%): loss=0.8257, lr=1.04e-04, step_time=1837.9ms, ETA 4d 8h
12/01/2025 23:28:22 - INFO - training.fm_trainer - Step 5550/210000 (2.64%): loss=3.0833, lr=1.04e-04, step_time=1864.3ms, ETA 4d 8h
12/01/2025 23:28:40 - INFO - training.fm_trainer - Step 5560/210000 (2.65%): loss=11.2425, lr=1.04e-04, step_time=1840.7ms, ETA 4d 8h
12/01/2025 23:28:59 - INFO - training.fm_trainer - Step 5570/210000 (2.65%): loss=2.6299, lr=1.04e-04, step_time=1875.3ms, ETA 4d 8h
12/01/2025 23:29:18 - INFO - training.fm_trainer - Step 5580/210000 (2.66%): loss=2.5425, lr=1.04e-04, step_time=1833.4ms, ETA 4d 8h
12/01/2025 23:29:36 - INFO - training.fm_trainer - Step 5590/210000 (2.66%): loss=0.9488, lr=1.04e-04, step_time=1839.5ms, ETA 4d 8h
12/01/2025 23:29:54 - INFO - training.fm_trainer - Step 5600/210000 (2.67%): loss=2.8282, lr=1.05e-04, step_time=1851.3ms, ETA 4d 8h
12/01/2025 23:30:13 - INFO - training.fm_trainer - Step 5610/210000 (2.67%): loss=1.6422, lr=1.05e-04, step_time=1867.9ms, ETA 4d 8h
12/01/2025 23:30:32 - INFO - training.fm_trainer - Step 5620/210000 (2.68%): loss=2.5161, lr=1.05e-04, step_time=1869.8ms, ETA 4d 9h
12/01/2025 23:30:50 - INFO - training.fm_trainer - Step 5630/210000 (2.68%): loss=3.2969, lr=1.05e-04, step_time=1886.9ms, ETA 4d 9h
12/01/2025 23:31:09 - INFO - training.fm_trainer - Step 5640/210000 (2.69%): loss=4.8961, lr=1.06e-04, step_time=1829.0ms, ETA 4d 9h
12/01/2025 23:31:27 - INFO - training.fm_trainer - Step 5650/210000 (2.69%): loss=3.9096, lr=1.06e-04, step_time=1856.1ms, ETA 4d 9h
12/01/2025 23:31:46 - INFO - training.fm_trainer - Step 5660/210000 (2.70%): loss=2.6860, lr=1.06e-04, step_time=1868.3ms, ETA 4d 9h
12/01/2025 23:32:04 - INFO - training.fm_trainer - Step 5670/210000 (2.70%): loss=8.1338, lr=1.06e-04, step_time=1862.8ms, ETA 4d 9h
12/01/2025 23:32:23 - INFO - training.fm_trainer - Step 5680/210000 (2.70%): loss=1.4888, lr=1.06e-04, step_time=1823.5ms, ETA 4d 9h
12/01/2025 23:32:41 - INFO - training.fm_trainer - Step 5690/210000 (2.71%): loss=2.7600, lr=1.06e-04, step_time=1833.0ms, ETA 4d 8h
12/01/2025 23:33:00 - INFO - training.fm_trainer - Step 5700/210000 (2.71%): loss=1.9116, lr=1.07e-04, step_time=1813.2ms, ETA 4d 8h
12/01/2025 23:33:18 - INFO - training.fm_trainer - Step 5710/210000 (2.72%): loss=3.6169, lr=1.07e-04, step_time=1854.1ms, ETA 4d 8h
12/01/2025 23:33:37 - INFO - training.fm_trainer - Step 5720/210000 (2.72%): loss=2.8752, lr=1.07e-04, step_time=1880.7ms, ETA 4d 8h
12/01/2025 23:33:55 - INFO - training.fm_trainer - Step 5730/210000 (2.73%): loss=0.9334, lr=1.07e-04, step_time=1819.7ms, ETA 4d 8h
12/01/2025 23:34:14 - INFO - training.fm_trainer - Step 5740/210000 (2.73%): loss=1.4524, lr=1.07e-04, step_time=1862.9ms, ETA 4d 8h
12/01/2025 23:34:33 - INFO - training.fm_trainer - Step 5750/210000 (2.74%): loss=11.2112, lr=1.07e-04, step_time=1841.8ms, ETA 4d 8h
12/01/2025 23:34:51 - INFO - training.fm_trainer - Step 5760/210000 (2.74%): loss=3.3807, lr=1.08e-04, step_time=1776.6ms, ETA 4d 8h
12/01/2025 23:35:10 - INFO - training.fm_trainer - Step 5770/210000 (2.75%): loss=1.5000, lr=1.08e-04, step_time=1838.2ms, ETA 4d 8h
12/01/2025 23:35:28 - INFO - training.fm_trainer - Step 5780/210000 (2.75%): loss=1.0856, lr=1.08e-04, step_time=1858.3ms, ETA 4d 8h
12/01/2025 23:35:47 - INFO - training.fm_trainer - Step 5790/210000 (2.76%): loss=1.1001, lr=1.08e-04, step_time=1823.5ms, ETA 4d 8h
12/01/2025 23:36:06 - INFO - training.fm_trainer - Step 5800/210000 (2.76%): loss=4.2549, lr=1.09e-04, step_time=1956.4ms, ETA 4d 9h
12/01/2025 23:36:24 - INFO - training.fm_trainer - Step 5810/210000 (2.77%): loss=3.4599, lr=1.09e-04, step_time=1834.1ms, ETA 4d 8h
12/01/2025 23:36:43 - INFO - training.fm_trainer - Step 5820/210000 (2.77%): loss=0.4194, lr=1.09e-04, step_time=1828.0ms, ETA 4d 8h
12/01/2025 23:37:01 - INFO - training.fm_trainer - Step 5830/210000 (2.78%): loss=2.1515, lr=1.09e-04, step_time=1825.2ms, ETA 4d 8h
12/01/2025 23:37:20 - INFO - training.fm_trainer - Step 5840/210000 (2.78%): loss=5.9275, lr=1.09e-04, step_time=1835.5ms, ETA 4d 8h
12/01/2025 23:37:39 - INFO - training.fm_trainer - Step 5850/210000 (2.79%): loss=5.9272, lr=1.09e-04, step_time=1879.5ms, ETA 4d 8h
12/01/2025 23:37:57 - INFO - training.fm_trainer - Step 5860/210000 (2.79%): loss=1.7641, lr=1.10e-04, step_time=1843.5ms, ETA 4d 8h
12/01/2025 23:38:16 - INFO - training.fm_trainer - Step 5870/210000 (2.80%): loss=1.0164, lr=1.10e-04, step_time=1853.9ms, ETA 4d 8h
12/01/2025 23:38:34 - INFO - training.fm_trainer - Step 5880/210000 (2.80%): loss=0.5555, lr=1.10e-04, step_time=1851.1ms, ETA 4d 8h
12/01/2025 23:38:53 - INFO - training.fm_trainer - Step 5890/210000 (2.80%): loss=1.0585, lr=1.10e-04, step_time=1870.2ms, ETA 4d 8h
12/01/2025 23:39:12 - INFO - training.fm_trainer - Step 5900/210000 (2.81%): loss=2.3360, lr=1.10e-04, step_time=1827.0ms, ETA 4d 8h
12/01/2025 23:39:31 - INFO - training.fm_trainer - Step 5910/210000 (2.81%): loss=0.7230, lr=1.10e-04, step_time=1882.3ms, ETA 4d 8h
12/01/2025 23:39:49 - INFO - training.fm_trainer - Step 5920/210000 (2.82%): loss=1.5966, lr=1.11e-04, step_time=1770.5ms, ETA 4d 8h
12/01/2025 23:40:08 - INFO - training.fm_trainer - Step 5930/210000 (2.82%): loss=1.1967, lr=1.11e-04, step_time=1829.9ms, ETA 4d 8h
12/01/2025 23:40:26 - INFO - training.fm_trainer - Step 5940/210000 (2.83%): loss=3.4917, lr=1.11e-04, step_time=1867.9ms, ETA 4d 8h
12/01/2025 23:40:45 - INFO - training.fm_trainer - Step 5950/210000 (2.83%): loss=0.4257, lr=1.11e-04, step_time=1820.9ms, ETA 4d 8h
12/01/2025 23:41:03 - INFO - training.fm_trainer - Step 5960/210000 (2.84%): loss=2.4165, lr=1.12e-04, step_time=1842.4ms, ETA 4d 8h
12/01/2025 23:41:21 - INFO - training.fm_trainer - Step 5970/210000 (2.84%): loss=0.7445, lr=1.12e-04, step_time=1862.4ms, ETA 4d 8h
12/01/2025 23:41:40 - INFO - training.fm_trainer - Step 5980/210000 (2.85%): loss=0.5195, lr=1.12e-04, step_time=1829.9ms, ETA 4d 8h
12/01/2025 23:41:59 - INFO - training.fm_trainer - Step 5990/210000 (2.85%): loss=4.0654, lr=1.12e-04, step_time=1808.0ms, ETA 4d 8h
12/01/2025 23:42:17 - INFO - training.fm_trainer - Step 6000/210000 (2.86%): loss=0.7906, lr=1.12e-04, step_time=1839.0ms, ETA 4d 8h
12/01/2025 23:42:17 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-6000
12/01/2025 23:42:17 - INFO - accelerate.accelerator - Saving FSDP model
12/01/2025 23:42:18 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-6000/pytorch_model_fsdp_0
12/01/2025 23:42:26 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-6000/pytorch_model_fsdp_0
12/01/2025 23:42:26 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-6000
12/01/2025 23:42:26 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/01/2025 23:42:28 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-6000/optimizer_0
12/01/2025 23:42:44 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-6000/optimizer_0
12/01/2025 23:42:45 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-6000
12/01/2025 23:42:45 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-6000/scheduler.bin
12/01/2025 23:42:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-6000/sampler.bin
12/01/2025 23:42:45 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-6000/random_states_0.pkl
12/01/2025 23:42:45 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-6000
12/01/2025 23:43:03 - INFO - training.fm_trainer - Step 6010/210000 (2.86%): loss=1.4536, lr=1.12e-04, step_time=1869.0ms, ETA 4d 8h
12/01/2025 23:43:22 - INFO - training.fm_trainer - Step 6020/210000 (2.87%): loss=8.6271, lr=1.13e-04, step_time=1899.7ms, ETA 4d 8h
12/01/2025 23:43:40 - INFO - training.fm_trainer - Step 6030/210000 (2.87%): loss=1.8033, lr=1.13e-04, step_time=1840.8ms, ETA 4d 8h
12/01/2025 23:43:59 - INFO - training.fm_trainer - Step 6040/210000 (2.88%): loss=4.7424, lr=1.13e-04, step_time=1830.2ms, ETA 4d 8h
12/01/2025 23:44:17 - INFO - training.fm_trainer - Step 6050/210000 (2.88%): loss=4.7946, lr=1.13e-04, step_time=1836.5ms, ETA 4d 8h
12/01/2025 23:44:36 - INFO - training.fm_trainer - Step 6060/210000 (2.89%): loss=1.4220, lr=1.13e-04, step_time=1889.9ms, ETA 4d 8h
12/01/2025 23:44:54 - INFO - training.fm_trainer - Step 6070/210000 (2.89%): loss=1.4917, lr=1.13e-04, step_time=1843.5ms, ETA 4d 8h
12/01/2025 23:45:13 - INFO - training.fm_trainer - Step 6080/210000 (2.90%): loss=2.5385, lr=1.14e-04, step_time=1796.1ms, ETA 4d 8h
12/01/2025 23:45:31 - INFO - training.fm_trainer - Step 6090/210000 (2.90%): loss=6.9578, lr=1.14e-04, step_time=1831.7ms, ETA 4d 8h
12/01/2025 23:45:50 - INFO - training.fm_trainer - Step 6100/210000 (2.90%): loss=1.2909, lr=1.14e-04, step_time=1826.6ms, ETA 4d 8h
12/01/2025 23:46:08 - INFO - training.fm_trainer - Step 6110/210000 (2.91%): loss=2.3432, lr=1.14e-04, step_time=1857.4ms, ETA 4d 8h
12/01/2025 23:46:27 - INFO - training.fm_trainer - Step 6120/210000 (2.91%): loss=6.3481, lr=1.15e-04, step_time=2073.9ms, ETA 4d 9h
12/01/2025 23:46:46 - INFO - training.fm_trainer - Step 6130/210000 (2.92%): loss=7.3949, lr=1.15e-04, step_time=1835.2ms, ETA 4d 9h
12/01/2025 23:47:12 - INFO - training.fm_trainer - Step 6140/210000 (2.92%): loss=2.2899, lr=1.15e-04, step_time=3162.0ms, ETA 4d 16h
12/01/2025 23:47:33 - INFO - training.fm_trainer - Step 6150/210000 (2.93%): loss=7.5559, lr=1.15e-04, step_time=1872.1ms, ETA 4d 16h
12/01/2025 23:47:51 - INFO - training.fm_trainer - Step 6160/210000 (2.93%): loss=4.7115, lr=1.15e-04, step_time=1818.7ms, ETA 4d 15h
12/01/2025 23:48:10 - INFO - training.fm_trainer - Step 6170/210000 (2.94%): loss=1.7651, lr=1.15e-04, step_time=1815.0ms, ETA 4d 14h
12/01/2025 23:48:28 - INFO - training.fm_trainer - Step 6180/210000 (2.94%): loss=1.0668, lr=1.16e-04, step_time=1870.9ms, ETA 4d 13h
12/01/2025 23:48:47 - INFO - training.fm_trainer - Step 6190/210000 (2.95%): loss=4.9218, lr=1.16e-04, step_time=1835.9ms, ETA 4d 13h
12/01/2025 23:49:05 - INFO - training.fm_trainer - Step 6200/210000 (2.95%): loss=1.4300, lr=1.16e-04, step_time=1836.7ms, ETA 4d 12h
12/01/2025 23:49:24 - INFO - training.fm_trainer - Step 6210/210000 (2.96%): loss=1.0300, lr=1.16e-04, step_time=1858.4ms, ETA 4d 12h
12/01/2025 23:49:43 - INFO - training.fm_trainer - Step 6220/210000 (2.96%): loss=2.7586, lr=1.16e-04, step_time=1824.9ms, ETA 4d 11h
12/01/2025 23:50:01 - INFO - training.fm_trainer - Step 6230/210000 (2.97%): loss=4.2682, lr=1.16e-04, step_time=1831.4ms, ETA 4d 11h
12/01/2025 23:50:20 - INFO - training.fm_trainer - Step 6240/210000 (2.97%): loss=2.0466, lr=1.17e-04, step_time=1793.8ms, ETA 4d 10h
12/01/2025 23:50:38 - INFO - training.fm_trainer - Step 6250/210000 (2.98%): loss=1.5489, lr=1.17e-04, step_time=1829.7ms, ETA 4d 10h
12/01/2025 23:50:57 - INFO - training.fm_trainer - Step 6260/210000 (2.98%): loss=4.7549, lr=1.17e-04, step_time=1859.3ms, ETA 4d 10h
12/01/2025 23:51:15 - INFO - training.fm_trainer - Step 6270/210000 (2.99%): loss=0.7747, lr=1.17e-04, step_time=1838.3ms, ETA 4d 10h
12/01/2025 23:51:34 - INFO - training.fm_trainer - Step 6280/210000 (2.99%): loss=7.3845, lr=1.18e-04, step_time=1838.7ms, ETA 4d 9h
12/01/2025 23:51:52 - INFO - training.fm_trainer - Step 6290/210000 (3.00%): loss=10.4082, lr=1.18e-04, step_time=1889.5ms, ETA 4d 10h
12/01/2025 23:52:11 - INFO - training.fm_trainer - Step 6300/210000 (3.00%): loss=9.7639, lr=1.18e-04, step_time=1823.1ms, ETA 4d 9h
12/01/2025 23:52:30 - INFO - training.fm_trainer - Step 6310/210000 (3.00%): loss=1.7051, lr=1.18e-04, step_time=1939.2ms, ETA 4d 10h
12/01/2025 23:52:49 - INFO - training.fm_trainer - Step 6320/210000 (3.01%): loss=0.5744, lr=1.18e-04, step_time=1931.2ms, ETA 4d 10h
12/01/2025 23:53:08 - INFO - training.fm_trainer - Step 6330/210000 (3.01%): loss=2.8901, lr=1.18e-04, step_time=1829.4ms, ETA 4d 10h
12/01/2025 23:53:26 - INFO - training.fm_trainer - Step 6340/210000 (3.02%): loss=0.8881, lr=1.19e-04, step_time=2010.7ms, ETA 4d 10h
12/01/2025 23:53:45 - INFO - training.fm_trainer - Step 6350/210000 (3.02%): loss=0.9896, lr=1.19e-04, step_time=1841.6ms, ETA 4d 10h
12/01/2025 23:54:04 - INFO - training.fm_trainer - Step 6360/210000 (3.03%): loss=1.0209, lr=1.19e-04, step_time=1850.2ms, ETA 4d 10h
12/01/2025 23:54:22 - INFO - training.fm_trainer - Step 6370/210000 (3.03%): loss=0.5991, lr=1.19e-04, step_time=1822.0ms, ETA 4d 10h
12/01/2025 23:54:41 - INFO - training.fm_trainer - Step 6380/210000 (3.04%): loss=0.8085, lr=1.19e-04, step_time=1961.1ms, ETA 4d 10h
12/01/2025 23:54:59 - INFO - training.fm_trainer - Step 6390/210000 (3.04%): loss=7.8174, lr=1.19e-04, step_time=1819.1ms, ETA 4d 10h
12/01/2025 23:55:18 - INFO - training.fm_trainer - Step 6400/210000 (3.05%): loss=0.4214, lr=1.20e-04, step_time=1812.9ms, ETA 4d 9h
12/01/2025 23:55:36 - INFO - training.fm_trainer - Step 6410/210000 (3.05%): loss=0.8106, lr=1.20e-04, step_time=1835.5ms, ETA 4d 9h
12/01/2025 23:55:55 - INFO - training.fm_trainer - Step 6420/210000 (3.06%): loss=1.2155, lr=1.20e-04, step_time=1831.0ms, ETA 4d 9h
12/01/2025 23:56:14 - INFO - training.fm_trainer - Step 6430/210000 (3.06%): loss=9.2082, lr=1.20e-04, step_time=1833.4ms, ETA 4d 9h
12/01/2025 23:56:32 - INFO - training.fm_trainer - Step 6440/210000 (3.07%): loss=3.7619, lr=1.21e-04, step_time=1848.9ms, ETA 4d 9h
12/01/2025 23:56:51 - INFO - training.fm_trainer - Step 6450/210000 (3.07%): loss=1.2017, lr=1.21e-04, step_time=1821.9ms, ETA 4d 8h
12/01/2025 23:57:10 - INFO - training.fm_trainer - Step 6460/210000 (3.08%): loss=2.5284, lr=1.21e-04, step_time=1833.1ms, ETA 4d 8h
12/01/2025 23:57:28 - INFO - training.fm_trainer - Step 6470/210000 (3.08%): loss=2.8466, lr=1.21e-04, step_time=1816.7ms, ETA 4d 8h
12/01/2025 23:57:47 - INFO - training.fm_trainer - Step 6480/210000 (3.09%): loss=1.6439, lr=1.21e-04, step_time=1846.4ms, ETA 4d 8h
12/01/2025 23:58:05 - INFO - training.fm_trainer - Step 6490/210000 (3.09%): loss=1.6802, lr=1.21e-04, step_time=1840.5ms, ETA 4d 8h
12/01/2025 23:58:24 - INFO - training.fm_trainer - Step 6500/210000 (3.10%): loss=8.9000, lr=1.22e-04, step_time=1850.6ms, ETA 4d 8h
12/01/2025 23:58:24 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-6500
12/01/2025 23:58:24 - INFO - accelerate.accelerator - Saving FSDP model
12/01/2025 23:58:25 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-6500/pytorch_model_fsdp_0
12/01/2025 23:58:34 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-6500/pytorch_model_fsdp_0
12/01/2025 23:58:34 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-6500
12/01/2025 23:58:34 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/01/2025 23:58:36 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-6500/optimizer_0
12/01/2025 23:58:52 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-6500/optimizer_0
12/01/2025 23:58:52 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-6500
12/01/2025 23:58:52 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-6500/scheduler.bin
12/01/2025 23:58:52 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-6500/sampler.bin
12/01/2025 23:58:52 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-6500/random_states_0.pkl
12/01/2025 23:58:52 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-6500
12/01/2025 23:59:11 - INFO - training.fm_trainer - Step 6510/210000 (3.10%): loss=4.6856, lr=1.22e-04, step_time=1899.1ms, ETA 4d 8h
12/01/2025 23:59:30 - INFO - training.fm_trainer - Step 6520/210000 (3.10%): loss=1.4666, lr=1.22e-04, step_time=1844.7ms, ETA 4d 8h
12/01/2025 23:59:49 - INFO - training.fm_trainer - Step 6530/210000 (3.11%): loss=6.5860, lr=1.22e-04, step_time=1815.7ms, ETA 4d 8h
12/02/2025 00:00:07 - INFO - training.fm_trainer - Step 6540/210000 (3.11%): loss=1.3757, lr=1.22e-04, step_time=1871.8ms, ETA 4d 8h
12/02/2025 00:00:25 - INFO - training.fm_trainer - Step 6550/210000 (3.12%): loss=1.2776, lr=1.22e-04, step_time=1834.8ms, ETA 4d 8h
12/02/2025 00:00:45 - INFO - training.fm_trainer - Step 6560/210000 (3.12%): loss=9.3435, lr=1.23e-04, step_time=1936.0ms, ETA 4d 9h
12/02/2025 00:01:03 - INFO - training.fm_trainer - Step 6570/210000 (3.13%): loss=1.8344, lr=1.23e-04, step_time=1856.5ms, ETA 4d 9h
12/02/2025 00:01:22 - INFO - training.fm_trainer - Step 6580/210000 (3.13%): loss=0.7528, lr=1.23e-04, step_time=2044.9ms, ETA 4d 10h
12/02/2025 00:01:41 - INFO - training.fm_trainer - Step 6590/210000 (3.14%): loss=3.9620, lr=1.23e-04, step_time=2295.4ms, ETA 4d 12h
12/02/2025 00:02:00 - INFO - training.fm_trainer - Step 6600/210000 (3.14%): loss=4.1117, lr=1.24e-04, step_time=1928.1ms, ETA 4d 12h
12/02/2025 00:02:18 - INFO - training.fm_trainer - Step 6610/210000 (3.15%): loss=3.5368, lr=1.24e-04, step_time=1836.9ms, ETA 4d 11h
12/02/2025 00:02:37 - INFO - training.fm_trainer - Step 6620/210000 (3.15%): loss=5.3874, lr=1.24e-04, step_time=1843.9ms, ETA 4d 11h
12/02/2025 00:02:55 - INFO - training.fm_trainer - Step 6630/210000 (3.16%): loss=0.7837, lr=1.24e-04, step_time=1857.7ms, ETA 4d 11h
12/02/2025 00:03:14 - INFO - training.fm_trainer - Step 6640/210000 (3.16%): loss=3.2052, lr=1.24e-04, step_time=1865.2ms, ETA 4d 11h
12/02/2025 00:03:33 - INFO - training.fm_trainer - Step 6650/210000 (3.17%): loss=9.0159, lr=1.24e-04, step_time=1994.8ms, ETA 4d 11h
12/02/2025 00:03:51 - INFO - training.fm_trainer - Step 6660/210000 (3.17%): loss=0.4785, lr=1.25e-04, step_time=1875.6ms, ETA 4d 11h
12/02/2025 00:04:10 - INFO - training.fm_trainer - Step 6670/210000 (3.18%): loss=5.2068, lr=1.25e-04, step_time=1834.0ms, ETA 4d 11h
12/02/2025 00:04:28 - INFO - training.fm_trainer - Step 6680/210000 (3.18%): loss=3.8795, lr=1.25e-04, step_time=1819.4ms, ETA 4d 10h
12/02/2025 00:04:48 - INFO - training.fm_trainer - Step 6690/210000 (3.19%): loss=1.1418, lr=1.25e-04, step_time=1879.4ms, ETA 4d 10h
12/02/2025 00:05:06 - INFO - training.fm_trainer - Step 6700/210000 (3.19%): loss=9.1201, lr=1.25e-04, step_time=1867.6ms, ETA 4d 10h
12/02/2025 00:05:25 - INFO - training.fm_trainer - Step 6710/210000 (3.20%): loss=0.5031, lr=1.25e-04, step_time=1846.0ms, ETA 4d 10h
12/02/2025 00:05:44 - INFO - training.fm_trainer - Step 6720/210000 (3.20%): loss=8.5650, lr=1.26e-04, step_time=1838.1ms, ETA 4d 10h
12/02/2025 00:06:02 - INFO - training.fm_trainer - Step 6730/210000 (3.20%): loss=0.6092, lr=1.26e-04, step_time=1850.2ms, ETA 4d 9h
12/02/2025 00:06:20 - INFO - training.fm_trainer - Step 6740/210000 (3.21%): loss=5.2299, lr=1.26e-04, step_time=1830.4ms, ETA 4d 9h
12/02/2025 00:06:39 - INFO - training.fm_trainer - Step 6750/210000 (3.21%): loss=2.1914, lr=1.26e-04, step_time=1824.9ms, ETA 4d 9h
12/02/2025 00:06:59 - INFO - training.fm_trainer - Step 6760/210000 (3.22%): loss=0.6139, lr=1.27e-04, step_time=1846.6ms, ETA 4d 9h
12/02/2025 00:07:17 - INFO - training.fm_trainer - Step 6770/210000 (3.22%): loss=1.7754, lr=1.27e-04, step_time=1839.2ms, ETA 4d 9h
12/02/2025 00:07:36 - INFO - training.fm_trainer - Step 6780/210000 (3.23%): loss=8.0324, lr=1.27e-04, step_time=1813.4ms, ETA 4d 8h
12/02/2025 00:07:55 - INFO - training.fm_trainer - Step 6790/210000 (3.23%): loss=0.8654, lr=1.27e-04, step_time=1881.0ms, ETA 4d 8h
12/02/2025 00:08:13 - INFO - training.fm_trainer - Step 6800/210000 (3.24%): loss=0.7545, lr=1.27e-04, step_time=1830.5ms, ETA 4d 8h
12/02/2025 00:08:32 - INFO - training.fm_trainer - Step 6810/210000 (3.24%): loss=1.1700, lr=1.27e-04, step_time=2053.9ms, ETA 4d 9h
12/02/2025 00:08:51 - INFO - training.fm_trainer - Step 6820/210000 (3.25%): loss=2.6753, lr=1.28e-04, step_time=1862.6ms, ETA 4d 9h
12/02/2025 00:09:09 - INFO - training.fm_trainer - Step 6830/210000 (3.25%): loss=4.4801, lr=1.28e-04, step_time=1810.5ms, ETA 4d 9h
12/02/2025 00:09:28 - INFO - training.fm_trainer - Step 6840/210000 (3.26%): loss=2.4083, lr=1.28e-04, step_time=1839.4ms, ETA 4d 9h
12/02/2025 00:09:46 - INFO - training.fm_trainer - Step 6850/210000 (3.26%): loss=1.4213, lr=1.28e-04, step_time=1814.8ms, ETA 4d 8h
12/02/2025 00:10:05 - INFO - training.fm_trainer - Step 6860/210000 (3.27%): loss=0.7526, lr=1.28e-04, step_time=1867.9ms, ETA 4d 9h
12/02/2025 00:10:23 - INFO - training.fm_trainer - Step 6870/210000 (3.27%): loss=1.2572, lr=1.28e-04, step_time=1823.8ms, ETA 4d 8h
12/02/2025 00:10:42 - INFO - training.fm_trainer - Step 6880/210000 (3.28%): loss=2.4836, lr=1.29e-04, step_time=1830.7ms, ETA 4d 8h
12/02/2025 00:11:00 - INFO - training.fm_trainer - Step 6890/210000 (3.28%): loss=9.3393, lr=1.29e-04, step_time=1960.9ms, ETA 4d 9h
12/02/2025 00:11:19 - INFO - training.fm_trainer - Step 6900/210000 (3.29%): loss=1.0039, lr=1.29e-04, step_time=1842.5ms, ETA 4d 9h
12/02/2025 00:11:37 - INFO - training.fm_trainer - Step 6910/210000 (3.29%): loss=0.9647, lr=1.29e-04, step_time=1849.9ms, ETA 4d 9h
12/02/2025 00:11:56 - INFO - training.fm_trainer - Step 6920/210000 (3.30%): loss=0.6170, lr=1.30e-04, step_time=1831.5ms, ETA 4d 8h
12/02/2025 00:12:14 - INFO - training.fm_trainer - Step 6930/210000 (3.30%): loss=1.5458, lr=1.30e-04, step_time=1844.5ms, ETA 4d 8h
12/02/2025 00:12:33 - INFO - training.fm_trainer - Step 6940/210000 (3.30%): loss=0.5945, lr=1.30e-04, step_time=1857.3ms, ETA 4d 8h
12/02/2025 00:12:51 - INFO - training.fm_trainer - Step 6950/210000 (3.31%): loss=5.0599, lr=1.30e-04, step_time=1836.6ms, ETA 4d 8h
12/02/2025 00:13:10 - INFO - training.fm_trainer - Step 6960/210000 (3.31%): loss=4.8042, lr=1.30e-04, step_time=1810.4ms, ETA 4d 8h
12/02/2025 00:13:28 - INFO - training.fm_trainer - Step 6970/210000 (3.32%): loss=0.7005, lr=1.30e-04, step_time=1842.9ms, ETA 4d 8h
12/02/2025 00:13:47 - INFO - training.fm_trainer - Step 6980/210000 (3.32%): loss=2.1013, lr=1.31e-04, step_time=1868.1ms, ETA 4d 8h
12/02/2025 00:14:05 - INFO - training.fm_trainer - Step 6990/210000 (3.33%): loss=2.5483, lr=1.31e-04, step_time=1849.0ms, ETA 4d 8h
12/02/2025 00:14:24 - INFO - training.fm_trainer - Step 7000/210000 (3.33%): loss=2.0369, lr=1.31e-04, step_time=1835.8ms, ETA 4d 8h
12/02/2025 00:14:24 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-7000
12/02/2025 00:14:24 - INFO - accelerate.accelerator - Saving FSDP model
12/02/2025 00:14:25 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-7000/pytorch_model_fsdp_0
12/02/2025 00:14:33 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-7000/pytorch_model_fsdp_0
12/02/2025 00:14:33 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-7000
12/02/2025 00:14:33 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/02/2025 00:14:36 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-7000/optimizer_0
12/02/2025 00:14:52 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-7000/optimizer_0
12/02/2025 00:14:52 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-7000
12/02/2025 00:14:52 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-7000/scheduler.bin
12/02/2025 00:14:52 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-7000/sampler.bin
12/02/2025 00:14:52 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-7000/random_states_0.pkl
12/02/2025 00:14:52 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-7000
12/02/2025 00:15:11 - INFO - training.fm_trainer - Step 7010/210000 (3.34%): loss=5.0491, lr=1.31e-04, step_time=1856.5ms, ETA 4d 8h
12/02/2025 00:15:29 - INFO - training.fm_trainer - Step 7020/210000 (3.34%): loss=4.1495, lr=1.31e-04, step_time=1864.2ms, ETA 4d 8h
12/02/2025 00:15:48 - INFO - training.fm_trainer - Step 7030/210000 (3.35%): loss=0.9510, lr=1.31e-04, step_time=1831.1ms, ETA 4d 8h
12/02/2025 00:16:06 - INFO - training.fm_trainer - Step 7040/210000 (3.35%): loss=1.0881, lr=1.32e-04, step_time=1798.2ms, ETA 4d 7h
12/02/2025 00:16:25 - INFO - training.fm_trainer - Step 7050/210000 (3.36%): loss=1.1230, lr=1.32e-04, step_time=1826.0ms, ETA 4d 7h
12/02/2025 00:16:43 - INFO - training.fm_trainer - Step 7060/210000 (3.36%): loss=6.7281, lr=1.32e-04, step_time=1865.0ms, ETA 4d 8h
12/02/2025 00:17:02 - INFO - training.fm_trainer - Step 7070/210000 (3.37%): loss=8.9446, lr=1.32e-04, step_time=1848.7ms, ETA 4d 8h
12/02/2025 00:17:20 - INFO - training.fm_trainer - Step 7080/210000 (3.37%): loss=2.0433, lr=1.33e-04, step_time=1836.3ms, ETA 4d 7h
12/02/2025 00:17:39 - INFO - training.fm_trainer - Step 7090/210000 (3.38%): loss=6.4707, lr=1.33e-04, step_time=1848.3ms, ETA 4d 7h
12/02/2025 00:17:58 - INFO - training.fm_trainer - Step 7100/210000 (3.38%): loss=0.9173, lr=1.33e-04, step_time=1834.2ms, ETA 4d 7h
12/02/2025 00:18:16 - INFO - training.fm_trainer - Step 7110/210000 (3.39%): loss=2.1256, lr=1.33e-04, step_time=1832.4ms, ETA 4d 7h
12/02/2025 00:18:35 - INFO - training.fm_trainer - Step 7120/210000 (3.39%): loss=0.5061, lr=1.33e-04, step_time=1857.3ms, ETA 4d 7h
12/02/2025 00:18:53 - INFO - training.fm_trainer - Step 7130/210000 (3.40%): loss=0.8453, lr=1.33e-04, step_time=1820.0ms, ETA 4d 7h
12/02/2025 00:19:12 - INFO - training.fm_trainer - Step 7140/210000 (3.40%): loss=0.3805, lr=1.34e-04, step_time=1848.7ms, ETA 4d 7h
12/02/2025 00:19:30 - INFO - training.fm_trainer - Step 7150/210000 (3.40%): loss=2.2794, lr=1.34e-04, step_time=1857.0ms, ETA 4d 7h
12/02/2025 00:19:49 - INFO - training.fm_trainer - Step 7160/210000 (3.41%): loss=0.6004, lr=1.34e-04, step_time=1853.0ms, ETA 4d 7h
12/02/2025 00:20:07 - INFO - training.fm_trainer - Step 7170/210000 (3.41%): loss=2.5968, lr=1.34e-04, step_time=1831.7ms, ETA 4d 7h
12/02/2025 00:20:26 - INFO - training.fm_trainer - Step 7180/210000 (3.42%): loss=1.5481, lr=1.34e-04, step_time=1869.2ms, ETA 4d 8h
12/02/2025 00:20:44 - INFO - training.fm_trainer - Step 7190/210000 (3.42%): loss=1.5059, lr=1.34e-04, step_time=1863.0ms, ETA 4d 8h
12/02/2025 00:21:03 - INFO - training.fm_trainer - Step 7200/210000 (3.43%): loss=0.8967, lr=1.35e-04, step_time=1901.3ms, ETA 4d 8h
12/02/2025 00:21:21 - INFO - training.fm_trainer - Step 7210/210000 (3.43%): loss=0.8117, lr=1.35e-04, step_time=1825.0ms, ETA 4d 8h
12/02/2025 00:21:40 - INFO - training.fm_trainer - Step 7220/210000 (3.44%): loss=1.8437, lr=1.35e-04, step_time=1842.7ms, ETA 4d 8h
12/02/2025 00:21:58 - INFO - training.fm_trainer - Step 7230/210000 (3.44%): loss=1.8216, lr=1.35e-04, step_time=1822.6ms, ETA 4d 8h
12/02/2025 00:22:17 - INFO - training.fm_trainer - Step 7240/210000 (3.45%): loss=0.7763, lr=1.36e-04, step_time=1835.4ms, ETA 4d 7h
12/02/2025 00:22:35 - INFO - training.fm_trainer - Step 7250/210000 (3.45%): loss=0.6710, lr=1.36e-04, step_time=1954.1ms, ETA 4d 8h
12/02/2025 00:22:54 - INFO - training.fm_trainer - Step 7260/210000 (3.46%): loss=1.8575, lr=1.36e-04, step_time=1814.9ms, ETA 4d 8h
12/02/2025 00:23:12 - INFO - training.fm_trainer - Step 7270/210000 (3.46%): loss=1.3993, lr=1.36e-04, step_time=1855.9ms, ETA 4d 8h
12/02/2025 00:23:31 - INFO - training.fm_trainer - Step 7280/210000 (3.47%): loss=1.6139, lr=1.36e-04, step_time=1835.5ms, ETA 4d 8h
12/02/2025 00:23:49 - INFO - training.fm_trainer - Step 7290/210000 (3.47%): loss=7.0094, lr=1.36e-04, step_time=1837.2ms, ETA 4d 8h
12/02/2025 00:24:08 - INFO - training.fm_trainer - Step 7300/210000 (3.48%): loss=3.9027, lr=1.37e-04, step_time=1895.9ms, ETA 4d 8h
12/02/2025 00:24:27 - INFO - training.fm_trainer - Step 7310/210000 (3.48%): loss=11.0373, lr=1.37e-04, step_time=1835.0ms, ETA 4d 8h
12/02/2025 00:24:45 - INFO - training.fm_trainer - Step 7320/210000 (3.49%): loss=7.9082, lr=1.37e-04, step_time=1812.6ms, ETA 4d 8h
12/02/2025 00:25:04 - INFO - training.fm_trainer - Step 7330/210000 (3.49%): loss=9.0687, lr=1.37e-04, step_time=1834.4ms, ETA 4d 7h
12/02/2025 00:25:22 - INFO - training.fm_trainer - Step 7340/210000 (3.50%): loss=2.0059, lr=1.37e-04, step_time=1886.7ms, ETA 4d 8h
12/02/2025 00:25:41 - INFO - training.fm_trainer - Step 7350/210000 (3.50%): loss=2.1451, lr=1.37e-04, step_time=1830.2ms, ETA 4d 8h
12/02/2025 00:25:59 - INFO - training.fm_trainer - Step 7360/210000 (3.50%): loss=1.1955, lr=1.38e-04, step_time=1800.3ms, ETA 4d 7h
12/02/2025 00:26:18 - INFO - training.fm_trainer - Step 7370/210000 (3.51%): loss=8.7256, lr=1.38e-04, step_time=1824.2ms, ETA 4d 7h
12/02/2025 00:26:36 - INFO - training.fm_trainer - Step 7380/210000 (3.51%): loss=5.6803, lr=1.38e-04, step_time=1841.7ms, ETA 4d 7h
12/02/2025 00:26:55 - INFO - training.fm_trainer - Step 7390/210000 (3.52%): loss=1.6896, lr=1.38e-04, step_time=1926.1ms, ETA 4d 8h
12/02/2025 00:27:14 - INFO - training.fm_trainer - Step 7400/210000 (3.52%): loss=6.6561, lr=1.39e-04, step_time=1878.8ms, ETA 4d 8h
12/02/2025 00:27:32 - INFO - training.fm_trainer - Step 7410/210000 (3.53%): loss=4.6205, lr=1.39e-04, step_time=1824.5ms, ETA 4d 8h
12/02/2025 00:27:51 - INFO - training.fm_trainer - Step 7420/210000 (3.53%): loss=1.7027, lr=1.39e-04, step_time=1869.9ms, ETA 4d 8h
12/02/2025 00:28:09 - INFO - training.fm_trainer - Step 7430/210000 (3.54%): loss=4.1232, lr=1.39e-04, step_time=1885.1ms, ETA 4d 8h
12/02/2025 00:28:28 - INFO - training.fm_trainer - Step 7440/210000 (3.54%): loss=0.8099, lr=1.39e-04, step_time=1948.3ms, ETA 4d 8h
12/02/2025 00:28:47 - INFO - training.fm_trainer - Step 7450/210000 (3.55%): loss=1.1672, lr=1.39e-04, step_time=1872.8ms, ETA 4d 8h
12/02/2025 00:29:05 - INFO - training.fm_trainer - Step 7460/210000 (3.55%): loss=1.4927, lr=1.40e-04, step_time=1857.7ms, ETA 4d 8h
12/02/2025 00:29:24 - INFO - training.fm_trainer - Step 7470/210000 (3.56%): loss=1.5034, lr=1.40e-04, step_time=1815.8ms, ETA 4d 8h
12/02/2025 00:29:42 - INFO - training.fm_trainer - Step 7480/210000 (3.56%): loss=1.0783, lr=1.40e-04, step_time=1827.7ms, ETA 4d 8h
12/02/2025 00:30:01 - INFO - training.fm_trainer - Step 7490/210000 (3.57%): loss=3.6626, lr=1.40e-04, step_time=1966.4ms, ETA 4d 9h
12/02/2025 00:30:19 - INFO - training.fm_trainer - Step 7500/210000 (3.57%): loss=4.1154, lr=1.40e-04, step_time=1858.9ms, ETA 4d 9h
12/02/2025 00:30:19 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-7500
12/02/2025 00:30:19 - INFO - accelerate.accelerator - Saving FSDP model
12/02/2025 00:30:20 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-7500/pytorch_model_fsdp_0
12/02/2025 00:30:28 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-7500/pytorch_model_fsdp_0
12/02/2025 00:30:28 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-7500
12/02/2025 00:30:28 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/02/2025 00:30:30 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-7500/optimizer_0
12/02/2025 00:30:49 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-7500/optimizer_0
12/02/2025 00:30:49 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-7500
12/02/2025 00:30:49 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-7500/scheduler.bin
12/02/2025 00:30:49 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-7500/sampler.bin
12/02/2025 00:30:49 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-7500/random_states_0.pkl
12/02/2025 00:30:49 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-7500
12/02/2025 00:31:08 - INFO - training.fm_trainer - Step 7510/210000 (3.58%): loss=2.4178, lr=1.40e-04, step_time=1815.1ms, ETA 4d 8h
12/02/2025 00:31:26 - INFO - training.fm_trainer - Step 7520/210000 (3.58%): loss=3.5858, lr=1.41e-04, step_time=1785.4ms, ETA 4d 8h
12/02/2025 00:31:45 - INFO - training.fm_trainer - Step 7530/210000 (3.59%): loss=1.9076, lr=1.41e-04, step_time=1844.8ms, ETA 4d 8h
12/02/2025 00:32:03 - INFO - training.fm_trainer - Step 7540/210000 (3.59%): loss=1.1682, lr=1.41e-04, step_time=1833.3ms, ETA 4d 8h
12/02/2025 00:32:22 - INFO - training.fm_trainer - Step 7550/210000 (3.60%): loss=2.1930, lr=1.41e-04, step_time=1834.3ms, ETA 4d 8h
12/02/2025 00:32:40 - INFO - training.fm_trainer - Step 7560/210000 (3.60%): loss=3.6285, lr=1.42e-04, step_time=1836.2ms, ETA 4d 7h
12/02/2025 00:32:59 - INFO - training.fm_trainer - Step 7570/210000 (3.60%): loss=1.5690, lr=1.42e-04, step_time=1840.2ms, ETA 4d 7h
12/02/2025 00:33:18 - INFO - training.fm_trainer - Step 7580/210000 (3.61%): loss=5.8787, lr=1.42e-04, step_time=1842.1ms, ETA 4d 7h
12/02/2025 00:33:36 - INFO - training.fm_trainer - Step 7590/210000 (3.61%): loss=0.3718, lr=1.42e-04, step_time=1823.9ms, ETA 4d 7h
12/02/2025 00:33:54 - INFO - training.fm_trainer - Step 7600/210000 (3.62%): loss=0.4972, lr=1.42e-04, step_time=1865.1ms, ETA 4d 7h
12/02/2025 00:34:13 - INFO - training.fm_trainer - Step 7610/210000 (3.62%): loss=8.3820, lr=1.42e-04, step_time=1862.8ms, ETA 4d 7h
12/02/2025 00:34:32 - INFO - training.fm_trainer - Step 7620/210000 (3.63%): loss=9.4046, lr=1.43e-04, step_time=1821.2ms, ETA 4d 7h
12/02/2025 00:34:50 - INFO - training.fm_trainer - Step 7630/210000 (3.63%): loss=0.4873, lr=1.43e-04, step_time=1857.5ms, ETA 4d 7h
12/02/2025 00:35:09 - INFO - training.fm_trainer - Step 7640/210000 (3.64%): loss=5.3606, lr=1.43e-04, step_time=1809.3ms, ETA 4d 7h
12/02/2025 00:35:27 - INFO - training.fm_trainer - Step 7650/210000 (3.64%): loss=1.0090, lr=1.43e-04, step_time=1842.6ms, ETA 4d 7h
12/02/2025 00:35:46 - INFO - training.fm_trainer - Step 7660/210000 (3.65%): loss=1.5231, lr=1.43e-04, step_time=1843.0ms, ETA 4d 7h
12/02/2025 00:36:04 - INFO - training.fm_trainer - Step 7670/210000 (3.65%): loss=0.5723, lr=1.43e-04, step_time=1827.0ms, ETA 4d 7h
12/02/2025 00:36:23 - INFO - training.fm_trainer - Step 7680/210000 (3.66%): loss=0.8630, lr=1.44e-04, step_time=1795.6ms, ETA 4d 7h
12/02/2025 00:36:42 - INFO - training.fm_trainer - Step 7690/210000 (3.66%): loss=1.2151, lr=1.44e-04, step_time=1840.8ms, ETA 4d 7h
12/02/2025 00:37:00 - INFO - training.fm_trainer - Step 7700/210000 (3.67%): loss=4.9261, lr=1.44e-04, step_time=1872.3ms, ETA 4d 7h
12/02/2025 00:37:19 - INFO - training.fm_trainer - Step 7710/210000 (3.67%): loss=1.7693, lr=1.44e-04, step_time=1839.4ms, ETA 4d 7h
12/02/2025 00:37:37 - INFO - training.fm_trainer - Step 7720/210000 (3.68%): loss=1.3224, lr=1.45e-04, step_time=1842.8ms, ETA 4d 7h
12/02/2025 00:37:56 - INFO - training.fm_trainer - Step 7730/210000 (3.68%): loss=1.7883, lr=1.45e-04, step_time=1824.0ms, ETA 4d 7h
12/02/2025 00:38:15 - INFO - training.fm_trainer - Step 7740/210000 (3.69%): loss=6.0102, lr=1.45e-04, step_time=1863.5ms, ETA 4d 7h
12/02/2025 00:38:33 - INFO - training.fm_trainer - Step 7750/210000 (3.69%): loss=5.4662, lr=1.45e-04, step_time=1838.9ms, ETA 4d 7h
12/02/2025 00:38:52 - INFO - training.fm_trainer - Step 7760/210000 (3.70%): loss=7.5980, lr=1.45e-04, step_time=1979.4ms, ETA 4d 8h
12/02/2025 00:39:10 - INFO - training.fm_trainer - Step 7770/210000 (3.70%): loss=6.5661, lr=1.45e-04, step_time=1855.5ms, ETA 4d 8h
12/02/2025 00:39:29 - INFO - training.fm_trainer - Step 7780/210000 (3.70%): loss=1.3260, lr=1.46e-04, step_time=1835.0ms, ETA 4d 8h
12/02/2025 00:39:47 - INFO - training.fm_trainer - Step 7790/210000 (3.71%): loss=1.2096, lr=1.46e-04, step_time=1836.2ms, ETA 4d 7h
12/02/2025 00:40:06 - INFO - training.fm_trainer - Step 7800/210000 (3.71%): loss=5.5400, lr=1.46e-04, step_time=1836.0ms, ETA 4d 7h
12/02/2025 00:40:25 - INFO - training.fm_trainer - Step 7810/210000 (3.72%): loss=1.2013, lr=1.46e-04, step_time=1855.1ms, ETA 4d 7h
12/02/2025 00:40:43 - INFO - training.fm_trainer - Step 7820/210000 (3.72%): loss=4.9199, lr=1.46e-04, step_time=1849.1ms, ETA 4d 7h
12/02/2025 00:41:02 - INFO - training.fm_trainer - Step 7830/210000 (3.73%): loss=10.5605, lr=1.46e-04, step_time=1863.5ms, ETA 4d 7h
12/02/2025 00:41:20 - INFO - training.fm_trainer - Step 7840/210000 (3.73%): loss=0.9685, lr=1.47e-04, step_time=1802.3ms, ETA 4d 7h
12/02/2025 00:41:39 - INFO - training.fm_trainer - Step 7850/210000 (3.74%): loss=1.1053, lr=1.47e-04, step_time=1838.1ms, ETA 4d 7h
12/02/2025 00:41:57 - INFO - training.fm_trainer - Step 7860/210000 (3.74%): loss=0.8251, lr=1.47e-04, step_time=1866.0ms, ETA 4d 7h
12/02/2025 00:42:16 - INFO - training.fm_trainer - Step 7870/210000 (3.75%): loss=1.4096, lr=1.47e-04, step_time=1887.8ms, ETA 4d 7h
12/02/2025 00:42:34 - INFO - training.fm_trainer - Step 7880/210000 (3.75%): loss=1.3329, lr=1.48e-04, step_time=1952.6ms, ETA 4d 8h
12/02/2025 00:42:53 - INFO - training.fm_trainer - Step 7890/210000 (3.76%): loss=2.6510, lr=1.48e-04, step_time=1828.0ms, ETA 4d 8h
12/02/2025 00:43:11 - INFO - training.fm_trainer - Step 7900/210000 (3.76%): loss=3.3089, lr=1.48e-04, step_time=1831.4ms, ETA 4d 8h
12/02/2025 00:43:30 - INFO - training.fm_trainer - Step 7910/210000 (3.77%): loss=0.8853, lr=1.48e-04, step_time=1831.3ms, ETA 4d 8h
12/02/2025 00:43:48 - INFO - training.fm_trainer - Step 7920/210000 (3.77%): loss=1.8047, lr=1.48e-04, step_time=1819.3ms, ETA 4d 7h
12/02/2025 00:44:07 - INFO - training.fm_trainer - Step 7930/210000 (3.78%): loss=0.7464, lr=1.48e-04, step_time=2097.1ms, ETA 4d 9h
12/02/2025 00:44:26 - INFO - training.fm_trainer - Step 7940/210000 (3.78%): loss=0.6898, lr=1.49e-04, step_time=1876.7ms, ETA 4d 9h
12/02/2025 00:44:44 - INFO - training.fm_trainer - Step 7950/210000 (3.79%): loss=0.9240, lr=1.49e-04, step_time=1835.3ms, ETA 4d 9h
12/02/2025 00:45:03 - INFO - training.fm_trainer - Step 7960/210000 (3.79%): loss=3.0562, lr=1.49e-04, step_time=1859.0ms, ETA 4d 8h
12/02/2025 00:45:21 - INFO - training.fm_trainer - Step 7970/210000 (3.80%): loss=0.8518, lr=1.49e-04, step_time=1821.9ms, ETA 4d 8h
12/02/2025 00:45:40 - INFO - training.fm_trainer - Step 7980/210000 (3.80%): loss=0.7599, lr=1.49e-04, step_time=1848.3ms, ETA 4d 8h
12/02/2025 00:45:59 - INFO - training.fm_trainer - Step 7990/210000 (3.80%): loss=1.0273, lr=1.49e-04, step_time=1844.5ms, ETA 4d 8h
12/02/2025 00:46:17 - INFO - training.fm_trainer - Step 8000/210000 (3.81%): loss=1.2922, lr=1.50e-04, step_time=1767.6ms, ETA 4d 7h
12/02/2025 00:46:17 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-8000
12/02/2025 00:46:17 - INFO - accelerate.accelerator - Saving FSDP model
12/02/2025 00:46:18 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-8000/pytorch_model_fsdp_0
12/02/2025 00:46:27 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-8000/pytorch_model_fsdp_0
12/02/2025 00:46:27 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-8000
12/02/2025 00:46:27 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/02/2025 00:46:29 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-8000/optimizer_0
12/02/2025 00:46:45 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-8000/optimizer_0
12/02/2025 00:46:45 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-8000
12/02/2025 00:46:45 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-8000/scheduler.bin
12/02/2025 00:46:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-8000/sampler.bin
12/02/2025 00:46:45 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-8000/random_states_0.pkl
12/02/2025 00:46:45 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-8000
12/02/2025 00:47:04 - INFO - training.fm_trainer - Step 8010/210000 (3.81%): loss=1.3932, lr=1.50e-04, step_time=1869.6ms, ETA 4d 8h
12/02/2025 00:47:22 - INFO - training.fm_trainer - Step 8020/210000 (3.82%): loss=1.8111, lr=1.50e-04, step_time=1887.6ms, ETA 4d 8h
12/02/2025 00:47:41 - INFO - training.fm_trainer - Step 8030/210000 (3.82%): loss=4.1823, lr=1.50e-04, step_time=1827.2ms, ETA 4d 8h
12/02/2025 00:48:00 - INFO - training.fm_trainer - Step 8040/210000 (3.83%): loss=8.5813, lr=1.51e-04, step_time=1846.5ms, ETA 4d 7h
12/02/2025 00:48:18 - INFO - training.fm_trainer - Step 8050/210000 (3.83%): loss=1.4575, lr=1.51e-04, step_time=1861.4ms, ETA 4d 8h
12/02/2025 00:48:37 - INFO - training.fm_trainer - Step 8060/210000 (3.84%): loss=0.9558, lr=1.51e-04, step_time=1819.7ms, ETA 4d 7h
12/02/2025 00:48:55 - INFO - training.fm_trainer - Step 8070/210000 (3.84%): loss=3.6905, lr=1.51e-04, step_time=1860.6ms, ETA 4d 7h
12/02/2025 00:49:16 - INFO - training.fm_trainer - Step 8080/210000 (3.85%): loss=2.7465, lr=1.51e-04, step_time=1882.6ms, ETA 4d 8h
12/02/2025 00:49:34 - INFO - training.fm_trainer - Step 8090/210000 (3.85%): loss=1.0387, lr=1.51e-04, step_time=1817.6ms, ETA 4d 7h
12/02/2025 00:49:53 - INFO - training.fm_trainer - Step 8100/210000 (3.86%): loss=1.2491, lr=1.52e-04, step_time=1854.1ms, ETA 4d 7h
12/02/2025 00:50:12 - INFO - training.fm_trainer - Step 8110/210000 (3.86%): loss=3.2369, lr=1.52e-04, step_time=1872.5ms, ETA 4d 7h
12/02/2025 00:50:30 - INFO - training.fm_trainer - Step 8120/210000 (3.87%): loss=6.0795, lr=1.52e-04, step_time=1820.3ms, ETA 4d 7h
12/02/2025 00:50:49 - INFO - training.fm_trainer - Step 8130/210000 (3.87%): loss=1.8738, lr=1.52e-04, step_time=1850.3ms, ETA 4d 7h
12/02/2025 00:51:07 - INFO - training.fm_trainer - Step 8140/210000 (3.88%): loss=1.2188, lr=1.52e-04, step_time=1852.2ms, ETA 4d 7h
12/02/2025 00:51:26 - INFO - training.fm_trainer - Step 8150/210000 (3.88%): loss=3.8126, lr=1.52e-04, step_time=1956.7ms, ETA 4d 8h
12/02/2025 00:51:45 - INFO - training.fm_trainer - Step 8160/210000 (3.89%): loss=4.9490, lr=1.53e-04, step_time=1766.5ms, ETA 4d 7h
12/02/2025 00:52:03 - INFO - training.fm_trainer - Step 8170/210000 (3.89%): loss=2.3253, lr=1.53e-04, step_time=1824.5ms, ETA 4d 7h
12/02/2025 00:52:21 - INFO - training.fm_trainer - Step 8180/210000 (3.90%): loss=3.1861, lr=1.53e-04, step_time=1826.2ms, ETA 4d 7h
12/02/2025 00:52:40 - INFO - training.fm_trainer - Step 8190/210000 (3.90%): loss=1.4654, lr=1.53e-04, step_time=1815.6ms, ETA 4d 7h
12/02/2025 00:52:58 - INFO - training.fm_trainer - Step 8200/210000 (3.90%): loss=1.2456, lr=1.54e-04, step_time=1867.6ms, ETA 4d 7h
12/02/2025 00:53:17 - INFO - training.fm_trainer - Step 8210/210000 (3.91%): loss=1.1447, lr=1.54e-04, step_time=1866.1ms, ETA 4d 7h
12/02/2025 00:53:35 - INFO - training.fm_trainer - Step 8220/210000 (3.91%): loss=5.5817, lr=1.54e-04, step_time=1844.2ms, ETA 4d 7h
12/02/2025 00:53:54 - INFO - training.fm_trainer - Step 8230/210000 (3.92%): loss=4.9282, lr=1.54e-04, step_time=1851.4ms, ETA 4d 7h
12/02/2025 00:54:12 - INFO - training.fm_trainer - Step 8240/210000 (3.92%): loss=1.1260, lr=1.54e-04, step_time=1827.3ms, ETA 4d 7h
12/02/2025 00:54:31 - INFO - training.fm_trainer - Step 8250/210000 (3.93%): loss=2.9982, lr=1.54e-04, step_time=1958.3ms, ETA 4d 8h
12/02/2025 00:54:50 - INFO - training.fm_trainer - Step 8260/210000 (3.93%): loss=1.1653, lr=1.55e-04, step_time=1823.7ms, ETA 4d 7h
12/02/2025 00:55:08 - INFO - training.fm_trainer - Step 8270/210000 (3.94%): loss=0.7620, lr=1.55e-04, step_time=1903.8ms, ETA 4d 8h
12/02/2025 00:55:26 - INFO - training.fm_trainer - Step 8280/210000 (3.94%): loss=4.1865, lr=1.55e-04, step_time=1852.9ms, ETA 4d 8h
12/02/2025 00:55:45 - INFO - training.fm_trainer - Step 8290/210000 (3.95%): loss=6.6385, lr=1.55e-04, step_time=1829.8ms, ETA 4d 7h
12/02/2025 00:56:04 - INFO - training.fm_trainer - Step 8300/210000 (3.95%): loss=9.3414, lr=1.55e-04, step_time=1809.4ms, ETA 4d 7h
12/02/2025 00:56:22 - INFO - training.fm_trainer - Step 8310/210000 (3.96%): loss=2.3748, lr=1.55e-04, step_time=1848.0ms, ETA 4d 7h
12/02/2025 00:56:41 - INFO - training.fm_trainer - Step 8320/210000 (3.96%): loss=5.6270, lr=1.56e-04, step_time=1841.6ms, ETA 4d 7h
12/02/2025 00:56:59 - INFO - training.fm_trainer - Step 8330/210000 (3.97%): loss=1.0479, lr=1.56e-04, step_time=1822.8ms, ETA 4d 7h
12/02/2025 00:57:18 - INFO - training.fm_trainer - Step 8340/210000 (3.97%): loss=0.6072, lr=1.56e-04, step_time=1856.5ms, ETA 4d 7h
12/02/2025 00:57:37 - INFO - training.fm_trainer - Step 8350/210000 (3.98%): loss=0.3601, lr=1.56e-04, step_time=1836.7ms, ETA 4d 7h
12/02/2025 00:57:55 - INFO - training.fm_trainer - Step 8360/210000 (3.98%): loss=3.1454, lr=1.57e-04, step_time=1839.4ms, ETA 4d 7h
12/02/2025 00:58:13 - INFO - training.fm_trainer - Step 8370/210000 (3.99%): loss=0.8126, lr=1.57e-04, step_time=1858.2ms, ETA 4d 7h
12/02/2025 00:58:32 - INFO - training.fm_trainer - Step 8380/210000 (3.99%): loss=1.6428, lr=1.57e-04, step_time=1856.2ms, ETA 4d 7h
12/02/2025 00:58:51 - INFO - training.fm_trainer - Step 8390/210000 (4.00%): loss=8.3584, lr=1.57e-04, step_time=1947.2ms, ETA 4d 8h
12/02/2025 00:59:09 - INFO - training.fm_trainer - Step 8400/210000 (4.00%): loss=0.5722, lr=1.57e-04, step_time=1886.8ms, ETA 4d 8h
12/02/2025 00:59:28 - INFO - training.fm_trainer - Step 8410/210000 (4.00%): loss=1.3380, lr=1.57e-04, step_time=1817.9ms, ETA 4d 7h
12/02/2025 00:59:46 - INFO - training.fm_trainer - Step 8420/210000 (4.01%): loss=2.3443, lr=1.58e-04, step_time=1840.1ms, ETA 4d 7h
12/02/2025 01:00:05 - INFO - training.fm_trainer - Step 8430/210000 (4.01%): loss=6.7059, lr=1.58e-04, step_time=1900.3ms, ETA 4d 8h
12/02/2025 01:00:23 - INFO - training.fm_trainer - Step 8440/210000 (4.02%): loss=1.8192, lr=1.58e-04, step_time=1827.0ms, ETA 4d 7h
12/02/2025 01:00:42 - INFO - training.fm_trainer - Step 8450/210000 (4.02%): loss=6.7657, lr=1.58e-04, step_time=1832.8ms, ETA 4d 7h
12/02/2025 01:01:00 - INFO - training.fm_trainer - Step 8460/210000 (4.03%): loss=0.5649, lr=1.58e-04, step_time=1839.3ms, ETA 4d 7h
12/02/2025 01:01:19 - INFO - training.fm_trainer - Step 8470/210000 (4.03%): loss=1.6728, lr=1.58e-04, step_time=1847.9ms, ETA 4d 7h
12/02/2025 01:01:38 - INFO - training.fm_trainer - Step 8480/210000 (4.04%): loss=1.0315, lr=1.59e-04, step_time=1963.8ms, ETA 4d 8h
12/02/2025 01:01:56 - INFO - training.fm_trainer - Step 8490/210000 (4.04%): loss=5.5632, lr=1.59e-04, step_time=1825.0ms, ETA 4d 8h
12/02/2025 01:02:15 - INFO - training.fm_trainer - Step 8500/210000 (4.05%): loss=2.1498, lr=1.59e-04, step_time=1827.8ms, ETA 4d 7h
12/02/2025 01:02:15 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-8500
12/02/2025 01:02:15 - INFO - accelerate.accelerator - Saving FSDP model
12/02/2025 01:02:16 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-8500/pytorch_model_fsdp_0
12/02/2025 01:02:24 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-8500/pytorch_model_fsdp_0
12/02/2025 01:02:24 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-8500
12/02/2025 01:02:24 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/02/2025 01:02:26 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-8500/optimizer_0
12/02/2025 01:02:43 - INFO - accelerate.utils.fsdp_utils - Optimizer state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-8500/optimizer_0
12/02/2025 01:02:43 - INFO - accelerate.accelerator - FSDP Optimizer saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-8500
12/02/2025 01:02:43 - INFO - accelerate.checkpointing - Scheduler state saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-8500/scheduler.bin
12/02/2025 01:02:43 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-8500/sampler.bin
12/02/2025 01:02:43 - INFO - accelerate.checkpointing - Random states saved in checkpoints/flow_matching_qwen_1.7b/checkpoint-8500/random_states_0.pkl
12/02/2025 01:02:43 - INFO - training.fm_trainer - Saved checkpoint to checkpoints/flow_matching_qwen_1.7b/checkpoint-8500
12/02/2025 01:03:01 - INFO - training.fm_trainer - Step 8510/210000 (4.05%): loss=1.2073, lr=1.59e-04, step_time=1857.3ms, ETA 4d 7h
12/02/2025 01:03:20 - INFO - training.fm_trainer - Step 8520/210000 (4.06%): loss=2.1536, lr=1.60e-04, step_time=1821.0ms, ETA 4d 7h
12/02/2025 01:03:38 - INFO - training.fm_trainer - Step 8530/210000 (4.06%): loss=1.5930, lr=1.60e-04, step_time=1821.1ms, ETA 4d 7h
12/02/2025 01:03:57 - INFO - training.fm_trainer - Step 8540/210000 (4.07%): loss=1.0417, lr=1.60e-04, step_time=1829.5ms, ETA 4d 7h
12/02/2025 01:04:15 - INFO - training.fm_trainer - Step 8550/210000 (4.07%): loss=3.7400, lr=1.60e-04, step_time=1980.5ms, ETA 4d 8h
12/02/2025 01:04:34 - INFO - training.fm_trainer - Step 8560/210000 (4.08%): loss=0.8351, lr=1.60e-04, step_time=1831.0ms, ETA 4d 7h
12/02/2025 01:04:53 - INFO - training.fm_trainer - Step 8570/210000 (4.08%): loss=7.5282, lr=1.60e-04, step_time=1814.9ms, ETA 4d 7h
12/02/2025 01:05:11 - INFO - training.fm_trainer - Step 8580/210000 (4.09%): loss=1.8560, lr=1.61e-04, step_time=1811.0ms, ETA 4d 7h
12/02/2025 01:05:30 - INFO - training.fm_trainer - Step 8590/210000 (4.09%): loss=0.7541, lr=1.61e-04, step_time=1850.8ms, ETA 4d 7h
12/02/2025 01:05:48 - INFO - training.fm_trainer - Step 8600/210000 (4.10%): loss=2.8380, lr=1.61e-04, step_time=1832.6ms, ETA 4d 7h
12/02/2025 01:06:07 - INFO - training.fm_trainer - Step 8610/210000 (4.10%): loss=4.5083, lr=1.61e-04, step_time=1832.9ms, ETA 4d 7h
12/02/2025 01:06:25 - INFO - training.fm_trainer - Step 8620/210000 (4.10%): loss=2.0053, lr=1.61e-04, step_time=1829.1ms, ETA 4d 7h
12/02/2025 01:06:43 - INFO - training.fm_trainer - Step 8630/210000 (4.11%): loss=0.8755, lr=1.61e-04, step_time=1815.8ms, ETA 4d 7h
12/02/2025 01:07:02 - INFO - training.fm_trainer - Step 8640/210000 (4.11%): loss=8.4692, lr=1.62e-04, step_time=1801.1ms, ETA 4d 6h
12/02/2025 01:07:20 - INFO - training.fm_trainer - Step 8650/210000 (4.12%): loss=1.2070, lr=1.62e-04, step_time=1933.4ms, ETA 4d 7h
12/02/2025 01:07:39 - INFO - training.fm_trainer - Step 8660/210000 (4.12%): loss=2.4087, lr=1.62e-04, step_time=1883.6ms, ETA 4d 7h
12/02/2025 01:07:58 - INFO - training.fm_trainer - Step 8670/210000 (4.13%): loss=5.2758, lr=1.62e-04, step_time=1827.3ms, ETA 4d 7h
12/02/2025 01:08:16 - INFO - training.fm_trainer - Step 8680/210000 (4.13%): loss=0.9328, lr=1.63e-04, step_time=1823.9ms, ETA 4d 7h
12/02/2025 01:08:35 - INFO - training.fm_trainer - Step 8690/210000 (4.14%): loss=2.0944, lr=1.63e-04, step_time=1860.6ms, ETA 4d 7h
12/02/2025 01:08:53 - INFO - training.fm_trainer - Step 8700/210000 (4.14%): loss=1.0697, lr=1.63e-04, step_time=1847.1ms, ETA 4d 7h
12/02/2025 01:09:12 - INFO - training.fm_trainer - Step 8710/210000 (4.15%): loss=1.1501, lr=1.63e-04, step_time=1821.4ms, ETA 4d 7h
12/02/2025 01:09:31 - INFO - training.fm_trainer - Step 8720/210000 (4.15%): loss=4.0124, lr=1.63e-04, step_time=1830.9ms, ETA 4d 7h
12/02/2025 01:09:50 - INFO - training.fm_trainer - Step 8730/210000 (4.16%): loss=9.0467, lr=1.63e-04, step_time=1823.9ms, ETA 4d 6h
12/02/2025 01:10:08 - INFO - training.fm_trainer - Step 8740/210000 (4.16%): loss=0.6994, lr=1.64e-04, step_time=1846.4ms, ETA 4d 6h
12/02/2025 01:10:27 - INFO - training.fm_trainer - Step 8750/210000 (4.17%): loss=1.2539, lr=1.64e-04, step_time=1885.2ms, ETA 4d 7h
12/02/2025 01:10:45 - INFO - training.fm_trainer - Step 8760/210000 (4.17%): loss=0.7289, lr=1.64e-04, step_time=1858.9ms, ETA 4d 7h
12/02/2025 01:11:03 - INFO - training.fm_trainer - Step 8770/210000 (4.18%): loss=8.5323, lr=1.64e-04, step_time=1821.0ms, ETA 4d 7h
12/02/2025 01:11:22 - INFO - training.fm_trainer - Step 8780/210000 (4.18%): loss=0.8763, lr=1.64e-04, step_time=1841.5ms, ETA 4d 7h
12/02/2025 01:11:41 - INFO - training.fm_trainer - Step 8790/210000 (4.19%): loss=4.0806, lr=1.64e-04, step_time=1863.0ms, ETA 4d 7h
12/02/2025 01:11:59 - INFO - training.fm_trainer - Step 8800/210000 (4.19%): loss=0.7087, lr=1.65e-04, step_time=1829.2ms, ETA 4d 7h
12/02/2025 01:12:18 - INFO - training.fm_trainer - Step 8810/210000 (4.20%): loss=4.1951, lr=1.65e-04, step_time=1826.2ms, ETA 4d 6h
12/02/2025 01:12:36 - INFO - training.fm_trainer - Step 8820/210000 (4.20%): loss=2.0642, lr=1.65e-04, step_time=1850.9ms, ETA 4d 7h
12/02/2025 01:12:55 - INFO - training.fm_trainer - Step 8830/210000 (4.20%): loss=1.1971, lr=1.65e-04, step_time=1912.8ms, ETA 4d 7h
12/02/2025 01:13:13 - INFO - training.fm_trainer - Step 8840/210000 (4.21%): loss=1.5026, lr=1.66e-04, step_time=1851.3ms, ETA 4d 7h
12/02/2025 01:13:32 - INFO - training.fm_trainer - Step 8850/210000 (4.21%): loss=9.9859, lr=1.66e-04, step_time=1853.3ms, ETA 4d 7h
12/02/2025 01:13:50 - INFO - training.fm_trainer - Step 8860/210000 (4.22%): loss=1.2617, lr=1.66e-04, step_time=1828.3ms, ETA 4d 7h
12/02/2025 01:14:09 - INFO - training.fm_trainer - Step 8870/210000 (4.22%): loss=8.6181, lr=1.66e-04, step_time=1863.7ms, ETA 4d 7h
12/02/2025 01:14:27 - INFO - training.fm_trainer - Step 8880/210000 (4.23%): loss=9.2760, lr=1.66e-04, step_time=1961.0ms, ETA 4d 7h
12/02/2025 01:14:46 - INFO - training.fm_trainer - Step 8890/210000 (4.23%): loss=1.8137, lr=1.66e-04, step_time=1844.8ms, ETA 4d 7h
12/02/2025 01:15:05 - INFO - training.fm_trainer - Step 8900/210000 (4.24%): loss=6.8554, lr=1.67e-04, step_time=1868.3ms, ETA 4d 7h
12/02/2025 01:15:23 - INFO - training.fm_trainer - Step 8910/210000 (4.24%): loss=0.8835, lr=1.67e-04, step_time=1833.8ms, ETA 4d 7h
12/02/2025 01:15:42 - INFO - training.fm_trainer - Step 8920/210000 (4.25%): loss=2.5608, lr=1.67e-04, step_time=1870.5ms, ETA 4d 7h
12/02/2025 01:16:00 - INFO - training.fm_trainer - Step 8930/210000 (4.25%): loss=1.8386, lr=1.67e-04, step_time=1851.3ms, ETA 4d 7h
12/02/2025 01:16:19 - INFO - training.fm_trainer - Step 8940/210000 (4.26%): loss=1.1598, lr=1.67e-04, step_time=1814.0ms, ETA 4d 7h
12/02/2025 01:16:37 - INFO - training.fm_trainer - Step 8950/210000 (4.26%): loss=0.7928, lr=1.67e-04, step_time=1867.0ms, ETA 4d 7h
12/02/2025 01:16:56 - INFO - training.fm_trainer - Step 8960/210000 (4.27%): loss=2.9485, lr=1.68e-04, step_time=1789.1ms, ETA 4d 7h
12/02/2025 01:17:14 - INFO - training.fm_trainer - Step 8970/210000 (4.27%): loss=8.5976, lr=1.68e-04, step_time=1839.9ms, ETA 4d 7h
12/02/2025 01:17:33 - INFO - training.fm_trainer - Step 8980/210000 (4.28%): loss=2.1460, lr=1.68e-04, step_time=1837.4ms, ETA 4d 7h
12/02/2025 01:17:51 - INFO - training.fm_trainer - Step 8990/210000 (4.28%): loss=1.2667, lr=1.68e-04, step_time=1827.7ms, ETA 4d 7h
12/02/2025 01:18:10 - INFO - training.fm_trainer - Step 9000/210000 (4.29%): loss=8.4769, lr=1.69e-04, step_time=1844.9ms, ETA 4d 7h
12/02/2025 01:18:10 - INFO - accelerate.accelerator - Saving current state to checkpoints/flow_matching_qwen_1.7b/checkpoint-9000
12/02/2025 01:18:10 - INFO - accelerate.accelerator - Saving FSDP model
12/02/2025 01:18:11 - INFO - accelerate.utils.fsdp_utils - Saving model to checkpoints/flow_matching_qwen_1.7b/checkpoint-9000/pytorch_model_fsdp_0
12/02/2025 01:18:19 - INFO - accelerate.utils.fsdp_utils - Model saved to checkpoints/flow_matching_qwen_1.7b/checkpoint-9000/pytorch_model_fsdp_0
12/02/2025 01:18:19 - INFO - accelerate.accelerator - FSDP Model saved to output dir checkpoints/flow_matching_qwen_1.7b/checkpoint-9000
12/02/2025 01:18:19 - INFO - accelerate.accelerator - Saving FSDP Optimizer
12/02/2025 01:18:22 - INFO - accelerate.utils.fsdp_utils - Saving Optimizer state to checkpoints/flow_matching_qwen_1.7b/checkpoint-9000/optimizer_0
Traceback (most recent call last):
  File "/workspace/flow-matching/CoDA/flow-matching-qwen/training/train_h100.py", line 287, in <module>
    main()
    ~~~~^^
  File "/workspace/flow-matching/CoDA/flow-matching-qwen/training/train_h100.py", line 283, in main
    trainer.train()
    ~~~~~~~~~~~~~^^
  File "/workspace/flow-matching/CoDA/flow-matching-qwen/training/fm_trainer.py", line 378, in train
    self._save_checkpoint()
    ~~~~~~~~~~~~~~~~~~~~~^^
  File "/workspace/flow-matching/CoDA/flow-matching-qwen/training/fm_trainer.py", line 452, in _save_checkpoint
    self.accelerator.save_state(checkpoint_dir)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.13/site-packages/accelerate/accelerator.py", line 3674, in save_state
    save_fsdp_optimizer(self.state.fsdp_plugin, self, opt, self._models[i], output_dir, i)
    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.13/site-packages/accelerate/utils/fsdp_utils.py", line 273, in save_fsdp_optimizer
    dist_cp.save(
    ~~~~~~~~~~~~^
        state_dict={"optimizer": optim_state},
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        storage_writer=dist_cp.FileSystemWriter(ckpt_dir),
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        planner=DefaultSavePlanner(),
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/root/miniconda3/lib/python3.13/site-packages/torch/distributed/checkpoint/logger.py", line 87, in wrapper
    result = func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.13/site-packages/torch/distributed/checkpoint/utils.py", line 475, in inner_func
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.13/site-packages/torch/distributed/checkpoint/state_dict_saver.py", line 193, in save
    return _save_state_dict(
        state_dict=_stateful_to_state_dict(state_dict),
    ...<4 lines>...
        use_collectives=use_collectives,
    )
  File "/root/miniconda3/lib/python3.13/site-packages/torch/distributed/checkpoint/state_dict_saver.py", line 479, in _save_state_dict
    metadata = distW.all_reduce("write", write_data, finish_checkpoint)
  File "/root/miniconda3/lib/python3.13/site-packages/torch/distributed/checkpoint/utils.py", line 243, in all_reduce
    all_data = self.gather_object(local_data)
  File "/root/miniconda3/lib/python3.13/site-packages/torch/distributed/checkpoint/utils.py", line 135, in gather_object
    dist.gather_object(
    ~~~~~~~~~~~~~~~~~~^
        obj=object,
        ^^^^^^^^^^^
    ...<2 lines>...
        group=self.group,
        ^^^^^^^^^^^^^^^^^
    )
    ^
  File "/root/miniconda3/lib/python3.13/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
  File "/root/miniconda3/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py", line 3283, in gather_object
    input_tensor, local_size = _object_to_tensor(obj, current_device, group)
                               ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py", line 3071, in _object_to_tensor
    _pickler(f).dump(obj)
    ~~~~~~~~~~~~~~~~^^^^^
TypeError: cannot pickle code objects
[rank0]: Traceback (most recent call last):
[rank0]:   File "/workspace/flow-matching/CoDA/flow-matching-qwen/training/train_h100.py", line 287, in <module>
[rank0]:     main()
[rank0]:     ~~~~^^
[rank0]:   File "/workspace/flow-matching/CoDA/flow-matching-qwen/training/train_h100.py", line 283, in main
[rank0]:     trainer.train()
[rank0]:     ~~~~~~~~~~~~~^^
[rank0]:   File "/workspace/flow-matching/CoDA/flow-matching-qwen/training/fm_trainer.py", line 378, in train
[rank0]:     self._save_checkpoint()
[rank0]:     ~~~~~~~~~~~~~~~~~~~~~^^
[rank0]:   File "/workspace/flow-matching/CoDA/flow-matching-qwen/training/fm_trainer.py", line 452, in _save_checkpoint
[rank0]:     self.accelerator.save_state(checkpoint_dir)
[rank0]:     ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
[rank0]:   File "/root/miniconda3/lib/python3.13/site-packages/accelerate/accelerator.py", line 3674, in save_state
[rank0]:     save_fsdp_optimizer(self.state.fsdp_plugin, self, opt, self._models[i], output_dir, i)
[rank0]:     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/root/miniconda3/lib/python3.13/site-packages/accelerate/utils/fsdp_utils.py", line 273, in save_fsdp_optimizer
[rank0]:     dist_cp.save(
[rank0]:     ~~~~~~~~~~~~^
[rank0]:         state_dict={"optimizer": optim_state},
[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:         storage_writer=dist_cp.FileSystemWriter(ckpt_dir),
[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:         planner=DefaultSavePlanner(),
[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:     )
[rank0]:     ^
[rank0]:   File "/root/miniconda3/lib/python3.13/site-packages/torch/distributed/checkpoint/logger.py", line 87, in wrapper
[rank0]:     result = func(*args, **kwargs)
[rank0]:   File "/root/miniconda3/lib/python3.13/site-packages/torch/distributed/checkpoint/utils.py", line 475, in inner_func
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/root/miniconda3/lib/python3.13/site-packages/torch/distributed/checkpoint/state_dict_saver.py", line 193, in save
[rank0]:     return _save_state_dict(
[rank0]:         state_dict=_stateful_to_state_dict(state_dict),
[rank0]:     ...<4 lines>...
[rank0]:         use_collectives=use_collectives,
[rank0]:     )
[rank0]:   File "/root/miniconda3/lib/python3.13/site-packages/torch/distributed/checkpoint/state_dict_saver.py", line 479, in _save_state_dict
[rank0]:     metadata = distW.all_reduce("write", write_data, finish_checkpoint)
[rank0]:   File "/root/miniconda3/lib/python3.13/site-packages/torch/distributed/checkpoint/utils.py", line 243, in all_reduce
[rank0]:     all_data = self.gather_object(local_data)
[rank0]:   File "/root/miniconda3/lib/python3.13/site-packages/torch/distributed/checkpoint/utils.py", line 135, in gather_object
[rank0]:     dist.gather_object(
[rank0]:     ~~~~~~~~~~~~~~~~~~^
[rank0]:         obj=object,
[rank0]:         ^^^^^^^^^^^
[rank0]:     ...<2 lines>...
[rank0]:         group=self.group,
[rank0]:         ^^^^^^^^^^^^^^^^^
[rank0]:     )
[rank0]:     ^
[rank0]:   File "/root/miniconda3/lib/python3.13/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/root/miniconda3/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py", line 3283, in gather_object
[rank0]:     input_tensor, local_size = _object_to_tensor(obj, current_device, group)
[rank0]:                                ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/root/miniconda3/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py", line 3071, in _object_to_tensor
[rank0]:     _pickler(f).dump(obj)
[rank0]:     ~~~~~~~~~~~~~~~~^^^^^
[rank0]: TypeError: cannot pickle code objects
