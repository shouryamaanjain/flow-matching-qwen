12/02/2025 08:34:35 - INFO - training.fm_trainer - Starting training
12/02/2025 08:34:35 - INFO - training.fm_trainer -   Max steps: 210000
12/02/2025 08:34:35 - INFO - training.fm_trainer -   Global batch size: 128
12/02/2025 08:34:35 - INFO - training.fm_trainer -   Gradient accumulation steps: 32
12/02/2025 08:34:35 - INFO - data.data_loader - Including The Stack v1 datasets (RECOMMENDED):
- Contains actual code content directly
- No AWS credentials needed
- Just login with: huggingface-cli login
See: https://huggingface.co/datasets/bigcode/the-stack-dedup
12/02/2025 08:34:35 - INFO - data.data_loader - Loading dataset: dclm-baseline from mlfoundations/dclm-baseline-1.0
