# Flow Matching LM Post-Training (SFT) Configuration
# Supervised Fine-tuning for instruction following
# Optimized for 4x H100 (80GB) GPUs

# Model - Load from mid-trained checkpoint
model_name_or_path: "./checkpoints/flow_matching_qwen_1.7b_midtrain/checkpoint-50000"
mask_token_id: 151669

# Discrete Flow Matching Parameters (same as mid-training)
scheduler_type: "polynomial_convex"
scheduler_n: 1.0
sampling_eps: 0.001

# SFT Training Configuration
training_mode: "sft"                  # Enables SFT-specific behavior
max_steps: 10000                      # Shorter for SFT
global_batch_size: 64                 # Smaller batch for instruction data
per_device_batch_size: 4              # 4 × 4 GPUs × 4 accum = 64
gradient_accumulation_steps: 4
sequence_length: 8192

# Lower LR for fine-tuning
learning_rate: 5.0e-5                 # Lower than pre-training
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1.0e-8
max_grad_norm: 1.0

# Learning Rate Scheduler
lr_scheduler_type: "cosine"
warmup_steps: 200                     # Shorter warmup for fine-tuning

# Checkpointing
checkpoint_dir: "./checkpoints/flow_matching_qwen_1.7b_instruct"
save_steps: 1000
resume_from_checkpoint: null

# Logging
logging_steps: 10
wandb_project: "flow-matching-lm"
run_name: "fm-qwen-1.7b-sft"

# Distributed Training
mixed_precision: "bf16"

# SFT Data
# Use instruction-following datasets similar to CoDA's SFT data
# Options: ShareGPT, OpenHermes, UltraChat, etc.
dataset_name: "HuggingFaceH4/ultrachat_200k"  # Example SFT dataset
data_dir: null

# SFT-specific settings
# Mask instruction tokens, only predict response tokens
mask_instruction: true
instruction_template: "<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n"
response_template: "<|im_end|>"

# Miscellaneous
seed: 42
